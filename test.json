{"paper_id": 408, "introduction": "GANs (Goodfellow et al., 2014), a family of deep generative models, have shown great capacities to generate photorealistic images (Karras et al., 2019).State-of-the-art models, like StyleGAN (Karras et al., 2019) or TransformerGAN (Jiang et al., 2021), show empirical benefits from relying on overparametrized networks with high-dimensional latent spaces.Besides, manipulating the latent representation of a GAN is also helpful for diverse tasks such as image editing (Shen et al., 2020;Wu et al., 2021) or unsupervised learning of image segmentation (Abdal et al., 2021).However, there is still a poor theoretical understanding of how GANs organize their latent space.We argue that this is a crucial step in better apprehending the behavior of GANs.To better understand GANs, the setting of disconnected distributions learning is enlightening.Experimental and theoretical works (Khayatkhoei et al., 2018;Tanielian et al., 2020) have shown a fundamental limitation of GANs when dealing with such distributions.Since the distribution modeled by GANs is connected, some areas of GANs' support are necessarily mapped outside the true data distribution.When covering modes of a disconnected distribution, GANs try to minimize the measure of the generated distribution lying outside the true modes (e.g. the purple area on the right of Figure 1).In other words, GANs need to minimize the measure of the borders between the modes in the latent space.Considering a Gaussian latent space, minimizing this measure is closely linked to the field of Gaussian isoperimetric inequalities (Ledoux, 1996).This field aims at deriving the partitions that decompose a Gaussian space with a minimal Gaussian-weighted perimeter.We argue that the optimal partitions derived in Gaussian isoperimetric inequalities cast a light on the structure of the latent space of GANs.Most notably, a recent result (Milman and Neeman, 2022) shows that, as long as the number of components m in the partition and the number of dimensions d of the Gaussian space are such that m \u2264 d + 1, the optimal partition is a 'simplicial cluster': a Voronoi diagram obtained from the cells of equidistant points (see left of Figure 1 for m = 3 and d = 3).In this paper, we apply this result to the field of GANs and show, both experimentally and theoretically, that GANs with 'simplicial cluster' latent space minimize out-of-distribution generated samples.We draw the connection between GANs and Gaussian isoperimetric inequalities by using the precision metric (Sajjadi et al., 2018;Kynk\u00e4\u00e4nniemi et al., 2019), which quantifies the portion of generated Figure 1: Illustration of the ability of GANs to find an optimal configuration in the latent space.On the left, the propeller shape is a partition of 3D Gaussian space with the smallest Gaussian-weighted perimeter (Figure from Heilman et al. (2013)).On the right, we show the 3D Gaussian latent space of a GAN trained on three classes of MNIST.Each area colored in blue, green, or red maps samples in one of the three classes.In purple, we observe the samples that are classified with low confidence.We see that the partition reached by the GAN (right) is close to optimality (left), since the latent space partition is similar to the intersection of the propeller on a sphere.points that support the target distribution.We show that GANs with a latent space organized as a simplicial cluster reach optimal precision levels and derive both an upper and a lower bound on the precision of such GANs.Experimentally, we show that the GANs with higher performances tend to organize their latent space as simplicial clusters.To summarize, our contributions are the following:\u2022 We are the first to import the latest results from Gaussian isoperimetric inequalities by (Milman and Neeman, 2022) to the study and understanding of GANs.We use it to show that the latent space structure has major implications on the precision of GANs.\u2022 We derive a new theoretical analysis, stating both an upper bound and a lower bound on the precision of GANs.We show that GANs with latent space organized as a simplicial cluster have an optimal precision whose lower bound decrease in the same order as the upper bound: \u221a log m, where m is the number of modes.\u2022 Experimentally, we show that GANs tend to structure their latent space as 'simplicial clusters' on image datasets.First, we explore two properties of the latent space: linear separability and convexity of classes.Then, we play with the latent space dimension and highlight how it impacts the performance of GANs.Finally, we show that overparametrization helps approaching the optimal structure and improving GANs performance."}
{"paper_id": 409, "introduction": "Treatment effect estimation is the problem of predicting the effect of an intervention (e.g. a treatmentdosage pair) on an outcome of interest to guide decision-making.The challenge for prediction models is to learn this map from observational data, which is formally generated from a different structural causal model in which treatment assignment varies according to an individual's covariates, instead of being fixed by the decision-maker.Counterfactuals define the outcome that would have been observed had the assigned treatment been different.For concreteness, consider designing a policy for the administration of chemotherapy regiments; not all cancer patients in the available data are equally likely to be offered the same type and dosage, with varied factors, e.g.age, wealth, etc., involved in the decision-making process.Evaluating a new treatment combination for a given patient is a data point that is invariably under-represented in the empirical distribution of the data.Treatment effect estimation is studied under a wide range of assumptions, including experimental designs that feature ignorability (Imbens, 2000;Imai & Van Dyk, 2004), multiple treatments, sequential decision-making problems, and different generative models encoded in general causal graphs (Pearl, 2009).There is a growing literature on several parts of this problem in the field of machine learning that attempts to define loss functions that are conducive to learning representations of covariates predictive of both observed and counterfactual outcomes.Existing methods could be generally categorized by the theoretical guarantees that inspire training objectives, driven either by bounds for the generalization error or by doubly-robustness guarantees.In the first line of research, Shalit et al. (2017); Johansson et al. (2020) showed in the binary treatment setting that the counterfactual error, that is not computable from data by design, could be instead bounded by the in-sample error plus a term that quantifies the difference in distributions between treated and untreated populations, leading to a differentiable loss function that can be used to train expressive neural networks.Several papers used this insight to investigate different neural network architectures for this problem.For example, Johansson et al. (2016) proposed to use separate feed-forward prediction heads on top of a common representation, Zhang et al. (2022) use transformers, De Brouwer et al. (2022); Seedat et al. (2022) use neural differential equations.In turn, doubly-robust estimators combine expressive function approximators and inverse probability weighting leveraging statistical non-parametric asymptotic guarantees of both estimators (Funk et al., 2011;Kennedy, 2016;2020).In particular, when the direct estimate of the outcome is biased, such as when using nonparametric or high-dimensional regression, the doubly robust estimator weights the model residuals by inverse propensity weights in order to remove the bias.Its convergence and consistency for treatment effect estimation requires only that one of the estimators is consistent.In principle, any consistent function approximator could be used, which in the context of neural networks has led to several adaptations of loss functions and architectures.For example, Shi et al. (2019) adapted the architecture of Johansson et al. (2016) for this purpose introducing targeted regularization, and Nie et al. (2020) proposed varying coefficient networks in the context of continuously-valued dosage parameters.In both cases, however, the authors provide guarantees for population average treatment effect estimation, in contrast with conditional average treatment effect estimation.Despite the generality of these results, no guarantees and no theoretically motivated loss functions exist for learning representations for counterfactual estimation in the general setting of multiple treatment types and/or continuous treatment values or dosages.The challenge in the context of representation learning is that there is no notion of treatment group as each individual gets assigned a potentially different and unique treatment value.Lack of overlap in finite samples and subsequently large estimation variance for counterfactual predictions are exacerbated in this setting to the extreme that adjustments for distributional differences are, in principle, not applicable.In particular, the intuition for reducing variance by regularization deviates from previous proposals (that regularize representations of covariates to match distributions among groups with different treatment types (Shalit et al., 2017)) as a potentially infinite set of counterfactuals for each individual must be considered.Even the analysis of multiple categorical treatments is currently an open question as, while pairwise comparisons between treatment specific distributions could be implemented in principle, it is not computationally tractable to do so in practice.At this moment, only heuristic neural network architectures for this problem have been proposed, including Dose Response networks that consist of multi-task layers for dosage sub-intervals defined on top of a common representation (Schwab et al., 2020), variants of generative adversarial networks (Bica et al., 2020), and varying coefficient networks (Nie et al., 2020).In this paper, we investigate the design of representation learning-based algorithms for predicting (conditional average) treatment effects in the context of multiple treatments and continuous dosage parameters.Our analysis starts by extending definitions of loss and generalization error to this broader setting, over all possible treatment-dosage pairs.We then show by using the definition of integral probability metrics that the generalization error can be bounded by a term that is computable from data and that involves the factual error and a term that quantifies the statistical dependence between the pair of treatment-dosage random variables and observed confounders.In principle, any treatment space on which we can define a probability measure is consistently accounted for, which gives welldefined bounds on the generalization error for treatments with multiple types and continuous values, and in particular, our bound includes as a special case existing guarantees in the binary treatment case (Shalit et al., 2017).This bound suggests new training objectives for learning representations conducive to counterfactual estimation.Moreover, such objectives are tractable: both avoiding combinatorial numbers of pairwise comparisons and avoiding binning dosage values into different sub-intervals.A further contribution we make is to design extensive numerical comparisons that compare both methods driven by bounds on the generalization error (that typically target conditional average treatment effects) and methods driven by doubly-robust guarantees (that typically target average treatment effects).Moreover, we do so independently of the adopted neural architecture which provides the first analysis of different objectives for the problem of treatment effect estimation with multiple, continuously-valued treatments.We hope these results can give some insight into the trade-offs of different approaches to this problem and demonstrate the ability of representation learning techniques to tackle wider ranging scenarios within treatment effect estimation."}
{"paper_id": 410, "introduction": "The unprecedented progress of machine learning during the past two decades has been catalysed and remains driven by the development of increasingly powerful computer hardware.This progress is enabled by the ability of deep neural networks to scale exceptionally well with increasing data availability and model complexity compared to other approaches.Thus, they can be trained for linear regression on small data sets and, with conceptually simple changes to the network architecture, for language translation or image generation on immense text corpora and image collections.While comparatively exceptional, deep neural networks are understood to still only scale linearly at an exponential cost (Schwartz et al., 2020), leading to diminishing returns (Thompson et al., 2021).Among the machine learning community, this has raised concern over the future direction of the field and a growing exclusivity driven by ever-increasing hardware and energy costs (Schwartz et al., 2020;Thompson et al., 2021;Jurowetzki et al., 2021).After a discourse on the intertwined recent history of deep learning and hardware advances, we will analyse the applicability of the most prominent concerns raised in machine learning research to applied machine learning research in biology and chemistry.We have categorised these concerns under socioeconimic, scientific, and environmental considerations.The hard-and software that catalysed rapid developments in machine learning In late 2002 and early 2003, the release of the Radeon 9700 and GeForce FX video cards introduced a fully programmable graphics pipeline, extending and later replacing the existing fixed function pipelines.Unlike the fixed function pipeline, which allowed the user to only supply input matrices and parameters to built-in operations, the programmable pipeline introduced the execution of user-written shader programs on the GPU (Contributors, 2015).This fundamental change allowed programmers and researchers to exploit the intrinsic parallelism of GPUs 2 years before Intel would introduce its first dual-core CPU.Within months of the availability of this new hardware and the accompanying APIs, researchers implemented linear algebra methods on GPUs and introduced programming frameworks to use GPUs for general-purpose computations (Thompson et al., 2002;Kr\u00fcger & West-ermann, 2003).This rapid development marked the dawn of general-purpose computing on graphics processing units (GPGPU).In a presentation at ICS '08, Harris presented the successes of GPGPU by highlighting a speed-up in molecular docking, N-body simulations, HD video stream transcoding, or image processing-applications in machine learning were not discussed.However, just one year later, the introduction of GPUs as general-purpose processors catalysed the deep learning explosion of the early 2010s by allowing deep learning algorithms pioneered by Alexey Ivakhnenko in 1971 to be run within practical time on widely available consumer hardware when Rajat et al. showed that GPUs outperform CPUs by an order of magnitude in large-scale deep unsupervised learning tasks (Ivakhnenko, 1971;Raina et al., 2009).Hardware and energy requirements increase in machine learning research In 2010, Ciresan et al. ( 2010) introduced a multi-layer perceptron (MLP) with up to 12.11 million free parameters where forward and backward propagation were implemented on a GPU using NVIDIA's proprietary CUDA API introduced by Harris at ICS '08 two years before, speeding up the routines by a factor of 40.In their arXiv paper, they also report the computer's hardware specifications as \"Core2 Quad 9450 2.66GHz processor, 3GB of RAM, and a GTX280 graphics card\".The GTX 280 graphics card by NVIDIA was, at the time of the paper's writing, two years old and cost USD 893 when first released (adjusted for inflation).Equipped with this 2-year-old hardware that cost well below USD 1,000, Cires \u00b8an et al. were able to improve upon the state-of-the-art performance on the MNIST classification benchmark set four years prior by Ranzato et al. (2006).As they not only reported the hardware used but also the time it took to train the model, the power usage of the GPU and CPU, with a thermal design power (TDP) of 236 and 95 Watt, respectively, can be calculated as 114.5h \u00d7 (236 + 95)W = 37.9kWh.Seven years later, Vaswani et al. (2017) introduced the transformer architecture.The training used 8 NVIDIA Tesla P100 GPUs, whose price was \u223cUSD 55,100 at the time, and took 84 hours, resulting in overall energy usage of 84h \u00d7 8 \u00d7 250W = 168kWh.Hardware and energy requirements explode in applied machine learning Applying the novel transformer architecture, NVIDIA reportedly trained the 345 million parameter BERT model in 2019, which was previously introduced by Google in the same year, on 4 DGX-2H servers (64 Tesla V100s) in 79.2 hours, with a maximum power usage of 12,000 Watt, resulting in a total power use of 3.8 MWh (79.2h \u00d7 4 \u00d7 12kW) (Devlin et al., 2018).The cost of this system at the time of training was USD 1,596,000.Alternatively, the BERT model could be trained on on-demand Google Cloud GPUs for USD 2.48 per GPU hour, resulting in total costs of USD 12,570 (2.48 \u00d7 64 \u00d7 79.2).The MT-NLG model presented by NVIDIA and Microsoft in 2021 represents the acceleration of hardware and energy cost in the field (Smith et al., 2022).The 530 billion parameter model was trained on 560 DGX A100 servers-a total of 4,480 NVIDIA A100 80GB Tensor Core GPUs-for 2,160 hours (Rajbhandari et al., 2022).The power usage of a cluster of 560 DGX A100 servers during 2,160 hours is 7.862 GWh (2,160 \u00d7 560 \u00d7 6.5kW).Taking the world average electricity price of USD 0.131 per kWh during December 2021, the total electricity bill for training MT-NLG was USD 1,029,922.The total cost of the hardware is hard to estimate as specialised network hardware is required to build such a cluster; however, the DGX A100 was priced at USD 199,000 on release, resulting in a minimum total cost of USD 111,440,000 (199,000 \u00d7 560).Training the model on on-demand Google Cloud GPUs for USD 2,141.82 per GPU month results in a total cost of USD 28,786,060.8 (3 \u00d7 4,480 \u00d7 2,141.82).Hardware and energy costs drive the de-democratization of machine learning The examples discussed above represent an increasing hardware and energy cost in conducting basic and applied deep learning-based machine learning research.The resulting diminishing returns and the environmental impact have previously been discussed by Thompson et al. (2021) and Schwartz et al. (2020).The development of increasing costs following a potential breakthrough stands in contrast to similar or even more disruptive changes in other fields, such as CRISPR-Cas9 lowering costs in molecular biology or the ever-decreasing costs of genome and RNA sequencing (Ledford, 2015;Wetterstrand, 2021;Gierahn et al., 2017).While CRISPR-Cas9 and affordable sequencing has led to what has been called the democratization of access to sequencing and genome editing (Guernet & Grumolato, 2017;McPherson, 2014;Srivathsan et al., 2019), cutting-edge machine learning research is becoming potentially increasingly expensive and exclusive (Ahmed & Wahed, 2020).Indeed, in a 2020 article on the most cited research articles, all mentioned machine learning research was conducted by, or in collaboration with, OpenAI, Microsoft, and Alphabet (Kingma & Ba, 2014;Ren et al., 2015;Mnih et al., 2015;Vaswani et al., 2017;Silver et al., 2016;Crew, 2020), suggesting a need for resources not available within academia.While the involvement of these companies, whose R&D budgets exceed those of most nations (Bajpai, 2021;Ballard, 2021;Uis, 2022), contributed greatly to the advancement of machine learning, a potential technological dependency of academia on this commercially driven progress combined with a brain-train from academia to industry may result in a narrowing of machine learning research (Jurowetzki et al., 2021;Klinger et al., 2020;Hagendorff & Meding, 2020).In the following paragraph we will discuss these developments and the concerns raised with a focus on applied machine learning in chemistry and biology."}
{"paper_id": 411, "introduction": "Explicit or implicit regularization is a key component for achieving better performance in deep learning.For instance, adding some regularization on the local sharpness of the loss surface is one common approach to enable the trained model to achieve better performance (Hochreiter & Schmidhuber, 1997;Foret et al., 2021;Jastrzebski et al., 2021).In the related literature, some recent studies have empirically reported that gradient regularization (GR), i.e., adding penalty of the gradient norm to the original loss, makes the training dynamics reach flat minima and leads to better generalization performance (Barrett & Dherin, 2021;Smith et al., 2021;Zhao et al., 2022).Using only the information of the first-order gradient seems a simple and computationally friendly idea.Because the first-order gradient is used to optimize the original loss, using its norm is seemingly easier to use than other sharpness penalties based on second-order information such as the Hessian and Fisher information (Hochreiter & Schmidhuber, 1997;Jastrzebski et al., 2021).Despite its simplicity, our understanding of GR has been limited so far in the following ways.First, we need to consider the fact that GR must compute the gradient of the gradient with respect to the parameter.This type of computation has been investigated in a slightly different context: input-Jacobian regularization, that is, penalizing the gradient with respect to the input dimension to increase robustness against input noise (Drucker & Le Cun, 1992;Hoffman et al., 2019).Some studies proposed the use of double backpropagation (DB) as an efficient algorithm for computing the gradient of the gradient for input-Jacobian regularization, whereas others proposed the use of finite-difference computation (Peebles et al., 2020;Finlay & Oberman, 2021).Second, theoretical understanding of GR has been limited.Although empirical studies have confirmed that the GR causes the gradient dynamics to eventually converge to better minima with higher performance, the previous work provides no concrete theoretical evaluation for this result.Third, it remains unclear whether the GR has any potential connection to other regularization methods.Because the finite difference is composed of both gradient ascent and descent steps by definition, we are reminded of some learning algorithms for exploring flat minima such as sharpness-aware minimization (SAM) (Foret et al., 2021) and the flooding method (Ishida et al., 2020), which are also composed of ascent and descent steps.Clarifying these points would help to deepen our understanding on efficient regularization methods for deep learning.In this work, we reveal that GR works efficiently with a finite-difference computation.This approach has a lower computational cost, and surprisingly achieves better generalization performance than the other computation methods.We present three main contributions to deepen our understanding of GR:\u2022 We demonstrate some advantages to using the finite-difference computation.We give a brief estimation of the computational costs of finite difference and DB in a deep neural network and show that the finite difference is more efficient than DB (Section 3.2).We find that a so-called forward finite difference leads to better generalization than a backward one and DB (Section 3.3).Learning with forward finite-difference GR requires two gradients of the loss function, gradient ascent and descent.A relatively large ascent step improves the generalization.\u2022 We give a theoretical analysis of the performance improvement obtained by GR. we analyze the selection of global minima in a diagonal linear network (DLN), which is a theoretically solvable model.We prove that GR has an implicit bias for selecting desirable solutions in the so-called rich regime (Woodworth et al., 2020) which would potentially lead to better generalization (Section 4.2).This implicit bias is strengthened when we use forward finitedifference GR with an increasing ascent step size.In contrast, it is weaken for a backward finite difference, i.e., a negative ascent step.\u2022 Finite-difference GR is also closely related to other learning methods composed of both gradient ascent and descent, that is, SAM and the flooding method.In particular, we reveal that the flooding method performs finite-difference GR in an implicit way (Section 5.2).Thus, this work gives a comprehensive perspective on GR for both practical and theoretical understanding."}
{"paper_id": 412, "introduction": "Machine learning (ML) models are becoming powerful and can be used as a feature extractor to generate representations (also known as embeddings) for the input data (He et al., 2016;Sandler et al., 2018;Huang et al., 2017).However, such representations are still in high-dimensional space (e.g., 512 dimensions for the ResNet-18).To better understand the representations of data or demonstrate the model's performance, people usually use dimension reduction techniques such as t-SNE (van der Maaten & Hinton, 2008) (abbreviation for t-distributed stochastic neighbor embedding) to reduce high-dimensional representations into a 2-dimensional space for visualization.Despite being powerful, ML models are also shown to be vulnerable to various privacy attacks that aim to reveal the sensitive information of the training dataset given access to the target model.Property inference attack (Ganju et al., 2018;Zhou et al., 2022;Mahloujifar et al., 2022) is one of the representative attacks whereby the adversary aims to infer the sensitive global properties of the training dataset (e.g., the race distribution) from the representations generated from the target model.t-SNE plots, on the other hand, are condensations of data representations.Such plots are usually considered to be safe and shared with the public via scientific papers or blogs.However, it is unclear whether such plots would leak sensitive property information about the data as well.Our Work.In this paper, we take the first step toward understanding the privacy leakage from t-SNE plots through the lens of the property inference attack against such plots.Here, we consider the general property as the macro-level information of the dataset, e.g., race distributions.Note that this property is not necessarily related to the t-SNE plots, for instance, the t-SNE plot is used to show how distinguishable the gender distribution is, and the adversary can infer the race distribution of the data used to generate the plot.A successful attack may cause severe consequences as it provides additional information to the adversary, which is often sensitive and should be protected.Also, it can be used to audit the fairness of the dataset (Buolamwini & Gebru, 2018).In this work, we first systematize the threat model and attack methodology, which is straightforward.We assume that the adversary can access the victim's t-SNE plot and may have knowledge of the distribution of the target dataset, but they do not necessarily have access to the target model.To infer the general property of the samples in the t-SNE plot, the adversary first samples groups of images from a shadow dataset with different properties (e.g.different proportions of males).Then, those groups of images will be used to query the shadow model to get representations and generate groups of t-SNE plots.An attack model is trained based on the <plot, property> pairs, which is an image classifier that can distinguish between t-SNE plots with different labels.Once well trained, the attack model can then be used to infer the property of public t-SNE plots.Our evaluations on both classification and regression tasks show that the proposed attack can achieve high accuracy, on some of the datasets like CelebA and LFW.For instance, the accuracy for predicting the proportion of males on CelebA t-SNE plots reaches 0.92, and the average regression error of predicting precise proportions is 0.02.We also study the reason for the relatively poor attack performance on the other datasets (e.g.FairFace) or attributes (e.g.Oval Face).We discover that this is due to the less distinguishable representations generated by the target model over these datasets/attributes.Also, we observe that the attack is still effective even when the shadow model is different from the target model.We further demonstrate that our attack is robust with fewer training plots and transferable to different t-SNE density settings.For instance, the regression attack model trained on t-SNE plots with 750 sample points can reach a low error of 0.04 on t-SNE plots with 1, 000 or 500 sample points.We additionally show that, by mixing only a small number of t-SNE plots from a new dataset, our attack can transfer to the new dataset.and we reveal the validity of our attack when the target and shadow models are fine-tuned, which is common in practice (see Section 5.5 for details).To mitigate the attack, we perturb the image embeddings/t-SNE coordinates and discover that it indeed reduces the attack performance to a large extent, e.g., by adding Gaussian noise to the t-SNE coordinates, the inference error of regression attack model increases significantly from 0.02 to 0.48.However, such defenses can still be bypassed by an adaptive attacker (see also Section 5.4).In short, our work demonstrates that the published t-SNE plots can be a valid side-channel for the property inference attacks.We appeal to our community's attention to the privacy protection of publishing t-SNE plots."}
{"paper_id": 413, "introduction": "Backdoor attack, also known as Trojan horse attacks, has become an increasing security threat in recent years, attracting many research interests (Chen et al., 2017;Doan et al., 2021).The attack aims to inject a backdoor into a deep model so that the model behaves normally on benign samples, while its predictions are maliciously and consistently changed to a predefined target class (or classes) when the backdoors are activated.Currently, poisoning training samples is the most straightforward and widely adopted method for performing backdoor attacks.Depending on whether the labels of the poisoned samples are changed or not, existing backdoor attacks can be roughly divided into poison-label backdoor attacks (Gu et al., 2017;Barni et al., 2019;Nguyen & Tran, 2020;Liu et al., 2020;Qi et al., 2021) and clean-label backdoor attacks (Turner et al., 2019;Saha et al., 2020).In this work, we follow the practice of poison-label backdoor attacks, which are much more efficient than clean-label ones.To perform a poison-label backdoor attack, it first selects a small number (a smaller number means a smaller impact on the performance of the benign data and a lower probability of being inspected by the developer) of benign samples from the training set, inserts a predefined trigger pattern into the inputs, and changes their labels to the target class (or classes).It then reinserts the poisoned samples into the training set and provides the resulting data to the victims to train the model with.In attack mode, it manipulates the victim model to produce the intended target class by injecting the predefined trigger pattern into the inputs.Existing backdoor attacks typically use a pattern that rarely occurs in benign data as the trigger pattern.For example, Gu et al. (2017) used stickers and checkerboards as trigger patterns for image data, Sun (2020) used special characters, words, and phrases as trigger patterns for text data.This strategy can prevent the attack from being falsely activated on benign data and mitigate the impact of the attack on the label predictions of benign samples.However, it also results in low-effort detection and mitigation of the attack by preventing activation of the trigger pattern.As far as we know, most existing backdoor attack defense algorithms build on this property and achieve great success (Chen et al., 2019;Wang et al., 2019;Zhao et al., 2020;Xu et al., 2020;Yang et al., 2021;Huang et al., 2022a;Mu et al., 2022).In this work, we introduce a new attack strategy to solve this dilemma.Instead of using a rare pattern as the trigger pattern, we extract the trigger pattern from benign data that frequently occurs in samples of the target class but rarely occurs in samples of non-target classes.Accordingly, we introduce a framework to extract the trigger pattern and insert it into benign inputs to perform data poisoning.The proposed strategy has two advantages over the prevailing one.First, it is more efficient since the trigger pattern comes from the target class.Training on benign samples of the target class will help the fitting of the trigger pattern.Second, it becomes harder to detect and defend against the attack because many benign samples of the target class contain the trigger pattern.It is difficult to distinguish the poisoned samples from benign samples of the target class using the trigger pattern, and disabling the activation of the trigger pattern will degrade performance of these benign target class samples.To evaluate the benefits of our proposed strategy, we conducted experiments on four widely studied datasets.The empirical results show that attacks performed with our strategy can achieve better attack performance with less poisoned data especially when the poisoned data size is small.Moreover, they can escape the defenses of several benchmark defense algorithms, while attacks performed with the conventional strategies often fail to do so.The main contributions of this work can be summarized in three points.1) A new attack strategy for designing the trigger pattern is proposed.2) An effective technique for extracting the trigger pattern and injecting it into benign data is proposed.3) Experiments are conducted on four widely studied datasets, and the results show that our strategy can improve both attack efficiency and stealthiness."}
{"paper_id": 414, "introduction": "Since the rising of modern deep learning, the contradiction between ever-increasing model size and limited deployment resources has persisted.For this reason, compression technologies are crucial for practical deep learning and have been widely studied, including model quantization (Gong et al., 2014;Wu et al., 2016;Vanhoucke et al., 2011;Gupta et al., 2015), network pruning (Han et al., 2015;2016;He et al., 2017), knowledge distillation (Hinton et al., 2015;Xu et al., 2018;Chen et al., 2018;Yim et al., 2017;Zagoruyko & Komodakis, 2017), lightweight architecture design (Howard et al., 2017;Sandler et al., 2018;Zhang et al., 2018b;Ma et al., 2018), and low-rank decomposition (Denton et al., 2014;Lebedev et al., 2015;Jaderberg et al., 2014;Lebedev & Lempitsky, 2016).As a compression approach that extremely reduces the bit-width to 1-bit, network binarization is regarded as the most aggressive quantization technology (Rusci et al., 2020;Choukroun et al., 2019;Qin et al., 2022;Shang et al., 2022b;Zhang et al., 2022b;Bethge et al., 2020;2019;Martinez et al., 2019;Helwegen et al., 2019).The binarized models leverage the most compact 1-bit parameters, which take little storage and memory and accelerate the inference by efficient bitwise operations.Compared to other compression technologies like network pruning and architecture design, network binarization enjoys stronger topological generics since it only applies to parameters.Therefore, in academic research, network binarization is widely studied as an independent compression technique instead of the 1-bit specialization of quantization (Gong et al., 2019;Gholami et al., 2021).It is impressive that State-of-The-Art (SoTA) binarization algorithms push binarized models to fullprecision performance on large-scale tasks (Deng et al., 2009;Liu et al., 2020).However, existing network binarization is still far from practical.We point out that two worrisome trends are emerging from accuracy and efficiency perspectives in current binarization research: Figure 1: Evaluation tracks of BiBench.Our benchmark evaluates binarization algorithms on the most comprehensive evaluation tracks, including \"Learning Task\", \"Neural Architecture\", \"Corruption Robustness\", \"Training Consumption\", \"Theoretical Complexity\", and \"Hardware Inference\".Trend-1.Accuracy comparison converging to limited scope.In recent binarization research, several image classification tasks, e.g., CIFAR-10 and ImageNet, are becoming standard options for comparing accuracy.The typical selection of evaluation tasks helps the clear and fair comparison of accuracy performance among different binarization algorithms.However, since most binarization algorithm studies are engineered for learning tasks with image modality inputs, the presented insights and conclusions are rarely verified in a broader range of other modalities and tasks.The monotonic tasks also hinder the comprehensive evaluation from an architectural perspective.Besides, data noise like corruption is a common problem on low-cost edge devices and is widely studied in compression (Lin et al., 2018;Rakin et al., 2021), whereas few advanced binarization algorithms consider the robustness of binarized models.Trend-2.Efficiency analysis remaining at the theoretical level.Network binarization is widely recognized for its significant storage and computation savings.For example, theoretical savings are up to 32\u00d7 and 64\u00d7 for convolutions, respectively (Rastegari et al., 2016;Bai et al., 2021).However, since lacking support from hardware libraries, the models compressed by binarization algorithms can hardly be evaluated on real-world edge hardware, leaving their efficiency claims lacking experimental evidence.In addition, the training efficiency of the binarization algorithm is usually neglected in current research, which causes several negative phenomena in training a binary network, such as the increasing demand for computation resources and time consumption, being sensitive to hyperparameters, and requiring detailed tuning in optimization, etc.In this paper, we present BiBench, a network Binarization Benchmark to evaluate binarization algorithms comprehensively from accuracy and efficiency perspectives (Table 1).Based on BiBench, we benchmark 8 representative binarization algorithms on 9 deep learning datasets, 13 different neural architectures, 2 deployment libraries, 14 hardware chips, and various hyperparameter settings.It costs us about 4 GPU years of computation time to build our BiBench, devoted to promoting comprehensive evaluation for network binarization from the perspectives of accuracy and efficiency.Furthermore, we analyze the benchmark results in depth and reveal insights along evaluation tracks, and give suggestions for designing practical binarization algorithms."}
{"paper_id": 415, "introduction": "The ability of a Deep Neural Network (DNN) to generalize to new data is mainly restricted to priorly known concepts in the training dataset.In real-world scenarios, Machine Learning (ML) models may encounter Out-Of-Distribution (OOD) samples, such as data belonging to novel concepts (classes) (Pimentel et al., 2014), abnormal samples (Tishby & Zaslavsky, 2015), or even carefully crafted attacks designed to exploit the model (Szegedy et al., 2013).The behavior of ML systems on unseen data is of great concern for safety-critical applications (Amodei et al., 2016b;a), such as medical diagnosis in healthcare (Subbaswamy & Saria, 2020), autonomous vehicle control in transportation (Bojarski et al., 2016), among others.To address safety issues arising from the presence of OOD samples, a successful line of work aims at augmenting ML models with an OOD binary detector to distinguish between abnormal and in-distribution examples (Hendrycks & Gimpel, 2017).An analogy to the detector is the human body's immune system, with the task of differentiating between antigens and the body itself.Distinguishing OOD samples is challenging.Some previous works developed detectors by combining scores at the various layers of the multi-layer pre-trained classifier (Sastry & Oore, 2020; Lee et al., 2018;Gomes et al., 2022;Huang et al., 2021).These detectors require either a held-out OOD dataset (e.g., adversarially generated or OOD data) or ad-hoc methods to linearly combine OOD scores computed on each layer embedding tightened to a particular architecture.A key observation is that existing aggregation techniques overlook the sequential nature of the underlying problem and, thus, limit the discriminative power of those methods.Indeed, an input sample passes consecutively through each layer and generates a highly correlated signature that can be statistically characterized.Our observations in this work motivate the statement that:The input's trajectory through a network is key for discriminating typical from atypical samples.In this paper, we introduce a significant change of perspective.Instead of looking at each layer score independently, we cast the scores into a sequential representation that captures the statistical trajectory of an input sample through the various layers of a multi-layer neural network.To this end, we adopt a functional point of view by considering the sequential representation as curves parametrized by each layer.Consequently, we redefine OOD detection as detecting samples whose trajectories are abnormal (or atypical) compared to reference trajectories characterized by the training set.Our method, which requires little parameter tuning and, perhaps more importantly, no additional OOD or synthetic data, can identify OOD samples from their trajectories.Furthermore, we show that typical multivariate detection methods fail to detect OOD patterns, which may manifest in an isolated fashion by shifting in magnitude or overall shape.Figure 1 summarizes our method.Contributions.This work presents a new principle and unsupervised method for detecting OOD samples that do not require OOD (or extra) data and brings novel insights into the problem of OOD detection.Our main contributions can be summarized as follows.1.A novel problem formulation.We reformulate the problem of OOD detection through a functional perspective that effectively captures the statistical dependencies of an input sample's path across a multi-layer neural classifier.Moreover, we propose a map from the multivariate feature space (at each layer) to a functional space that relies on the probability weighted projection of the test sample onto the class conditional training prototypes at the layer.It is computationally efficient and straightforward to implement.Computing OOD scores from trajectories.We compute the inner product between the test trajectory and the average training trajectories to measure the similarity of the input w.r.t the training set.Low similarity indicates that the test sample is likely sampled from OOD.3. Empirical evaluation.We validate the value of the proposed method using a mid-size OOD detection benchmark on ImageNet-1k.We obtain competitive results, demonstrating an average ROC gain of 3.7% across three architectures and four OOD datasets.We release our code anonymized online."}
{"paper_id": 416, "introduction": "Figure 1: Super-resolution of a squirrel using Bicubic upsample, OTS (ours) and DASR (Wei et al., 2021) methods (4\u00d74 upsample, 370\u00d7800 crops).The problem of image super-resolution (SR) is to reconstruct a high-resolution (HR) image from its low-resolution (LR) counterpart.In many modern deep learning approaches, SR networks are trained in a supervised manner by using synthetic datasets containing LR-HR pairs (Lim et al., 2017, 4.1); (Zhang et al., 2018b, 4.1).For example, it is common to create LR images from HR with a simple downscaling, e.g., bicubic (Ledig et al., 2017, 3.2).However, such an artificial setup barely represents the practical setting, in which the degradation is more sophisticated and unknown (Maeda, 2020).This obstacle suggests the necessity of developing methods capable of learning SR maps from unpaired data without considering prescribed degradations.Contributions.We study the unpaired image SR task and its solutions based on Generative Adversarial Networks (Goodfellow et al., 2014, GANs) and analyse them from the Optimal Transport (OT, see (Villani, 2008)) perspective.1. We investigate the GAN optimization objectives regularized with content losses, which are common in unpaired image SR methods ( 5,4).We prove that the solution to such objectives is always an optimal transport map.We theoretically and empirically show that such maps are biased ( 7.1), i.e., they do not transform the LR image distribution to the true HR image distribution.2. We provide an algorithm to fit an unbiased OT map for perceptual transport cost ( 6.1) and apply it to the unpaired image SR problem ( 7.2).We establish connections between our algorithm and regularized GANs using integral probability metrics (IPMs) as a loss ( 6.2).Our algorithm solves a minimax optimization objective and does not require extensive hyperparameter search, which makes it different from the existing methods for unpaired image SR.At the same time, the algorithm provides a nearly state-of-art performance in the unpaired image SR problem ( 7.2).Notation.We use X , Y to denote Polish spaces and P(X ), P(Y) to denote the respective sets of probability distributions on them.We denote by \u03a0(P, Q) the set of probability distributions on X \u00d7 Y with marginals P and Q.For a measurable map T : X \u2192 Y, we denote the associated push-forward operator by T # .The expression \u2225 \u2022 \u2225 denotes the usual Euclidean norm if not stated otherwise.We denote the space of Q-integrable functions on Y by L 1 (Q)."}
{"paper_id": 417, "introduction": "As a commonly-used data format describing the real world, point clouds-based representations preserve more geometric information residing in 3D scenes, and have become one of the most important data types for 3D scene perception and real applications such as robotics (Rusu et al., 2008;Rusu & Cousins, 2011), autonomous driving (Sun et al., 2020;Shi et al., 2020), and augmented and virtual reality (Tredinnick et al., 2016), giving a better understanding of the surrounding environment for machines.In recent years, point clouds-based vision tasks (Shi et al., 2020) have achieved great progress on the public benchmarks (Vishwanath et al., 2009;Chang et al., 2015;Dai et al., 2017), which largely owes to the fact that the collected point clouds are carefully annotated, sufficiently large, and low level noised.But in the real world, acquiring such data from a new target domain and manually labeling these extensive 3D data are highly dependent on professionals in this filed, which makes the data acquisition and annotation more difficult, labor-intensive, and time-consuming.One effective solution to transfer the model from fully-labeled source domain to a new domain without extra human labor is Unsupervised Domain Adaptation (UDA) (Shen et al., 2022;Zou et al., 2021;Fan et al., 2022;Yang et al., 2021), whose purpose is to learn a more generalizable representation between the labeled source domain and unlabeled target domain, such that the model can be adapted to the data distribution of the target domain.For example, when point cloud data distribution from the target domain undergoes serious geometric variances (Shen et al., 2022), performing a correct source-to-target correspondence can boost the model's adaptability.Besides, GAST (Zou et al., 2021) learns a domain-shared representation for different semantic categories, while a vot-ing reweighting method is designed (Fan et al., 2022) that can assign reliable target domain pseudo labels.However, these techniques are highly dependent on the accessibility of the target domain data, which is a strong assumption and prerequisite for the models running in an unprecedented circumstance, such as autonomous driving system and medical scenarios.Thus, it is meaningful and important to investigate the model's cross-domain generalization ability under the zero-shot target domain constraint, which derivates the task of Domain Generalization (DG) for 3D scenario.However, achieving such zero-shot domain adaptation, i.e., DG, is more challenging in 3D scenario mainly due to the following reasons.(1) Unknown Domain-variance Challenge: 3D point cloud data collected from different sensors or geospatial regions with different data distributions often present serious domain discrepancies.Due to the inaccessibility of the target domain data (or sensor), modeling of source-to-target domain variance is intangible.(2) Uneven Domain Adaptation Challenge: Considering that our goal is to learn a transferable representation that can be generalized to multiple target domains, a robust model needs to perform an even domain adaptation, rather than lean to fit the data distribution on one of the multiple target domains.But for 3D point cloud data with more complex sample-level modality variances, how to ensure an even model adaptation under the zero-shot target domains setting still remains challenging.To tackle the above challenges, we study the typical DG problem in 3D scenario, and introduce a Singe-dataset Unified Generalization (SUG) framework for addressing the 3D point cloud generalization problem.We study a one-to-many domain generalization problem, where the model can be trained on only a single 3D dataset, and is required to be simultaneously generalized to multiple target datasets.Different from previous DG works in 2D scenarios (Shankar et al., 2018;Piratla et al., 2020;Chen et al., 2021), 3D point cloud data have a more irregular data structure and diverse data distribution within a single dataset, which provides the possibility to exploit the modality and sub-domain changes without accessing any target-domain datasets.To be specific, our SUG framework consists of a Multi-grained Sub-domain Alignment (MSA) method and a Sample-level Domain-aware Attention (SDA) strategy.To address the unknown domain-variance challenge, the MSA method first splits the selected single dataset into different sub-domains.And then, based on the splitted different sub-domains from a single dataset, the baseline model is constrained to simulate as many domain variances as possible from multi-grained features, so that the baseline model can learn multi-grained and multi-domains agnostic representations.To solve the uneven domain adaptation challenge, the SDA is developed, which assumes that the instances from different sub-domains often present different adaptation difficulties.Thus, we add sample-level constraints to the whole sub-domain alignment process according to the dynamically changing sample-level inter-domain distance, leading to an even inter-domain adaptation process.We conduct extensive experiments on several common benchmarks (Qin et al., 2019) under the single-dataset DG scenario, which includes three sub-datasets and our experiments cover the following three scenarios: 1) ModelNet-10\u2192ShapeNet-10/ScanNet-10, meaning that the model is only trained on ModelNet-10 and directly evaluated on both ShapeNet-10 and ScanNet-10; 2) ShapeNet-10\u2192ModelNet-10/ScanNet-10; 3) ScanNet-10\u2192ModelNet-10/ShapeNet-10.Experimental results demonstrate the effectiveness of SUG framework in learning generalizable features of 3D point clouds, and it can also significantly boost the DG ability for many selected baseline models.The main contributions of this paper can be summarized as follows:1) From a new perspective of one-to-many 3D DG, we explore the possibilities of adapting a model from its original source domain to many unseen domains, and study how to leverage the feature's multi-modal information residing in a single dataset.2) We propose a SUG to tackle the one-to-many 3D DG problem.The SUG consists of a designed MSA method to learn the domain-agnostic and discriminative features during the source-domain training phase, and a SDA strategy to calculate the sample-level inter-domain distance and balance the adaptation degree of different sub-domains with different inter-domain distances."}
{"paper_id": 418, "introduction": "A wide range of artificial intelligence applications and services rely on deep neural models because of their remarkable accuracy.When a trained model is deployed in production, its operation should be monitored for abnormal behavior, and a flag should be raised if such is detected.Corrective measures can be taken if the underlying cause of the abnormal behavior is identified.For example, simple distributional changes may only require retraining with fresh data, while more severe cases may require redesigning the model (e.g., when new classes emerge).In this paper we focus on distribution shift detection in the context of deep neural models and consider the following setting.Pretrained model f is given, and we presume it was trained with data sampled from some distribution P .In addition to the dataset used in training f , we are also given an additional sample of data from P , which is used to train a detector D (we refer to this as the detection-training dataset).While f is used in production to process a stream of emerging input data, we continually feed D with the most recent window W k of k input elements.The detector also has access to the final layers of the model f and should be able to determine whether the data contained in W k came from a distribution different from P .Detection algorithms based on a window, such as we consider here, have rarely been considered in the context of deep neural networks.To the best of our knowledge window-based deep detection has only been considered by (Rabanser et al., 2019).We emphasize that in this paper we are not considering the problem of identifying single-instance out-of-distribution or outlier instances (Liang et al., 2018;Hendrycks & Gimpel, 2017;Hendrycks et al., 2019;Golan & El-Yaniv, 2018;Ren et al., 2019;Nalisnick et al., 2019;Nado et al., 2021;Fort et al., 2021), but rather the information residing in a population of k instances.Single-instance methods are trivially applicable to a window.However, these methods are not designed to detect population-based changes (see discussion in Section 2).We also note that this paper does not address the issue of characterizing the type of distribution shift, nor correcting it (by \"redesigning\" the model to make accurate predictions on the shifted distribution).The detection of distribution shifts is a fundamental topic in machine learning and statistics, and the standard method for tackling it is by performing a dimensionality reduction over both the detectiontraining (source) and test (target) samples, and then applying a two-sample statistical test over these reduced representations to detect a deviation.This is further discussed in Section 2. Distribution shift detection has been scarcely considered in the context of deep neural networks (DNNs).Deep models can benefit from the semantic representation created by the model itself, which provides meaningful dimensionality reduction that is readily available at the last layers of the model.Using the embedding layer (or softmax) along with statistical two-sample tests was recently proposed by (Lipton et al., 2018) and (Rabanser et al., 2019) who termed solutions of this structure black-box shift detection (BBSD).Using both the univariate Kolmogorov-Smirnov (KS) test and the maximum mean discrepancy (MMD) method, see details below, (Rabanser et al., 2019) achieve impressive detection results when using MNIST and CIFAR-10 as proxies for the distribution P .As we demonstrate here, the KS-BBSD method is also very effective over ImageNet when a stronger model is used .BBSD methods have the disadvantage of being computationally intensive due to the use of two-sample tests between the detection-training set (which can, and are preferred to be the largest possible) and the window W (a complexity analysis is provided in table 1).We propose a different approach based on selective prediction (El-Yaniv & Wiener, 2010;Geifman & El-Yaniv, 2017), where a model quantifies its prediction uncertainty and abstains from predicting uncertain instances.First, we develop a method for selective prediction with guaranteed coverage.This method identifies the best abstaining threshold and coverage bound for a given pretrained classifier f , such that the resulting empirical coverage will not violate the bound with a high probability (when abstention is determined using the threshold).The guaranteed coverage method is of independent interest, and it is analogous to selective prediction with guaranteed risk (Geifman & El-Yaniv, 2017).Because the empirical coverage of such a classifier is highly unlikely to violate the bound if the underlying distribution remains the same, a systematic violation indicates a shift in distribution.To be more specific, given a detection-training sample S m , our coverage-based detection algorithm computes log 2 m tight generalization coverage bounds, which are then used to detect a distribution shift in a window W of test data.Due to its aggressive reduction of S m to O(log m) numbers, the proposed detection algorithm is extremely efficient in its computation requirements, unlike the baseline algorithms mentioned above, which follow the framework depicted in Figure 3 in Appendix 7.1.For example, consider the JFT-3B dataset (Zhai et al., 2021).Previous methods that require the processing of this set for each incoming window are infeasible, while our method allows one to summarize it with only 32 scalars.In a comprehensive empirical study, we compared our coverage-based detection algorithm with the best-performing BBSD baselines, including the KS approach of (Rabanser et al., 2019).All methods used the same underlying models (ResNet-18, ResNet-50 and EfficientNet) for a fair comparison.We simulated source distributions using both the CIFAR-10 and ImageNet databases.Distribution shifts were produced using various methods, beginning with simple noise and ending with adversarial examples.Based on these experiments, we can claim that our coverage-based detection method is significantly more powerful than the baselines across a wide range of test window sizes.To summarize, the contributions of this paper are: (1) A theoretically justified algorithm (Algorithm 1), that produces a coverage bound, which is of independent interest, and allows for the creation of selective classifiers with guaranteed coverage.(2) A theoretically motivated \"windowed\" detection algorithm (Algorithm 2), which detects a distribution shift over a window.(3) A comprehensive empirical study demonstrating significant improvements relative to existing baselines over a variety of datasets and architectures."}
{"paper_id": 419, "introduction": "Automatic generation of realistic assets enables content creation at a scale that is not possible with traditional manual workflows.It is driven by the growing demand for virtual assets in both the creative industries, virtual worlds, and increasingly data-hungry deep model training.In the context of automatic asset generation, 3D scene and layout generation plays a central role as much of the demand is for the types of real-world scenes we see and interact with every day, such as building interiors.Deep generative models for assets like images, videos, 3D shapes, and 3D scenes have come a long way to meet this demand.In the context of 3D scene and layout modeling, in particular auto-regressive models based on transformers enjoy great success.Inspired by language modeling, these architectures treat layouts as sequences of tokens that are generated one after the other and typically represent attributes of furniture objects, such as the type, position, or scale of an object.These architectures are particularly well suited for modeling spatial relationships between elements of a layout.For example, (Para et al., 2021) generate two-dimensional interior layouts with two transformers, one for furniture objects and one for spatial constraints between these objects, while SceneFormer (Wang et al., 2021) and ATISS (Paschalidou et al., 2021) extend interior layout generation to 3D.A key limitation of a basic autoregressive approach is that it only provides limited control over the generated scene.It enforces a sequential generation order, where new tokens can only be conditioned on previously generated tokens and in addition it requires a consistent ordering of the token sequence.This precludes both object-level conditioning, where generation is conditioned on a partial scene, e.g., an arbitrary subset of furniture objects, and attribute-level conditioning, where generation is conditioned on an arbitrary subset of attributes of the furniture objects, e.g., class or position of target objects.Most recently, ATISS (Paschalidou et al., 2021) partially alleviates this problem by randomly permuting furniture objects during training, effectively enabling object-level conditioning.However, attribute-level conditioning still remains elusive.We aim to improve on these results by enabling attribute-level conditioning, in addition to object-level conditioning.For example, a user might be interested to ask for a room with a table and two chairs, without specifying exactly where these objects should be located.Another example is to perform Figure 1: Motivation.Current autoregressive layout generators (A) provide limited control over the generated result, since any generated value (denoted by black triangles) can only be conditioned on values that occur earlier in the sequence (values that are given as condition are denoted with c).Our proposed encoder-decoder architecture (B) adds bidirectional attention through an encoder, allowing the model to look ahead, so that all values in the sequence can be given as condition.This enables conditioning on an arbitrary subset of objects or object attributes in a layout.In C1, C2 only the position of an object, shown as pink cuboid, is given as condition and COFS performs context-aware generation of the remaining attributes.In D1, only object types are provided as condition, and D2 adds the bed orientation to the condition.Note how the layout adapts to fit the updated condition.object queries for given geometry attributes.The user could specify the location of an object and query the most likely class, orientation, and size of an object at the given location.Our model thereby extends the baseline ATISS with new functionality while retaining all its existing properties and performance.The main technical difficulty in achieving attribute-level conditioning is due to the autoregressive nature of the generative model.Tokens in the sequence that define a scene are generated iteratively, and each step only has information about the previously generated tokens.Thus, the condition can only be given at the start of the sequence, otherwise some generation steps will miss some of the conditioning information.The main idea of our work is to allow for attribute-level conditioning using two mechanisms: (i) Like ATISS, we train our generator to be approximately invariant to object permutations by randomly permuting furniture objects at training time.This enables object-level conditioning since an arbitrary subset of objects can be given as the start of the sequence.To condition on a partial set of object attributes however, the condition is not restricted to the start of the sequence.Attributes that are given as condition follow unconstrained attributes that need to be generated.(ii) To give our autoregressive model knowledge of the entire conditioning information in each step, we additionally use a transformer encoder that provides cross-attention over the complete conditioning information in each step.These two mechanisms allow us to accurately condition on arbitrary subsets of the token sequence, for example, only on tokens corresponding to specific object attributes.In our experiments, we demonstrate four applications: (i) attribute-level conditioning, (ii) attributelevel outlier detection, (iii) object-level conditioning, and (iv) unconditional generation.We compare to three current state-of-the-art layout generation methods (Ritchie et al., 2019;Wang et al., 2021;Paschalidou et al., 2021) and show performance that is on par or superior on unconditional generation and object-level conditioning, while also enabling attribute-level conditioning, which, to the best of our knowledge, is currently not supported by any existing layout generation method."}
{"paper_id": 420, "introduction": "Since their proposal, neural networks are constantly evolving as they are being adapted for many diverse tasks.They have a tendency to become more complex and larger, since e.g.overparamatrization has proven to be highly beneficial.Training such large and complex neural networks usually requires a huge amount of (labeled) high-quality data.Since this amount of data is not available in all domains, transfer learning was proposed.The idea is to transfer the knowledge of a trained model from the so called source domain to a similar, related task in a target domain for which only a small amount of data exists.Usually, the transfer is considered successful if the model achieves high accuracy on the target domain.However, accuracy is not the only desired property of neural networks.Adversarial robustness is often equally important, especially in safety-critical domains.Some techniques applied in transfer learning (Shafahi et al., 2020;Chen et al., 2021) claim that they improve robustness of transfer learning.However, there is no study that directly compares these techniques to standard methods for improving robustness such as adversarial training or training with a (local) Lipschitz constant.We fill this gap by answering the following questions:1. Which training procedure results in the most robust source models?2. Is robustness preserved during target retraining?3. Does robust retraining on the target domain improve robustness? 4. Which training/target retraining provides models that are robust against distribution shifts? 5. Does transferability correlate with model robustness?To answer these questions, we use a popular transfer learning framework consisting of two parts (see Figure 1): a feature extractor f which extracts representations from the inputs and is trained on the source domain and a classifier h which maps extracted representations to predictions and is retrained on the target domain.We investigate and compare how different training procedures and target retraining techniques affect performance and robustness of this model.More specifically, we compare 10 training procedures that can be grouped in three categories.Category one consists of training methods that aim at achieving robustness by changing inputs, i.e. (1) training on clean inputs (ce), (2) randomly perturbed inputs (ceN) and (3) adversarially perturbed inputs (ceA), (4) supervised contrastive learning (con) (Khosla et al., 2020), (5) supervised contrastive learning based on (5) randomly perturbed inputs (conN) and ( 6) adversarially perturbed inputs (conA).The second category of training approaches consists of methods that change the latent space of the model to achieve robustness, i.e. (7) latent adversarial training (feA) (Singh et al., 2019), (8) adversarial representation loss minimization (feD) (Chen et al., 2021) and (9) a combination of supervised contrastive learning and adversarial representation loss minimization (conF).Our third category of methods uses constraints on the whole model to improve robustness.These constraints are realized by (10) training with a local Lipschitz constant (llc) (Huang et al., 2021).In order to analyze how target retraining affects model robustness we compare target retraining on (a) clean (R ce ), (b) randomly perturbed (R ceN ) and (c) adversarially perturbed inputs (R ceA ). Figure 1: Transfer learning framework consisting of a feature extractor f , classifier h S on the source domain and h T on the target domain.For input x, f (x) = z provides the features and h S (z) = h S (f (x)) or h T (z) = h T (f (x)) the output.Source training procedures is grouped in methods that change inputs (ce, ceN, ceA, con, conN, conA), methods that change the latent space (feD, feA, conF) and methods that constrain the whole model (llc).To provide a more complete picture of robustness we consider robustness certification, performance against a variety of attacks, and performance under distribution shift.Namely, we employ (i) randomized smoothing based certification and (ii) Fast gradient sign method (FGSM), (iii) Project Gradient Descent (PGD) and (iv) DeepFool (DF) attacks on the source domain and the target domain.In terms of distribution shift, we determine source and target accuracy under different shifts based on random noise, changes of contrast, and Gaussian Blur shift.Next, we investigate whether there is a correlation between transferability and model robustness.We compute a transferability metric and analyze it together with model robustness and zero-shot performance.For transferability quantification we use the H-score, proposed by (Bao et al., 2019) to quantify the usability of representations learned on a source domain for learning a target task.This battery of robustness tests can tell us when is adversarial robustness transferable.As we will show in Section 4, target models inherit robustness from the source models while target retraining has a minor impact.Our findings suggest that model robustness is transferable when source models are trained based on a procedure that enhances model robustness without being too focused on dataspecific adversarial examples."}
{"paper_id": 421, "introduction": "In recent years, deep neural networks (DNNs) have achieved state-of-the-art results in a large variety of tasks, including image recognition (Du, 2018), game playing (Mnih et al., 2013), protein folding (Jumper et al., 2021), and more.In particular, deep reinforcement learning (DRL) (Sutton & Barto, 2018) has emerged as a popular paradigm for training DNNs that perform complex tasks through continuous interaction with their environment.Indeed, DRL models have proven remarkably useful in robotic control tasks, such as navigation (Kulh\u00e1nek et al., 2019) and manipulation (Nguyen & La, 2019;Corsi et al., 2021), where they often outperform classical algorithms (Zhu & Zhang, 2021).The success of DRL-based systems has naturally led to their integration as control policies in safety-critical tasks, such as autonomous driving (Sallab et al., 2017), surgical assistance (Pore et al., 2021), flight control (Koch et al., 2019), and more.Consequently, the learning community has been seeking to create DRL-based controllers that simultaneously demonstrate high performance and high reliability; i.e., are able to perform their primary tasks while adhering to some prescribed properties, such as safety and robustness.An emerging family of approaches for achieving these two goals, known as constrained DRL (Achiam et al., 2017), attempts to simultaneously optimize two functions: the reward, which encodes the main objective of the task; and the cost, which represents the safety constraints.Current state-of-the-art algorithms include IPO (Liu et al., 2020), SOS (Marchesini et al., 2021b), CPO (Achiam et al., 2017), and Lagrangian approaches (Ray et al., 2019).Despite their success in some applications, these methods generally suffer from significant setbacks: (i) there is no uniform and human-readable way of defining the required safety constraints; (ii) it is unclear how to encode these constraints as a signal for the training algorithm; and (iii) there is no clear method for balancing cost and reward during training, and thus there is a risk of producing sub-optimal policies.In this paper, we present a novel approach for addressing these challenges, by enabling users to encode constraints into the DRL training loop in a simple yet powerful way.Our approach generates policies that strictly adhere to these user-defined constraints without compromising performance.We achieve this by extending and integrating two approaches: the Lagrangian-PPO algorithm (Ray et al., 2019) for DRL training, and the scenario-based programming (SBP) (Damm & Harel, 2001;Harel et al., 2012b) framework for encoding user-defined constraints.Scenario-based programming is a software engineering paradigm intended to allow engineers to create a complex system in a way that is aligned with how humans perceive that system.A scenario-based program is comprised of scenarios, each of which describes a single desirable (or undesirable) behavior of the system at hand; and these scenarios are then combined to run simultaneously, in order to produce cohesive system behavior.We show how such scenarios can be used to directly incorporate subject-matterexpert (SME) knowledge into the training process, thus forcing the resulting agent's behavior to abide various safety, efficiency and predictability requirements.In order to demonstrate the usefulness of our approach to safety-critical tasks, we used it to train a policy for performing mapless navigation (Zhang et al., 2017;Tai et al., 2017) for robotics by the Robotis Turtlebot3 platform.While common DRL-training techniques were shown to give rise to high-performance policies for this task (Marchesini & Farinelli, 2020), these policies are often unsafe, inefficient, or unpredictable, thus dramatically limiting their potential deployment in realworld systems (Marchesini et al., 2021a;b).Our experiments demonstrate that, by using our novel approach and injecting subject-matter expert knowledge into the training process, we are able to generate trustworthy policies that are both safe and high performance.To have a complete assessment of the resulting behaviors, we performed a formal verification analysis, following methods such as with (Katz et al., 2017;Liu et al., 2019), of various predefined safety properties that proved that our approach generates safe agents to deploy in any environment."}
{"paper_id": 422, "introduction": "Despite significant performance breakthroughs in recent years (Mnih et al., 2015;Silver et al., 2016;Berner et al., 2019, e.g.,), Deep Reinforcement Learning (DRL) policies can be brittle.Specifically, recent works have shown that DRL policies are vulnerable to adversarial attacks -adversarially manipulated inputs (e.g., images) of small magnitude can cause RL agents to take incorrect actions (Ilahi et al., 2022;Chen et al., 2019;Behzadan & Munir, 2017;Oikarinen et al., 2021;Lee et al., 2021;Chan et al., 2020;Bai et al., 2018).To counter such attacks, recent work has proposed a range of defense strategies including adversarial training (Oikarinen et al., 2021;Behzadan & Munir, 2018;Han et al., 2018), robust learning (Mandlekar et al., 2017;Smirnova et al., 2019;Pan et al., 2019), defensive distillation (Rusu et al., 2016), and adversarial detection (Gallego et al., 2019a;Havens et al., 2018;Gallego et al., 2019a).While these defense methods can be effective, each has its limitations; adversarial training and adversarial detection require specific knowledge about the attacker.Robust learning adds noise during agent training, which can degrade performance (Tsipras et al., 2019;Yang et al., 2020).Defensive distillation is typically unable to protect against diverse adversarial attacks (Carlini & Wagner, 2016;Soll et al., 2019).In this work, we explore an alternative defense strategy that exploits existing knowledge encoded in auxiliary task policies and known relationships between the policies.The key intuition underlying our approach is that existing task policies encode learnt low-level knowledge regarding the environment (e.g., possible observations, dynamics), whilst high-level specifications can provide guidance for transfer or generalization.Our approach is to leverage known and learnt relations between different policies as structural priors for an ensemble of policies; our hypothesis is that while a single task policy can be attacked, perturbing inputs such that multiple policies are negatively affected in a consistent manner is more difficult.Our framework, which we call Knowledge-based Policy Fusion (KPR), is partially inspired by the use of domain knowledge to address vulnerabilities to adversarial attacks in supervised learning (Melacci et al., 2021;G\u00fcrel et al., 2021;Zhang et al., 2022).In these works, domain knowledge is encoded as logical formulae over predicted labels and a set of features.A soft satisfiability score between < l a t e x i t s h a 1 _ b a s e 6 4 = \" B k V U r d O 1 K P H F 2 Y 9 0 P C J I a 3 l A p E Q = \" > A A A B 7 H i c b Z D L S s N A F I Z P 6 q 3 G W 9 W l m 8 E i u C q J S H U j F t 2 4 r G D a Q h v K Z D J p h 0 4 m Y W Y i l N B n c O N C E V e C r + L e j f g 2 T i 8 L b f 1 h 4 O P / z 2 H O O U H K m d K O 8 2 0 V l p Z X V t e K 6 / b G 5 t b 2 T m l 3 r 6 G S T B L q k Y Q n s h V g R T k T 1 N N M c 9 p K J c V x w G k z G F y P 8 + Y 9 l Y o l 4 k 4 P U + r H u C d Y x A j W x v I 6 H I u w W y o 7 F W c i t A j u D M q X H / Z F + v Z l 1 7 u l z 0 6 Y k C y m Q h O O l W q 7 T q r 9 H E v N C K c j u 5 M p m m I y w D 3 a N i h w T J W f T 4 Y d o S P j h C h K p H l C o 4 n 7 u y P H s V L D O D C V M d Z 9 N Z + N z f + y d q a j c z 9 n I s 0 0 F W T 6 U Z R x p B M 0 3 h y F T F K i + d A A J p K Z W R H p Y 4 m J N v e x z R H c + Z U X o X F S c a u V 0 1 u n X L u C q Y p w A I d w D C 6 c Q Q 1 u o A 4 e E G D w A E / w b A n r 0 X q x X q e l B W v W s w 9 / Z L 3 / A C r 2 k e 0 = < / l a t e x i t > ^< l a t e x i t s h a 1 _ b a s e 6 4 = \" X l z X + g U 0 4  A T P 1 q 3 1 a L 1 Y r 5 P S j D X t 2 Y U / s t 5 + A E v F k E Q = < / l a t e x i t > z < l a t e x i t s h a 1 _ b a s e 6 4 = \" X l z X + g U 0 4 5 8 n 4 n u T T l U 0 j U x p O G 0 = \" > A A A B 6 H i c b Z D J S g N B E I Z r 4 h b H L e r R S 2 M Q P I U Z E f U i B r 1 4 T M A s k A y h p 1 O T t O l Z 6 O 4 R Y s g T e P G g i F d 9 G O 9 e x L e x s x w 0 8 Y e G j / + v o q v K T w R X 2 n G + r c z C 4 t L y S n b V X l v f 2 N z K b e 9 U V Z x K h h U W i 1 j W f a p Q 8 A g r m m u B 9 U Q i D X 2 B N b 9 3 N c p r d y g V j 6 M b 3 U / Q C 2 k n 4 g F n V B u r f N / K 5 Z 2 C M x a Z B 3 c K + Y s P + z x 5 / 7 J L r d x n s x 2 z N M R I M 0 G V a r h O o r 0 B l Z o z g U O 7 m S p M K O v R D j Y M R j R E 5 Q 3 G g w 7 J g X H a J I i l e Z E m Y / d 3 x 4 C G S v V D 3 1 S G V H f V b D Y y / 8 s a q Q 7 O v A G P k l R j x C Y f B a k g O i a j r U m b S 2 R a 9 A 1 Q J r m Z l b A u l Z R p c x v b H M G d X X k e q k c F 9 6 R w X H b y x U u Y K A t 7 s A + H 4 M I p F O E a S l A B B g g P 8 A T P 1 q 3 1 a L 1 Y r 5 P S j D X t 2 Y U / s t 5 + A E v F k E Q = < / l a t e x i t > z < l a t e x i t s h a 1 _ b a s e 6 4 = \" a c K w p a q 3 H w 7 N n d / b N g D r t a q y 5 p c = \" >z < l a t e x i t s h a 1 _ b a s e 6 4 = \" X l z X + g U 0 4 5 8 n 4 n u T T l U 0X k e q k c F 9 6 R w X H b y x U u Y K A t 7 s A + H 4 M I p F O E a S l A B B g g P 8 A T P 1 q 3 1 a L 1 Y r 5 P S j D X t 2 Y U / s t 5 + A E v F k E Q = < / l a t e x i t > z < l a t e x i t s h a 1 _ b a s e 6 4 = \" X l z X + g U 0z < l a t e x i t s h a 1 _ b a s e 6 4 = \" X l z X + g U 0 4 5 8 n 4 n u T T l U 0 j U x p O G 0 = \" > A A A B 6 H i c b Z D J S g N B E I Z r 4 h b H L e r R S 2 M Q P I U Z E f U i B r 1 4 T M A s k A y h p 1 O T t O l Z 6 O 4 R Y s g T e P G g i F d 9 G O 9 e x L e x s x w 0 8 Y e G j / + v o q v K T w R X 2 n G + r c z C 4 t L y S n b V X l v f 2 N z K b e 9 U V Z x K h h U W i 1 j W f a p Q 8 A g r m m u B 9 U Q i D X 2 B N b 9 3 N c p r d y g V j 6 M b 3 U / Q C 2 k n 4 g F n V B u r f N / K 5 Z 2 C M x a Z B 3 c K + Y s P + z x 5 / 7 J L r d x n sX k e q k c F 9 6 R w X H b y x U u Y K A t 7 s A + H 4 M I p F O E a S l A B B g g P 8 A T P 1 q 3 1 a L 1 Y r 5 P S j D X t 2 Y U / s t 5 + A E v F k E Q = < / l a t e x i t > z < l a t e x i t s h a 1 _ b a s e 6 4 = \" I z D W F j l g V g n 8 a o r Y M u f g 3 f y C< l a t e x i t s h a 1 _ b a s e 6 4 = \" c e F 9 4 f P T I + t J U P 4 L s s q t m o i 6 5< l a t e x i t s h a 1 _ b a s e 6 4 = \" c e F 9 4 f P T I + t J U P 4 L s s q t m o i 6 5\u00ac < l a t e x i t s h a 1 _ b a s e 6 4 = \" E u u 5 N C K 3 n C 0 P j B 8 5 i y K L H J / f p e s = \" > A A A B 6 3 i c b Z D L S g M x F I b P e K 3 j r e r S T b A I r s q M i L o R i 2 5 c V r A X a I e S S T N t a J Ix c 9 2 N y a p o N I Q j r V u + V 5 i g g w r w w i n I 7 e d a p p g M s A 9 2 r I o s a A 6 y M a z j t C h d b o o i p V 9 0 q C x + 7 s j w 0 L r o Q h t p c C m r 2 e z 3 P w v a 6 U m O g 8 y J p P U U E k m H 0 U p R y Z G + e K o y x Q l h g 8 t Y K K Y n R W R P l a Y G H s e 1 x 7 B n 1 1 5 H u r H Z f + 0 f H L r l S p X M F E B 9 u E A j s C H M 6 j A D V S h B g T 6 8 A B P 8 O w I 5 9 F 5 c V 4 n p Q v O t G c P / s h 5 / w F r W p F + < / l a t e x i t > \u00ac < l a t e x i t s h a 1 _ b a s e 6 4 = \"w A E / w b A n r 0 X q x X q e l B W v W s w 9 / Z L 3 / A C r 2 k e 0 = < / l a t e x i t > ^< l a t e x i t s h a 1 _ b a s e 6 4 = \" J a< l a t e x i t s h a 1 _ b a s e 6 4 = \" / z J g T o 4 n Y E 0 F S P 6 7 4 y 6 f n   the predictions and given logic formulae is added to the objective to encourage the predictions to comply with the logical formulae.Even if part of the sample is corrupted by the adversary, the final output can be corrected by enforcing the domain knowledge rules.KPR extends this line of research to RL settings.Note that this extension is nontrivial; the auxiliary feature detectors used in supervised learning (G\u00fcrel et al., 2021;Zhang et al., 2022) do not capture temporal features, and more importantly, the consistency between the predictions and actions is not directly computable since the optimal actions for each state are unknown.We address these issues by using auxiliary task policies and their relationships.In practice, these policies could be the intermediate by-products of curriculum/hierarchical learning or obtained via direct training with sub-goals.By combining these policies with specified and learnt relations, we construct an ensemble of policies on the target task.KPR uses graph neural networks (GNNs) as a backbone, which enables natural incorporation of graph-based domain knowledge while retaining the flexibility to learn from interaction data.The ensemble is then fused in a simple parameter-less manner to obtain a new robust task policy (Figure 1).From a practical perspective, KPR has a key advantage: it is both policy and attack agnostic.Specifically, KPR can utilize any type of policy representation (e.g., neural networks, rule-based policies) as either the main task policy or an auxiliary task policy.In addition, KPR doesn't require knowledge of the specific attack or access to the adversarial environment.Our empirical results show that KPR results in more robust policies across multiple attacks compared to baselines in a representative selection of Atari Games (Bellemare et al., 2013) and a high-dimensional Robot Food Court Environment (RoFoCo).To summarize, this paper contributes Knowledge-based Policy Recycling (KPR), which leverages domain knowledge to defend against adversarial attacks in RL.Different from prior defense methods in reinforcement learning, such as adversarial training and robust learning, KPR is able to incorporate domain knowledge as structural prior and then learn flexible relations from interaction data.To the best of our knowledge, this is the first work to demonstrate that domain knowledge in the form of policies can be used to defend against adversarial attacks."}
{"paper_id": 423, "introduction": "Recently, multimodal pre-trained models such as CLIP Radford et al. (2021) have attracted much attention.By utilizing these pre-trained models, many works have achieved new progress in downstream tasks such as classification Zhang et al. (2020); Wei et al. (2022); Lee et al. (2022), semantic segmentation Xie et al. (2021b); Wang et al. (2021b), object detection Xie et al. (2021a); Wang et al. (2022a), speech recognition Baevski et al. (2020), etc.Although the CLIP model has strong generalization in open-world data, as mentioned in CLIP paper Radford et al. (2021), the ability to match image-text samples that are not in its training data distribution is still weak.The natural idea to alleviate this problem is to scale the training data to cover different data domains.However, it is infeasible to train infinite data with limited hardware at once.In this paper, trying to break this non-iterability, we explore the feasibility of continuously training the CLIP model through streaming data, a training paradigm that follows Continual Learning (CL) McCloskey & Cohen (1989).To simulate continual CLIP training, we randomly and evenly divide the training data (joint-dataset) into multiple sub-datasets and train the CLIP sequentially using these sub-datasets.For comparison with continual training, we train a CLIP additionally from scratch using the joint-dataset, which is named joint training, as the upper bound on the performance of the continuously trained model.Traditional supervised continual learning has been proven to suffer from catastrophic forgetting Rebuffi et al. (2017); Kirkpatrick et al. (2017).The model's performance on old tasks drops significantly as training phases rise.Recently, some work Ni et al. (2021b); Hu et al. (2021) has validated that self-supervised models like SimCLR Chen et al. (2020) and BarlowTwins Zbontar et al. (2021) do not suffer from severe catastrophic forgetting during continual training.Some works Madaan et al. (2021); Thai et al. (2021) conjecture that the reason is that the contrastive loss is not directly affected by the supervised signal, and the self-supervised framework does not have a SoftMax function to amplify the influence of labels.However, the performance of CLIP with a continual training setting overturns this hypothesis.There is a significant degradation of multimodal retrieval results with continual training compared with joint training (in Section 3 and 5).We name this phenomenon Cognitive Disorder (CD).Due to the vision and language encoders within the CLIP normalizing the representation to a unit vector through a dimension-based L 2 norm, which limits the diffusion of representation vectors length, we try to analyze the representation space variation of modal extractors from a spatial geometry perspective.By tracking the directional changes of the representation vectors in the continuously updated CLIP model (in Section 3), we explore and summarize the spatial variation of the modal encoders within the CLIP: the Intra-modal Rotation and Inter-modal Deviation.The intra-modal rotation refers to the representation space of the single-modal feature extractors (vision and language) within the CLIP that rotates around the center of the high-dimensional sphere, accompanied by a slow topology change during the continual CLIP training.The inter-modal deviation refers to the cognitive deviation of different modal extractors (vision and language) to the same entities during continuous training.Moreover, we empirically and theoretically demonstrate how intra-modal rotation and inter-modal deviation lead to cognitive disorder (in Section 3).To alleviate this cognitive disorder in continual CLIP training, we propose a simple yet effective framework Mod-X: Maintain off-diagonal information-matriX.Unlike contrastive loss Oord et al. (2018) only focuses on the proportion of positive and negative sample pairs.The Mod-X framework pays more attention to the distribution of off-diagonal information in the contrastive matrix.The similarity distribution on the off-diagonal illustrates the model's cognition of all entities on current data.By selectively aligning the off-diagonal information distribution of the contrastive matrixes constructed by the current and past models based on the recent training sample, Mod-X helps the model preserve the correct cognition of various old entities while fitting the current vision-language data during continual training.The evaluations in Experiments 5 with different scale and scope datasets show that our Mod-X framework helps the model not only better fits the newly trained data domain (in Section 5.3) but also maintains the multimodal cognitive ability between the current model and old model on the old data domain during the continual large-scale training (in Section 5.4).More technological details and evaluations have been shown in Section 4 and Section 5.In summary, our contributions are as follows:\u2022 We discuss the feasibility of training the CLIP model continuously through streaming data.Empirical experiments demonstrate that continual CLIP training leads to persistent performance degrades on multimodal retrieval.We name this Cognitive Disorder.\u2022 By introducing a series of tools to track the directional changes of the representation vectors in the continuously updated CLIP model, we explore and summarize the spatial variation of the modal encoders within the CLIP: 1) The Intra-modal Rotation 2) The Inter-modal Deviation.Furthermore, we empirically and theoretically demonstrate how intra-modal rotation and inter-modal deviation lead to cognitive disorder (in Section 3).\u2022 We propose a simple yet effective continual CLIP training framework Mod-X that alleviates CLIP's cognitive disorder by selectively aligning off-diagonal information in contrastive matrixes between the past and current models in continual training."}
{"paper_id": 424, "introduction": "Model-Based Reinforcement Learning (MBRL) is attractive because it tends to have a lower sample complexity compared to model-free algorithms like Soft Actor Critic (SAC) (Haarnoja et al. (2018)).MBRL agents function by building a model of the environment in order to predict trajectories of future states based off of imagined actions.An MBRL agent maintains an extensive history of its observations, its actions in response to observations, the resulting reward, and new observation in an experience replay buffer.The information stored in the replay buffer is used to train a single-shot dynamics model that iteratively predicts the outcomes of imagined actions into a trajectory of future states.At each time step, the agent executes only the first action in the trajectory, and then the model re-imagines a new trajectory given the result of this action (Nagabandi et al. (2018)).Yet, many real-world tasks consist in sequences of subtasks of arbitrary length accruing repetitive experiences, for example driving over a long straight and then taking a corner.Capturing the complete dynamics here requires longer sessions of continual learning.(Xie & Finn (2021))Optimization of the experience replay methodology is an open problem.Choice of size and maintenance strategy for the replay buffer both have considerable impact on asymptotic performance and training stability (Zhang & Sutton (2017)).From a resource perspective, the size and maintenance strategy of the replay buffer pose major concerns for longer learning sessions.The issue of overfitting is also a concern when accumulating similar or repetitive states.The buffer can become inundated with redundant information while consequently under-representing other important states.Indefinite training on redundant data can result in an inability to generalize to, or remember, less common states.Conversely, too small a buffer will be unlikely to retain sufficient relevant experience into the future.Ideally, a buffer's size would be the exact size needed to capture sufficient detail for all relevant states (Zhang & Sutton (2017)).Note that knowing a priori all relevant states is unfeasible without extensive exploration.Under review as a conference paper at ICLR 2023 We argue that these problems can be subverted by employing a strategy that avoids retaining experiences that the model already has sufficiently mastered.Humans seem to perform known actions almost unconsciously (e.g., walking) but they reflect on actions that lead to unanticipated events (e.g.walking over seemingly solid ice and falling through).Such is our inspiration to attempt to curate the replay buffer based on whether the experiences are predictable for the model.Through this work, we propose techniques to capture both common and sporadic experiences with sufficient detail for prediction in longer learning sessions.The approach comprises strategies for: i) determining reliable predictions of the dynamics model with respect to the imagined actions, ii) retaining only the unimaginable experiences in the replay buffer, iii) training further only when sufficient novel experience has been acquired, and iv) reducing the effects of catastrophic forgetting.These strategies enable a model to self-manage both its buffer size and its decisions to train, drastically reducing the wall-time needed to converge.These are critical improvements toward the implementation of effective and stable continual-learning agents.Our contributions can be summarized as follows: i) contributions towards the applicability of MBRL in continual learning settings, ii) a method to keep the replay buffer size to a minimum without sacrificing performance, iii) a method that reduces the training time.These contributions result in keeping only useful information in a balanced replay buffer even during longer learning sessions."}
{"paper_id": 425, "introduction": "Over the past decade, deep Neural Networks (NN) have achieved tremendous success in solving various real-world problems ("}
{"paper_id": 426, "introduction": "Reinforcement learning (RL) provides a powerful framework for automated decision-making.However, RL still requires significantly engineered reward functions for good practical performance.Imitation learning offers the instruments to learn policies directly from the demonstrations, without an explicit reward function.It enables the agents to learn to solve tasks from expert demonstrations, such as helicopter control (Abbeel et al., 2006;2007;Ng et al., 2004;Coates et al., 2008;Abbeel et al., 2008a;2010), robot navigation (Ratliff et al., 2006;Abbeel et al., 2008b;Ziebart et al., 2008;2010), and building controls (Barrett & Linder, 2015).The goal of imitation learning is to induce the expert policies from expert demonstrations without access to the reward signal from the environment.We divide these methods into two broad categories: Behavioral Cloning (BC) and Inverse Reinforcement Learning (IRL).Among IRL, adversarial imitation learning (AIL) induces expert policies by minimizing the distribution distance between expert samples and agent policy rollouts.Prior AIL methods model the reward function as a discriminator to learn the mapping from the state-action pair to a scalar value, i.e., reward (Ho & Ermon, 2016;Zhang et al., 2020;Fu et al., 2017).However, the discriminator in the AIL framework would easily find the differences between expert samples and agent-generated ones, even though some differences are minor.Therefore, the discriminator-based reward function would yield a sparse reward signal to the agent.Consequently, how to make AIL robust and efficient to use is still subject to research.Our AEAIL is an instance of AIL by formulating the reward function as an auto-encoder.Since auto-encoder reconstruct the full state, unlike traditional discriminator based AIL, our method will not overfit to the minor differences between expert samples and generated samples.In many cases, our reward signal provides richer feedback to the policy training process.Thus, our new method achieves better performance on a wide range of tasks.Our contributions are three-fold:\u2022 We propose the Auto-Encoding Adversarial Imitation Learning (AEAIL) architecture.It models the reward function as the reconstruction error of an auto-encoder.It focuses on the full-scale differences between the expert and generated samples, and less susceptible to the discriminator attending to minor differences.\u2022 Experiments show that our proposed AEAIL achieves the best overall performance on many environments compared to other state-of-the-art baselines.\u2022 Empirically, we justify that our AEAIL works under a wide range of distribution divergences and auto-encoders.And the major contributing factor of our method is the encodingdecoding process rather than the specific divergence or auto-encoder."}
{"paper_id": 427, "introduction": "Nowadays rapid development of video-capture devices has made videos become a mainstream information carrier (Hansen, 2004).People usually post videos accompanied with different color styles on social media (Kopf et al., 2012;Xu et al., 2014) to share daily life, express different emotions, and get more exposures (Yan et al., 2016;Zabaleta & Bertalm\u00edo, 2021).Thus, photorealistic video style transfer or automatic color stylization becomes popular in many mobile devices.Different from artistic style transfer (Gatys et al., 2016;Huang & Belongie, 2017), photorealistic video style transfer or automatic color stylization needs to replace color styles in original videos with one or multiple reference images and keep the outputs maintain \"photorealism\".The photorealism in style transfer refers to that stylization results should look like real photos taken from cameras without any spatial distortions or unrealistic artifacts.Moreover, algorithms need to run in realtime.Several popular algorithms have been proposed to conduct photorealistic style transfer for single image.DeepPhoto (Luan et al., 2017) incorporated semantic segmentation masks to guide style transfer and utilized a photorealism regularization term to reduce spatial distortions.PhotoWCT (Li et al., 2018) exploited whitening and coloring transforms (WCT (Li et al., 2017c)) to conduct arbitrary style transfer and used photorealistic smoothing to remove spatially inconsistent stylization.WCT 2 (Yoo et al., 2019) proposed a wavelet corrected transfer based on WCT to preserve structural information while stylizing images at the same time.PhotoNAS (An et al., 2020) proposed a neural architecture search framework for photorealistic style transfer and achieved impressive results.Although these algorithms can conduct style transfers in many scenarios, their stylization results still contain unpleasant artifacts or look unreal, and some algorithms need additional supports.In Figure 1 (a), given a content image which contains a tree in autumn and a style reference, previous state-ofthe-art algorithm WCT 2 (Yoo et al., 2019) will generate synthesized images with obvious structural artifacts.Besides, these algorithms conduct style transfer by matching the summary statistics of content features with style references completely, which will lead to unrealistic stylization as in Figure 1 (b).For photorealistic style transfer in videos, there are only very few existing algorithms that can only perform style transfer with constraints.MVStylizer (Li et al., 2020) need good stylization initilaization at the first frame and Xia's method (Xia et al., 2021) incorporates additional semantic masks for each frame in videos.These problems limit these methods' usage in many real applications.(Li et al., 2020) or semantic masks (Xia et al., 2021), to guide style transfer.In this paper, we aim to solve the problems listed above in photorealistic video style transfer.Different from previous algorithms which match summary statistics of content images to that of style references through whitening and coloring transformation (Li et al., 2018), adaptive instance normalization (An et al., 2020) and the Gram loss (Luan et al., 2017), we propose a style removal and restoration framework in a self-supervised manner to conduct arbitrary style transfer while keeping photorealism.Our motivation is that during photorealistic style transfer, if we can remove the style of image content without destroying image structures, we can recover its original style by using the content image both as style reference and stylization target.According to our experiences, artifacts produced by PhotoWCT (Li et al., 2018), WCT 2 (Yoo et al., 2019), and PhotoNAS (An et al., 2020) come from two parts: (1) the Gram loss; (2) whitening and coloring transformation (WCT (Li et al., 2017c)).In our method, we avoid using the Gram loss and train networks with the content loss only (Gatys et al., 2016).We improve the summary statistics matching scheme with decoupled instance normalization which can remove original image styles and add new styles for inputs without hurting image structures.Meanwhile, decoupled instance normalization does not match styles of reference images completely and avoid unrealistic stylization in Figure 1 (b).To keep temporal consistency in videos, we exploit optical flow estimation (Teed & Deng, 2020) and ConvLSTM (Shi et al., 2015a) to conduct consecutively style transfer.We summarize our contributions as follows:\u2022 In this paper, we propose a novel photorealistic video style transfer network called ColoristaNet, which can conduct color style transfer in videos without introducing painterly spatial distortions and inconsistent flickering artifacts.We put many videos in the supplementary material to compare with other state-of-the-art algorithms.\u2022 We propose decoupled instance normalization which works together with ConvLSTM (Shi et al., 2015a) to implement structure-preserving and temporally consistent feature transformation.The decoupled instance normalization decomposes style transfer into feature whitening and stylization, which can avoid unrealistic style transfer.\u2022 ColoristaNet can adapt color styles in videos consecutively with multiple different style references and runs faster than most of recent algorithms.Qualitative results and a user study show that our method outperforms other state-of-art algorithms in making a balance between good stylization results and photorealism.Besides, we also conduct extensive ablation studies whose results demonstrate the effectiveness of different modules and designs in ColoristaNet clearly.Neural style transfer algorithms (Gatys et al., 2016;Li et al., 2017b) have achieved great success in creating artistic images of high perceptual quality.Using neural representations to separate and recombine content and style of arbitrary images is widely investigated and adopted by researchers (Li et al., 2017a;Zhu et al., 2017b;Johnson et al., 2016;Ledig et al., 2017).In Gatys' paper, the Gram matrix consists of the correlations between different filter responses and describe the overall image style, and features in deeper layers is thought capturing the high-level content in term of objects and their arrangement.Then style transfer problems can be solved by matching summary statistics of content inputs to that of style references.However, although such a framework works quite well for artistic style transfer, it is not suitable for photorealistic style transfer.Because matching the summary statistics of content images with arbitrary style references will generate unpleasant artifacts or distortions.Photorealistic style transfer algorithms, such as DeepPhoto (Luan et al., 2017), PhotoWCT (Li et al., 2018), WCT 2 (Yoo et al., 2019), and PhotoNAS (An et al., 2020), focus on eliminating artifacts or distortions with additional smoothing term or other regularization terms.As shown in Figure 1, structural artifacts and unrealistic stylization is hard to be avoided.In this paper, we hold an assumption that in previous methods, it's the summary statistics matching scheme in learning objectives and feature transformation modules that lead to structural artifacts or unrealistic stylization.That means the Gram loss (Gatys et al., 2016), AdaIN (Huang & Belongie, 2017)] and WCT (Li et al., 2018) are problematic in photorealistic style transfer.To address these issues, we propose ColoristaNet with: (1) a self-supervised style transfer framework that avoids employing the Gram loss during training; and (2) a novel feature transformation module to substitute AdaIN or WCT to perform summary statistics matching.For the self-supervised style transfer framework, as shown in Figure 2, if we can remove the style of an image without hurting its structure, the style restoration problem becomes a fully supervised one.The reason why our idea works is because in the photorealistic setting, image structures are shared and unchanged during style transfer.The benefits of our self-supervised learning scheme come from two folds: (1) We avoid employing the Gram loss or other regularization loss functions that will result structural artifacts or blur effects;(2) Our learning targets are real photos which can ensure that the stylization results make a good balance between stylization and photorealism.We discuss more details in Appendix B.1.To address the problems brought by AdaIN and WCT, ColoristaNet incorporates a novel feature transformation module called decoupled instance normalization (DecoupleIN) to match the styles of content images with that of reference images.DecoupledIN is inspired by AdaIN, and decompose the feature transformation into a style whitening step and a restylization step.This decomposition avoids forcing the feature statistics of content images to match that of style images directly, and show impressive results.In addition, as we conduct video style transfer, we need to keep temporal coherency in consecutive frames.So we employ optical flow methods to estimate pixel locations in a next frame and propagate style information through ConvLSTM (Shi et al., 2015a) as shown in Figure 3.We give more explanations about the design of ColoristaNet in Appendix B.2.For each frame, it firstly passes through a style transfer network to conduct style removal and then go through another style transfer network for style restoration.In style restoration, features from different time steps are connected with a ConvLSTM unit.A flow estimation network (RAFT (Teed & Deng, 2020) with fixed parameters) predicts optical flow between two adjacent frames to warp the hidden states of ConvLSTM for movement compensation.Note that parameters of style removal and restoration networks at different time steps are shared."}
{"paper_id": 428, "introduction": "Transformers have achieved remarkable progress in natural language processing (Devlin et al., 2019;Radford et al., 2019;Brown et al., 2020), computer vision (Dosovitskiy et al., 2020;Liu et al., 2021;Arnab et al., 2021) and audio processing (Gulati et al., 2020).As an important ingredient in transformers, positional encoding assigns a unique representation for each position of a token in a sequence so that the transformers can sense the position of input tokens.Among these encoding methods, absolute positional encoding (Vaswani et al., 2017;Sukhbaatar et al., 2015;Devlin et al., 2019;Liu et al., 2020) maps each individual position index into a continuous encoding.Whereas relative positional encoding (Shaw et al., 2018;Su et al., 2021;Horn et al., 2021;Liutkus et al., 2021;Huang et al., 2020;Raffel et al., 2019) generates encoding for each query-key pair, representing their relative positional offset.We focus on relative positional encoding as they are not constrained by input lengths (Chen, 2021) while showing superior performance (Shaw et al., 2018).Linear transformers Chen (2021); Qin et al. (2022); Su et al. (2021) attract more attention recently as they can achieve linear space-time complexity with respect to input sequence length, while maintaining comparable performance with vanilla transformers.Most existing linear transformers use absolute positional encoding methods to encode positional information, since most existing relative positional encoding methods are designed for vanilla transformers and are not directly applicable to linear transformers.The main cause behind this limitation is that linear transformers decompose key and value representations in the self-attention modules into separate kernel functions to achieve linear space-time complexity.Such an additional requirement on the decomposibility is not always satisfied by existing relative positional encoding methods.On the other hand, despite some individual works (Qin et al., 2022;Chen, 2021), general principles to design relative positional encoding for linear transformers remain largely under-studied.A recent work, RoPE Su et al. (2021) proposes a new set of multiplicative encoding solutions based on rotate positional encoding and can be applied to linear transformers.In Section C.7, we show that RoPE can be seen as a special form of LRPE.O(nd 2 + nd 2 )\u2248O(n) Q, K, and V are all in the shape of n by d, where n is input length and d is feature dimension.Tensors in the same dashed line box are associated for computation.In the vanilla relative positional encoding, query key attention has to be calculated first, leading to a quadratic complexity.W t-s refers to relative positional encoding, where t, s are two positional indices on the query and key, respectively.Our LRPE achieves a decomposable encoding, i.e., W t and W s are only dependent on positions of the query and key, making it fully compatible with linear transformers.When dealing with long sequences, d \u226a n, the computation complexity is dominated by n, rendering d negligible.In this work, we aim to bridge this gap and study principal framework to develop relative positional encoding applicable for both linear and vanilla transformers.To this end, we start by presenting a canonical form of relative positional encoding, which reveals that differences in existing encoding methods boil down to choices of a set of query, key and relative positional matrix primitives.By properly selecting and composing these primitives, we could derive various existing encoding methods for vanilla (Vaswani et al., 2017;Huang et al., 2020;Shaw et al., 2018) and linear (Qin et al., 2022) transformers.Taking advantage of the canonical form, we introduce the main contribution of our work, i.e., a special family of relative positional encoding methods called linearized relative positional encoding (LRPE).Specifically, we supply a sufficient condition to design compatible encoding methods specially for linear transformers and prove that the linearized relative positional encoding is unitary transformation.Benefits of using unitary transformation are two-fold.On one side, since it is derived from the decomposable positional matrix, it can maintain the linear space-time complexity as shown in Fig. 1.Second, the property of the unitary transformation allows us to effectively derive the family of closed-form solutions.In particular, we show that a number of encoding methods pertain to the LRPE family, including those used in RoPE (Su et al., 2021) and PermuteFormer (Chen, 2021).Furthermore, LRPE sheds light on a simple yet flexible theoretical paradigm to develop new effective relative positional encodings.To demonstrate this, we derive non-exhaustively three additional LRPE encoding methods by parameterizing the generic solution differently, including solutions living in either real or complex domains.Since unitary transformations are special cases of relative positional matrix, LRPE are applicable in both linear and vanilla transformers, and exclusively suitable within encoder and/or decoder layers.We experimentally demonstrate the effectiveness of the LRPE family on autoregressive and bidirectional language modelling, and on challenging downstream tasks, including machine translation and text classification.Results show that LRPE achieves competitive capability in representing relative positional information, commonly resulting in superior performance than previous encoding methods.In summary, our main contributions are three-fold:\u2022 We present a canonical form of relative positional encoding, which derives most existing relative positional encoding methods as its special case, including those used in linear and vanilla transformers.\u2022 Based on the canonical form, we propose linearized relative position encoding (LRPE), a simple yet principal formulation to derive an encoding family that respect the linear spacetime complexity in linear transformers, while being also applicable to vanilla transformers.We show several existing relative positional encoding methods in linear transformers are in LRPE family.We also provide additional particular solutions from this generic form.\u2022 Experiments on various downstream tasks, including language modeling, machine translation and text classification show that the LRPE family show more robust and commonly superior results across tasks than previous relative encoding methods, are flexible in be-ing plugged into linear/vanilla models, in encoder and/or decoder layers.In addition, it is generic to derive existing and potentially new encoding methods."}
{"paper_id": 429, "introduction": "RL is one of the most notable approaches to solving decision-making problems such as robot control (Hester et al., 2012;Ebert et al., 2018), traffic light control (Wei et al., 2018;Wu et al., 2020) and games (Mnih et al., 2015;Silver et al., 2017).The goal of RL is to find an optimal policy that maximizes expected return.To guarantee convergence of model-free RL, the assumption that each element in the joint state-action space should be visited infinitely often is required (Sutton & Barto, 2018), but this is impractical due to large state and/or action spaces in real-world problems.Thus, effective exploration has been a core problem in RL.In practical real-world problems, however, the given time for learning is limited and thus the learner should exploit its own policy based on its experiences so far.Hence, the learner should balance exploration and exploitation in the dimension of time and this is called exploration-exploitation trade-off in RL.The problem of explorationexploitation trade-off becomes more challenging in multi-agent RL (MARL) because the state-action space grows exponentially as the number of agents increases.Furthermore, the necessity and benefit of exploration can be different across agents and even one agent's exploration can hinder other agents' exploitation.Thus, the balance of exploration and exploitation across multiple agents should also be considered for MARL in addition to that across the time dimension.We refer to this problem as multi-agent exploration-exploitation trade-off.Although there exist many algorithms for better exploration in MARL (Mahajan et al., 2019;Kim et al., 2020;Liu et al., 2021a;Zhang et al., 2021), the research on multi-agent exploration-exploitation trade-off has not been investigated much yet.In this paper, we propose a new framework based on entropy regularization for adaptive exploration in MARL to handle the multi-agent exploration-exploitation trade-off.The proposed framework allocates different target entropy across agents and across time based on our newly-proposed metric for the benefit of further exploration for each agent.To implement the proposed framework, we adopt the method of disentanglement between exploration and exploitation (Beyer et al., 2019;Han & Sung, 2021) to decompose the joint soft value function into two types: one for the return and the other for the entropy sum.This disentanglement alleviates instability which can occur due to the updates of the temperature parameters.It also enables applying value factorization to return and entropy separately since the contribution to the reward can be different from that to the entropy from an agent's perspective.Based on this disentanglement, we propose a metric for the desired level of exploration for each agent, based on the partial derivative of the joint value function of pure return with respect to (w.r.t.) policy action entropy.The intuition behind this choice is clear for entropy-based exploration: Agents with higher gradient of joint pure-return value w.r.t.their action entropy should increase their target action entropy resulting in higher exploration level in order to contribute more to pure return.Under the constraint of total target entropy sum across all agents, which we will impose, the target entropy of agents with lower gradient of joint pure-return value w.r.t.their action entropy will then be reduced and inclined to exploitation rather than exploration.Thus, multi-agent exploration-exploitation trade-off can be achieved.The experiments demonstrate the effectiveness of the proposed framework for multi-agent exploration-exploitation trade-off."}
{"paper_id": 430, "introduction": "The rapid progress of reinforcement learning (RL) algorithms enables trained agents to navigate around complicated environments and solve complex tasks.The standard reinforcement learning methods, however, may fail catastrophically in another environment, even if the two environments only differ slightly in dynamics (Farebrother et al., 2018;Packer et al., 2018;Cobbe et al., 2019;Song et al., 2019;Raileanu & Fergus, 2021).In practical applications, such mismatch of environment dynamics are common and can be caused by a number of reasons, e.g., model deviation due to incomplete data, unexpected perturbation and possible adversarial attacks.Part of the sensitivity of standard RL algorithms stems from the formulation of the underlying Markov decision process (MDP).In a sequence of interactions, MDP assumes the dynamic to be unchanged, and the trained agent to be tested on the same dynamic thereafter.To model the potential mismatch between system dynamics, the framework of robust MDP is introduced to account for the uncertainty of the parameters of the MDP (Satia & Lave Jr, 1973;White III & Eldeib, 1994;Nilim & El Ghaoui, 2005;Iyengar, 2005).Under this framework, the dynamic of an MDP is no longer fixed but can come from some uncertainty set, such as the rectangular uncertainty set, centered around a nominal transition kernel.The agent sequentially interacts with the nominal transition kernel to learn a policy, which is then evaluated on the worst possible transition from the uncertainty set.Therefore, instead of searching for a policy that may only perform well on the nominal transition kernel, the objective is to find the worst-case best-performing policy.This can be viewed as a dynamical zero-sum game, where the RL agent tries to choose the best policy while nature imposes the worst possible dynamics.Intrinsically, solving the robust MDPs involves solving a max-min problem, which is known to be challenging for efficient algorithm designs.More specifically, if a generative model (also known as a simulator) of the environment or a suitable offline dataset is available, one could obtain a \u03f5-optimal robust policy with \u00d5(\u03f5 -2 ) samples under a rectangular uncertainty set (Qi & Liao, 2020;Panaganti & Kalathil, 2022;Wang & Zou, 2022;Ma et al., 2022).Yet the presence of a generative model is stringent to fulfill for real applications.In a more practical online setting, the agent sequentially interacts with the environment and tackles the exploration-exploitation challenge as it balances between exploring the state space and exploiting the high-reward actions.In the robust MDP setting, previous sample complexity results cannot directly imply a sublinear regret in general Dann et al. (2017) and so far no asymptotic result is available.A natural question then arises:Can we design a robust RL algorithm that attains sublinear regret under robust MDP with rectangular uncertainty set?In this paper, we answer the above question affirmatively and propose the first policy optimization algorithm for robust MDP under a rectangular uncertainty set.One of the challenges for deriving a regret guarantee for robust MDP stems from its adversarial nature.As the transition dynamic can be picked adversarially from a predefined set, the optimal policy may be randomized (Wiesemann et al., 2013).This is in contrast with conventional MDPs, where there always exists a deterministic optimal policy, which can be found with value-based methods and a greedy policy (e.g.UCB-VI algorithms).Bearing this observation, we resort to policy optimization (PO)-based methods, which directly optimize a stochastic policy in an incremental way.With a stochastic policy, our algorithm explores robust MDPs in an optimistic manner.To achieve this robustly, we propose a carefully designed bonus function via the dual conjugate of the robust bellman equation.This quantifies both the uncertainty stemming from the limited historical data and the uncertainty of the MDP dynamic.In the episodic setting of robust MDPs, we show that our algorithm attains sublinear regret O( \u221a K) for both (s, a) and s-rectangular uncertainty set, where K is the number of episodes.In the case where the uncertainty set contains only the nominal transition model, our results recover the previous regret upper bound of non-robust policy optimization (Shani et al., 2020).Our result achieves the first provably efficient regret bound in the online robust MDP problem, as shown in Table 1.We further validated our algorithm with experiments.Table 1: Comparisons of previous results and our results, where S, A are the size of the state space and action space, H is the length of the horizon, K is the number of episodes, \u03c1 is the radius of the uncertainty set and \u03f5 is the level of suboptimality.We shorthand \u03b9 = log(SAH 2 K 3/2 (1 + \u03c1)).The regret upper bound by Panaganti & Kalathil (2022) are obtained through converting their sample complexity results and the sample complexity result for our work is converted through our regret bound.We use \"GM\" to denote the requirement of a generative model.The superscript * stands for results obtained via batch-to-online conversion.The reference to the previous works are"}
{"paper_id": 431, "introduction": "SGD (Robbins & Monro, 1951) and its variant with momentum (Sutskever et al., 2013) are used widely in training deep neural networks.They perform well empirically and have theoretical guarantee (Szegedy et al., 2015;He et al., 2016;Lee et al., 2016;Hardt et al., 2016).However, SGD suffers from two issues.It often has slow convergence speed since it adopts a single learning rate for all the gradient coordinates.Moreover, it is also hard to tune the single learning rate (Wilson et al., 2017), since not all gradient coordinates share the same optimization properties.To resolve this problem, several adaptive gradient methods have been proposed to adopt different learning rate for different gradient coordinates.Typical examples of such methods include Adagrad (Duchi et al., 2011), RMSProp (Tieleman et al., 2012), andAdam (Kingma &Ba, 2014).Emprically, these methods have shown faster convergence speed and eased the burden of carefully tuning the learning rate in SGD across many kinds of networks.However, their generalization performance are often worse than SGD in many scenarios (Wilson et al., 2017;Zhou et al., 2020).Some algorithms are proposed to combine the fast convergence speed of adaptive gradient methods and good generalization performance of SGD.Instances of this type of algorithms include SWATS (Keskar & Socher, 2017) which automatically switchs from Adam to SGD, ND-Adam (Zhang et al., 2017) which utilizes vector learning rate and normalization to control direction and stepsize, and AMSGrad (Reddi et al., 2018) which maintains a monotone increasing second moment.Unfortunately, these methods only slightly bridge the generalization gap between SGD and Adam, but does not attain as good generalization performance as SGD, needless to say the state-of-the-art performance on test set.Accordingly, these algorithms are rarely used to train deep networks in practice.To combine the merits of Adam and SGD, i.e. fast convergence speed in Adam and excellent generalization in SGD, we proposed a Dimension-Reduced Adaptive Gradient Method (DRAG for short) which minimizes the loss from several descent directions to trade-off the whole space search in Adam and the minimization along a single gradient direction in SGD.For Adam, adjusting stepsizes for each gradient coordinate actually transforms the n-dimensional gradient into n independent directions to optimize, in which each direction inherits one coordinate element from the gradient and sets the remaining coordinate positions as zeros.In contrast, SGD only uses a single learning rate for all gradient coordinates and minimizes the loss along one descent direction.Though the adaptive learning rate for each coordinate shows faster convergence speed than a single learning rate for all coordinates, as shown in many works (Wilson et al., 2017;Zhou et al., 2018), it also leads to the inferior generalization in Adam, since minimizing n independent directions means searching the whole parameter space and could results in overfitting.So it is natural to trade-off the number of descent directions.To this end, motivated by DRSOM (Zhang et al., 2022), we update the parameters along the gradient direction and momentum direction through a trust-region-like approach, which greatly reduces the high adaptivity of Adam while adding flexibility to SGD.At each iteration, DRAG searches for the optimal update along the gradient and the momentum which are widely used in accelerated algorithm (Polyak, 1964;Nesterov, 2003) by solving a two-dimensional trust-region subproblem to find the best stepsizes for these two directions.For the trust-region subproblem, we use a quadratic approximation to estimate the loss with the Hessian matrix estimated by the second moment in Adam which is a diagonal matrix and can greatly reduce the computational cost.Moreover, we heuristically design a simple and effective trust-region radius for the subproblem.Despite the delicate design of our algorithm, we also theoretically prove that on non-convex problems, our DRAG can converge and enjoys a stochastic gradient complexity of O(\u03f5 -4 ) to find an \u03f5-approximate first-order stationary point.To summarize, our contributions are as follows:\u2022 We proposed the DRAG algorithm to optimize the loss from several descent directions for balancing the whole space search in Adam and the optimization along a single gradient direction in SGD.Moreover, we formulate the optimum stepsize search for these descent directions into a low-dimensional trust region problem whose computational cost is negligible when compared with the vanilla cost in adaptive gradient algorithms.\u2022 We theoretically prove that to find an \u03f5-approximate stationary point on non-convex stochastic problems, DRAG has the stochastic gradient complexity of O(\u03f5 -4 ) which matches the lower bound \u2126(\u03f5 -4 ) in (Arjevani et al., 2022) under the same non-convex optimization setting.\u2022 Experimental results show that on several representative benchmarks, our DRAG method can achieve faster convergence speed than SGD, and also state-of-the-art generalization performance."}
{"paper_id": 432, "introduction": "Anomalies indicate a departure of a system from its normal behaviour.In Industrial systems, they often lead to failures.By definition, anomalies are rare events.As a result, from a Machine Learning standpoint, collecting and classifying anomalies pose significant challenges.For example, when anomaly detection is posed as a classification problem, it leads to extreme class imbalance (data paucity problem).Morales-Forero & Bassetto (2019) have applied a semi-supervised neural network, a combination of an autoencoder and LSTM, to detect anomalies in the industrial dataset to mitigate the data paucity problem.Sperl et al. (2020) also tried to address the data imbalance issue of anomaly detection and applied a semi-supervised method to inspect large amounts of data for anomalies.However, these approaches do not address the problem completely since they still require some labeled data.Our proposed approach is to train models on a normal dataset and device some post-processing techniques to detect anomalies.It implies that the model tries to capture the normal behavior of the industrial device.Hence, no expensive dataset labeling is required.Similar approaches were tried in the past.Autoencoder-based family of models uses some form of thresholds to detect anomalies.For example, Sakurada & Yairi (2014); Jinwon & Ch (2015) mostly relied on reconstruction errors.The reconstruction error can be considered as an anomaly score.If the reconstruction error of a datapoint is higher than a threshold, then the datapoint is declared as an anomaly.However, the threshold value can be specific to the domain and the model, and deciding the threshold on the reconstruction error can be cumbersome.Unlike the above, our proposed quantile-based thresholds applied in the quantile-LSTM are generic and not specific to the domain or dataset.We have introduced multiple versions of the LSTM-based anomaly detector in this paper, namely (i) quantile-LSTM (ii) iqr-LSTM and (iii) Median-LSTM.All the LSTM versions are based on estimating the quantiles instead of the mean behaviour of an industrial device.For example, the median is 50% quantile.Our contributions are three-fold:(1) Introduction of Quantiles in design of quantile-based LSTM techniques and their application in anomaly identification.(2) Proposal of the Parameterized Elliot as a 'flexible-form, adaptive, learnable' activation function in LSTM, where the parameter is learnt from the dataset.We have shown empirically that the modified LSTM architecture with PEF performed better than the Elliot Function (EF) and showed that such behavior might be attributed to the slower saturation rate of PEF.PEF contributes to improved performance in anomaly detection in comparison to its non-parameterized siblings.(3) Evidence of superior performance of the proposed Long Short Term Memory networks (LSTM) methods over state-of-the-art (SoTA) deep learning and non-deep learning algorithms across multiple Industrial and Non-industrial data sets including Numenta Anomaly Benchmark and the VLDB anomaly benchmark (Appendix, Table 7, 8, 9 and10).There are three key pieces to modelling anomalies: type of time-series we need to work with; model the temporal dependency and post-process the forecasts to flag that forecast as an anomaly.Given the nature of anomalies, it is obvious they should model the departure normality or the tail behaviour.Quantities are the natural statistical quantities to consider in this respect.The temporal modeling of time-series models is some sort of dynamical systems, including the classical statistical models like ARMA and its variants.LSTMs are the most popular versions of the non-parametric non-linear dynamical models.One could technically swap LSTMs with any other sequence architectures suitable for the problem.The added advantage LSTMs brings is the multiplicative gates which help prevent vanishing gradients.This is coupled with the introduction of Parameterized Elliot as activation function (PEF) which shifts the saturation.A classifier to flag anomalies is also a comparator, either learnt via supervised task or is based on reasonable heuristics.For the former, we need labels which we assume do not have in large numbers in reality, For the latter, there is no option but to default to some heuristics.But thankfully, with a non-parametric, non-linear dynamical system such as q-LSTM modelling the quantities, even fixed, deterministic comparators turn out to be adaptive comparators.Therefore, we can consider our contribution as setting this template and making certain sensible choices in each of the three important puzzles of this template.The rest of the paper is organized as follows.The proposal and discussion of various LSTM-based algorithms are presented in section 2. Section 3 describes the LSTM structure and introduces the PEF.This section also explains the intuition behind choosing a parameterized version of the AF and better variability due to it.Experimental results are presented in section 4. Section 5 discusses relevant literature in anomaly detection.We conclude the paper in section 6."}
{"paper_id": 433, "introduction": "Graph augmentation is a crucial enabler for graph contrastive learning (GCL) (You et al., 2020;Qiu et al., 2020;Zhu et al., 2020).It pre-trains the model to yield instance-discriminative representations by contrasting augmented samples against each other, without hand-annotated labels.To achieve this goal, early studies (You et al., 2020;2021;Qiu et al., 2020;Zhu et al., 2020) conduct random corruptions in topological structures (i.e., nodes and edges) or attributes to construct contrastive pairs.However, such random corruptions, especially on salient substructures, easily cause a semantic gap between two augmented views of the same anchor graph, misguiding the following contrastive optimization procedure (Wang et al., 2021;Li et al., 2022).To mitigate this, there has been recent interest in rationale discovery (Chang et al., 2020;Suresh et al., 2021;Li et al., 2022) as graph augmentation.We systematize these studies as rationale-aware augmentations, where a rationale exhibits a graph's instance-discriminative information from the others.The dominant paradigm often consists of two subsequent modules: the rationale discovery function and the rationale encoder, which aim at creating the rationale-aware views and yielding their representations to contrast, respectively.To find rationales, early studies turn to domain knowledge to highlight the salient parts of graphs (Zhu et al., 2021;Liu et al., 2022).For instance, Rong et al. (2020) leverage RDkit (Landrum, 2010), an assistant software of chemistry, to capture crucial functional groups with high activity in molecule graphs.However, such expertise is expensive or even inaccessible in some scenarios (Tang et al., 2014).Besides, bringing in too much prior knowledge might harm generalization (Wang et al., 2022).To mitigate this problem, recent efforts (Suresh et al., 2021;Li et al., 2022) introduce an auxiliary model instead to automatically identify rationales, which is named the rationale generator and co-train with the rationale encoder.In this ad-hoc scheme, however, we reveal two inherent limitations:\u2022 Typically, the generator is tailor-made for one single transformation of graph data (Suresh et al., 2021;Li et al., 2022), forcing the focus on either node-or edge-wise rationales (e.g., Figures 1(b and 1(c)).The lack of view diversity confines the rationale-aware augmentations to one transformation, while leaving the cross-transformation untouched.Worse still, it might degenerate the effectiveness of contrastive learning, as the studies (Chen et al., 2020;You et al., 2020) empirically show that \"no single transformation suffices to learn good representations\".Here we ascribe this crux of generator to the lack of transformation diversity, and argue that a high-performing generator is supposed to be equipped with perspectives of both node and edge (e.g., Figure 1(d)).\u2022 As illustrated in Figure 2(a), the generator, aiming at discovering rationales, separates from the subsequent encoder, which specializes in encoding them.While conceptually appealing, we hypothesize that these separate modules cooperate with each other to pursue high-quality rationales unsmoothly.Because the supervision signal for the generator is remotely generated by the contrastive optimization of the encoder, much of which is weak.Moreover, co-optimizing two submodels could make the pre-training more complicated and time-consuming but less stable.To resolve these limitations, we draw inspiration from the transformers (Vaswani et al., 2017) to reshape the generator-encoder scheme.Despite originally being proposed for language (Devlin et al., 2019) and vision tasks (Dosovitskiy et al., 2021), transformers are attracting a surge of interest in graph area (Wu et al., 2021;Chen et al., 2022;Ramp\u00e1sek et al., 2022).At the core is the self-attention operation, which models pairwise connections between tokens and yields high-quality representations.We find self-attention de facto a natural mechanism to concurrently discover and condense rationale information from both edge-and node-wise transformations.By prepending a special token as its proxy and treating its nodes as other tokens, self-attention is able to elegantly indicate the importance of each node and each edge (See Section 2.2 and Figure 4).Sampling nodes and edges based on the importance scores (i.e., heterogeneous transformations) allows us to generate both the node-and edge-wise subgraphs (i.e., rationales) simultaneously.Moreover, self-attention can directly output the rationale representations without additional modules.In stark contrast to the prior generator-encoder scheme, the \"self-attentive rationalization\" not only accomplishes diverse rationales in one shot, but also integrates the functions of rationale discovery and encoding together.With the Self-attentive Rationalization as graph augmentation, we incorporate it into GCL and name the framework SR-GCL.Specifically, two augmented views stem from the node-and edge-wise rationales, respectively; subsequently, the contrastive optimization pulls close representations of contrastive pair augmented from the same anchor graph and pushes away those of different anchors by minimizing contrastive loss.Compared with conventional GCL methods, our SR-GCL collates and conflates the instance-discriminative information across both node-and edge-wise transformations to construct rationale-aware contrastive pairs from dual perpectives.Henceforth, we find that such strategy improves the generalization performance of pre-trained model on downstream tasks, while simultaneously interpreting the contribution of each node/edge to instance-discrimination. Extensive experiments show that SR-GCL sets the new state-of-the-art for graph pre-training across a number of biochemical molecule and social network benchmark datasets in (Wu et al., 2018a;Morris et al., 2020).Codes are available at https://anonymous.4open.science/r/SR-GCL-EDD3. Figure 3: SR-GCL framework, which constructs node-and edge-wise rationales (i.e., R v (g)/R e (g)) by sampling from corresponding probability distributions (i.e., P V (\u2022|G)/P E (\u2022|G)).Then contrastive optimization is performed to encourage the agreement between views of the same anchor."}
{"paper_id": 434, "introduction": "In recent years, decoding has gained in popularity in neuroscience (Kay et al., 2008), specifically decoding external variables (e.g.stimulus category) from internal states (i.e.brain activity).Such analyses can be useful for brain-computer interface (BCI) applications (Willett et al., 2021) or to gain neuroscientific insights (Guggenmos et al., 2018;Kay et al., 2008).Analysing deep learning methods on such data is also beneficial for the machine learning community.Namely, the small, noisy, high-dimensional datasets test the limits of popular architectures on real data and demand research into new methods (Zubarev et al., 2019;Kostas et al., 2021).Applications of decoding to brain recordings typically fit separate (often linear) models per dataset, per subject (Guggenmos et al., 2018;Dash et al., 2020b).This has the benefit that the decoding is tuned to the dataset/subject, but has the drawback that it is unable to leverage knowledge that could be transferred across datasets/subjects.This is especially desirable for the field of neuroimaging, because gathering more data is expensive and often impossible (e.g. in clinical populations).More practical drawbacks of subject-specific (subject-level) models include increased computational load, a higher chance of overfitting, and the inability to adapt to new subjects.We aim to leverage data from multiple subjects and train a shared model that can generalise across subjects (group-level).A conceptual visualisation of subject-level (SL) and group-level (GL) models is given in Figure 1.Magnetoencephalography (MEG) measures magnetic fields induced by electrical activity in the brain, and it is one of the main noninvasive brain recording methodologies, next to electroencephalography (EEG) and functional Magnetic Resonance Imaging (fMRI).Due to high temporal resolution and relatively good spatial resolution, MEG is an excellent method for studying the fast dynamics of brain activity.MEG is highly suitable for decoding analyses (Du et al., 2019), which is mostly done using SL models.This is because between-subject variability of neuroimaging data limits the application of a single shared model between subjects without capturing the structure of between-subject variability (Olivetti et al., 2014;Li et al., 2021).Such an approach, which we call naive group modelling,  effectively pretends that all data comes from the same subject (see Figure 1b).Between-subject variability has multiple sources, such as different anatomical structures, different positions in the scanner, signal-to-noise ratio, etc. (Saha & Baumert, 2020).To overcome this, we propose a general architecture capable of jointly decoding multiple subjects with the help of subject embeddings (Figure 2).The scope of this paper is full-epoch decoding, and comparisons with sliding-window decoding approaches often used in neuroscience are left for future work.To qualify how we aim to improve on SL models, we will next describe the two main approaches to evaluating decoding models, with different underlying assumptions and goals.One approach is to construct separate train and test splits for each subject that are made up of different, non-overlapping trials.This can be called within-subject splitting evaluation.SL models are evaluated by definition in this way, and it is a very common setup in the neuroscience literature (Guggenmos et al., 2018;Cooney et al., 2019b;Cichy & Pantazis, 2017;Dash et al., 2020b;a;Nath et al., 2020).In this work, our main aim is to improve over SL models in the context of within-subject splitting evaluation and improve the prediction of left-out trials, by using a single group decoding model that generalises across subjects.We call this GL method across-subject decoding.We are motivated by the fact that GL models that perform well in this manner can be useful for gaining neuroscientific insights that are relevant at the group level, as we will show in Sections 4.4 and 4.5.The other prominent approach to evaluating group models, leave-one-subject-out (LOSO) analysis, is also presented in Section 4.3.In this scenario, GL models are trained on data from multiple subjects and tested on a new, unseen subject (Zubarev et al., 2019), which can be especially useful in zero-shot BCI applications.Although in this case, we find no improvement using our embedding-aided group model, we think this may change with larger datasets with many more subjects.Our aim is to improve across-subject decoding of MEG data by using a group model that generalizes across subjects.To be clear this objective and the datasets we use are not related to any kind of direct BCI application.We make the following contributions using a MEG dataset with visual task (Cichy et al., 2016): 1.A GL model with subject embeddings is introduced, substantially improving over naive group modelling.2. Insight is provided into how non-linearity and subject embedding helps group modelling.3. Neuroscientific insights are gained from the deep learning-based decoding model.4. Analysis of model weights reveals how meaningful spatio-temporal and spectral information is encoded."}
{"paper_id": 435, "introduction": "Recently, deep learning is widely utilized in many research areas, such as computer vision, natural language processing, recommender systems, etc., but its success deeply depends on the large-scale labeled dataset for training the deep neural networks.The importance of the dataset is related to the generalization issue in deep learning, which refers that the model learned with the training dataset suffers from the degradation of performance when the unseen test dataset is encountered for deployment.This degradation results from the neural networks that are prone to overfitting under the lack of the training dataset (Keskar et al., 2016;Neyshabur et al., 2017;Kawaguchi et al., 2017).The dependency on the dataset also invokes an adaptive data selection by acquisition functions, or active learning, which aims at the efficient use of the limited budget for annotations from oracle (Cohn et al., 1996;Tong, 2001;Settles, 2009).Recently, various methods for active learning have been proposed; but the model trained with a small number of data from the adaptive selection is often difficult to be generalized (Dasgupta & Hsu, 2008).Although there exist some prior works that deal with the generalization issue in active learning; those methods solve the problem by either proposing a new risk function (Farquhar et al., 2020) or adopting a new classifier network (Wan et al., 2021), rather than by inventing a new acquisition function that considers the generalization.In this paper, we propose a new active learning algorithm, named Sharpness-Aware Active Learning (SAAL), that connects active learning and generalization ability to construct the acquisition function.Specifically, we are inspired by Sharpness-Aware Minimization, or SAM (Foret et al., 2020), which minimizes the maximally perturbed loss of training dataset, leading to minimizing the loss sharpness as well as the task loss, itself.Such optimization leads to a flat minima of the loss landscape, which is shown to have a strong correlation with the generalization performance (Jiang et al., 2019).Hence, SAAL adopts the maximally perturbed loss as the acquisition score.When calculating the acquisition score for SAAL, we cannot observe the labels for the unlabeled instances, so it is infeasible to compute the perturbed loss.To overcome this challenge, we utilize pseudo labels predicted by the current model, and we theoretically show that our proposed pseudo labeling conservatively estimates the maximally perturbed loss w.r.t.ground-truth label.Also, we theoretically derive the upper bound of the acquisition score of SAAL, which includes the loss, the norm of gradients, and the first eigenvalue of loss Hessian.Among the three terms of the upper bound, the loss and gradient terms are widely used metrics for active learning, which captures the model change by acquiring the instance (Yoo & Kweon, 2019;Ash et al., 2020;Settles et al., 2007).Meanwhile, the first eigenvalue, which is newly considered by SAAL, is connected to the loss sharpness (Keskar et al., 2017).Therefore, the selected instances by SAAL might contribute to the generalization of the model.We summarize our contributions in three points.First, we propose Sharpness-Aware Active Learning (SAAL), which considers the loss sharpness for constructing the acquisition function.The loss sharpness is related to the generalization of model, so selecting instances with a high value of loss sharpness might lead to a model with a better generalization performance.Second, we theoretically derive the upper bound of the acquisition score of SAAL and show the connection with the recent active learning methods.Specifically, we find that the upper bound also contains the first eigenvalue of loss Hessian, which is related to the generalization ability.Third, we empirically show that SAAL outperforms the baselines in various vision-based tasks on the benchmark dataset."}
{"paper_id": 436, "introduction": "Distance metric learning (DML) addresses the problem of finding an embedding function such that the semantically similar samples are embedded close to each other while the dissimilar ones are placed relatively apart in the Euclidean sense.Although the prolific and diverse literature of DML includes various architectural designs (Kim et al., 2018;Lin et al., 2018;Ermolov et al., 2022), loss functions (Musgrave et al., 2020), and data-augmentation techniques (Roth et al., 2020;Venkataramanan et al., 2022), many of these methods have a shared component: a convolutional neural network (CNN) followed by a global pooling layer, mostly global average pooling (GAP) (Musgrave et al., 2020).Common folklore to explain the effectiveness of GAP is considering each pixel of the CNN feature map as corresponding to a separate semantic entity.For example, spatial extent of one pixel can correspond to a \"tire\" object making the resulting feature a representation for \"tireness\" of the image.If this explanation is correct, the representation space defined via output of GAP is a convex combination of semantically independent representations defined by each pixel in the feature map.Although this folklore is later empirically studied in (Zeiler & Fergus, 2014;Zhou et al., 2016;2018, and references therein) and further verified for classification in (Xu et al., 2020), its algorithmic implications are not clear.If each feature is truly representing a different semantic entity, should we really average over all of them?Surely, some classes belong to the background and should be discarded as nuisance variables.Moreover, is uniform average of them the best choice?Aren't some classes more important than others?In this paper, we try to answer these questions within the context of metric learning.We propose a learnable and generalized version of GAP which learns to choose the subset of the semantic entities to utilize as well as weights to assign them while averaging.In order to generalize the GAP operator to be learnable, we re-define it as a solution of an optimization problem.We let the solution space to include 0-weight effectively enabling us to choose subset of the features as well as carefully regularize it to discourage degenerate solution of using all the features.Crucially, we rigorously show that the original GAP is a specific case of our proposed optimization problem for a certain realization.Our proposed optimization problem closely follows optimal transport based top-k operators (Cuturi et al., 2019) and we utilize its literature to solve it.Moreover, we present an algorithm for an efficient computation of the gradients over this optimization problem enabling learning.A critical desiderata of such an operator is choosing subset of features which are discrimantive and ignoring the background classes corresponding to nuisance variables.Although supervised metric learning losses provide guidance for seen classes, they carry no such information to generalize the behavior to unseen classes.To enable such a behavior, we adopt a zeroshot prediction loss as a regularization term which is built on expressing the class label embeddings as a convex combination of attribute embeddings (Demirel et al., 2017;Xu et al., 2020).In order to validate the theoretical claims, we design a synthetic empirical study.The results confirm that our pooling method chooses better subsets and improve generalization ability.Moreover, our method can be applied with any DML loss as GAP is a shared component of them.We applied our method on 6 DML losses and test on 4 datasets.Results show consistent improvements with respect to direct application of GAP as well as other pooling alternatives."}
{"paper_id": 437, "introduction": "The recent trend in machine learning to chase higher benchmark scores by adding additional parameters, has led to an explosive increase in the size of neural network architectures.A prime example of this phenomenon are the GPT models.While the first model in the family (Radford et al., 2018) has 117 million parameters, the latest model (Brown et al., 2020) already has a whopping 175 billion parameters which amounts to a > 1000 times increase.However, this explosive rise in parameters poses new problems.Training a single transformer model with a parameter count of 213 millon -still orders of magnitude smaller than GPT-3 -using Neural Architecture Search emits as much CO2 as five cars during their lifetime (Strubell et al., 2020).Furthermore larger models typically need specialised hardware and a lot of computing power for training and inference, which constrain the ability of the model to run on mobile devices, thus limiting powerful models to well-funded institutions.Finally, research has shown that these large models are typically overparameterized and encode a lot of redundant information that can be removed (Denil et al., 2013).To alleviate these issues, numerous approaches have been studied to scale down the number of parameters in a model, while still preserving (roughly) the same performance.This can be achieved by, e.g., designing parameter-efficient network structures (Sandler et al., 2018), or sparsifying existing neural network structures via pruning (le Cun, 1990;Hassibi & Stork, 1993;Han et al., 2015;Louizos et al., 2018;Molchanov et al., 2017).Until recently, it was thought to be difficult to train sparse neural networks from scratch (Evci et al., 2019), which was further strengthened by the finding that over-parameterized network architectures are proven to lead to an optimal global minimum when training (Zou & Gu, 2019).As such the classical way to reduce parameter count was via the train-prune-finetune loop, in which a model is first trained to completion, then redundant connections are pruned and finally the resulting network is finetunedThe recently introduced lottery ticket hypothesis (LTH) (Frankle & Carbin, 2019) challenged this notion and introduced a procedure to extract a sparse trainable network -a lottery ticket (LT)from a dense network.This is done using a pruning criterion in the form of the global weight magnitude in combination with a gradual pruning procedure.In this paper we study a refinement on this criterion by adding a notion of layerwise importance, which we introduce in section 2. We do this by considering a number of different weight rescaling methods, such that the comparisons are more calibrated between layers.Quantitative and qualitative comparisons between different importance measures and the baseline are reported in section 3.In addition, we shine light on the observable differences in the generated LTs (section 4), and determine how LTs emerge and differ when considering identical training conditions (section 5).A brief overview of related work is laid out in section 6 and finally, the paper is concluded in section 7.The key observations of our study are that: i) given a fixed weight initialization, it is possible to extract different lottery tickets that have similar performance, but differ significantly in their structure, ii) these tickets have a noticeable amount of common connections which have low-variance across tickets, and iii) these stable common connections survive the LTH procedure even when the other weights in the model are reinitialized.Together these observations suggest that these connections might be a promising avenue towards finding LTs more efficiently."}
{"paper_id": 438, "introduction": "Instance segmentation is the task of segmenting all objects in an image and assigning each of them a different id.It is the necessary first step to analyze individual objects in a scene and is thus of paramount importance in many computer vision applications.Over the recent years, fully supervised instance segmentation methods have made tremendous progress both in natural image applications and in scientific imaging, achieving excellent segmentations for very difficult tasks (Chen, Wang, and Qiao 2021;Lee et al. 2017).A large corpus of training images is hard to avoid when the segmentation method needs to take into account the full variability of the natural world.However, in many practical segmentation tasks the appearance of the objects can be expected to conform to certain rules that are known a priori.Examples include surveillance, industrial quality control and especially medical and biological imaging applications where full exploitation of such prior knowledge is particularly important as the training data is sparse and difficult to acquire: pixelwise annotation of the necessary instance-level groundtruth for a microscopy experiment can take weeks or even months of expert time.The use of shape priors has a strong history in this domain (Delgado-Gonzalo et al. 2014;Osher and Paragios 2007), but the most powerful learned shape models still require groundtruth (Oktay et al. 2018) and generic shapes are hard to combine with the CNN losses and other, non-shape, priors.For many high-level priors it has already been demonstrated that integration of the prior directly into the CNN loss can lead to superior segmentations while significantly reducing the necessary amounts of training data (Kervadec et al. 2019).However, the requirement of formulating the prior as a differentiable function poses a severe limitation on the kinds of high-level knowledge that can be exploited with such an approach.Our contribution addresses this limitation and establishes a framework in which a rich set of non-differentiable rules and expectations can be used to steer the network training.To circumvent the requirement of a differentiable loss function, we turn to the reinforcement learning paradigm, where the rewards can be computed from a non-differentiable cost function.We base our framework on a stateless actor-critic setup (Pfau and Vinyals 2016), providing one of the first practical applications of this important theoretical construct.In more detail, we solve the instance segmentation problem as agglomeration of image superpixels, with the agent predicting the weights of the edges in the superpixel region adjacency graph.Based on the predicted weights, the segmentation is obtained through (non-differentiable) graph partitioning.The segmented objects are evaluated by the critic, which learns to approximate the rewards based on object-and image-level reasoning (see Fig. 1).The main contributions of this work can be summarized as follows: (i) we formulate instance segmentation as a RL problem based on a stateless actor-critic setup, encapsulating the non-differentiable step of instance extraction into the environment and thus achieving end-to-end learning; (ii) we do not use annotated images for supervision and instead exploit prior knowledge on instance appearance and morphology by tying the rewards to the conformity of the predicted objects to pre-defined rules and learning to approximate the (non-differentiable) reward function with the critic; (iii) we introduce a strategy for spatial decomposition of rewards based on fixed-sized subgraphs to enable localized supervision from combinations of object-and image-level rules.(iv) we demonstrate the feasibility of our approach on synthetic and real images and show an application to two important segmentation tasks in biology.In all experiments, our framework delivers excellent segmentations with no supervision other than high-level rules."}
{"paper_id": 439, "introduction": "\"Divide et impera\".Distributed training arises as an important topic due to privacy constraints of decentralized data storage (McMahan et al., 2017;Kairouz et al., 2019).As the server-worker paradigm suffers from a single point of failure, there is a growing amount of works on training in the absence of server (Lian et al., 2017;Nedic, 2020;Koloskova et al., 2020b).We are particularly interested in decentralized scenarios where direct communication may be unavailable due to physical constraints.For example, devices in a sensor network can only communicate devices within short physical distances.Failures-from malfunctioning or even malicious participants-are ubiquitous in all kinds of distributed computing.A Byzantine adversarial worker can deviate from the prescribed algorithm and send arbitrary messages and is assumed to have the knowledge of the whole system (Lamport et al., 2019).It means Byzantine workers not only collude, but also know the data, algorithm, and models of all regular workers.However, they cannot directly modify the states on regular workers, nor compromise messages sent between two connected regular workers.Defending Byzantine attacks in a communication-constrained graph is challenging.As secure broadcast protocols are no longer available (Pease et al., 1980;Dolev & Strong, 1983;Hirt & Raykov, 2014), regular workers can only utilize information from their own neighbors who have heterogeneous data distribution or are malicious, making it very difficult to reach global consensus.While there are some works attempt to solve this problem (Su & Vaidya, 2016a;Sundaram & Gharesifard, 2018), their strategies suffer from serious drawbacks: 1) they require regular workers to be very densely connected; 2) they only show asymptotic convergence or no convergence proof; 3) there is no evidence if their algorithms are better than training alone.In this work, we study the Byzantine-robustness decentralized training in a constrained topology and address the aforementioned issues.The main contributions of our paper are summarized as follows:\u2022 We identify a novel network robustness criterion, characterized in terms of the spectral gap of the topology (\u03b3) and the number of attackers (\u03b4), for consensus and decentralized training, applying to a much broader spectrum of graphs than (Su & Vaidya, 2016a;Sundaram & Gharesifard, 2018).\u2022 We propose CLIPPEDGOSSIP as the defense strategy and provide, for the first time, precise rates of robust convergence to a O(\u03b4 max \u03b6 2 /\u03b3 2 ) neighborhood of a stationary point for stochastic objectives under standard assumptions. 1We also empirically demonstrate the advantages of CLIPPEDGOSSIP over previous works.\u2022 Along the way, we also obtain the fastest convergence rates for standard non-robust (Byzantine-free) decentralized stochastic non-convex optimization by using local worker momentum."}
{"paper_id": 440, "introduction": "Estimation of shortest-path (SP) distance lies at the heart of many network analysis tasks, such as centrality computation (Sch\u00f6nfeld & Pfeffer, 2021), node separation (Houidi et al., 2020), community detection (Zhang et al., 2020;Asif et al., 2022), which also directly contributes to enormous downstream applications, including point of interest (POI) search (Qi et al., 2020;Chen et al., 2021a) social relationship analysis (Carlton, 2020;Melkonian et al., 2021), biomedical structure prediction (Yue et al., 2019;Sokolowski & Wasserman, 2021), learning theory (Yang et al., 2021;Yuan et al., 2021), optimization (Rahmad Syah et al., 2021;Jiang et al., 2021b), etc. Nowadays, a key challenge of computing SP distance is the prohibitive complexity in very large and complex graphs.e.g., for a sparse undirected graph with N nodes and k queries, the time complexity of A* (Hart et al., 1968) and Dijkstra algorithm (Thorup & Zwick, 2004) are up to O(kN ) and O(kN log N ) for unweighted and weighted graph, respectively.Regarding this issue, various methods (Cohen et al., 2003;Fu et al., 2013;Akiba et al., 2013;Delling et al., 2014;Farhan et al., 2019;Liu et al., 2021) attempt answering exact distance in microseconds online via indexing or compressing techniques, which suffer huge storage costs on all pair SP distance representations and fail to reflect latent sub-structures in graphs for scalable queries (see Figure 1).Highly concise SP representation for large-scale and complex graphs remains to be studied yet.Regarding this, a surging number of approximate SP-representing algorithms that transform a graph into compact and low-dimensional representations are thus critical for fast and scalable online analysis.They can be categorized into oracle-based (Thorup & Zwick, 2004;Baswana & Kavitha, 2006), landmark-based (Potamias et al., 2009;Sarma et al., 2010;Gubichev et al., 2010) and learning-based (Rizi et al., 2018;Schl\u00f6tterer et al., 2019;Qi et al., 2020;Jiang et al., 2021a) SP representation methods.Among these categories, learning-based methods are of high accuracy and short response time (see Table 1), owing much to flexible node embeddings in a metric space.Figure 1: Differences between approximate (ours.)and exact (PLL (Akiba et al., 2013), efficient implementation of the hub-labeling method) SP representation methods regarding storage cost (megabytes, MB) and response time (nanoseconds, ns).We simulate a group of Bernoulli random graphs with |V | nodes, and each edge is filled independently with probability p.(a) and (c) show the storage cost of exact representations increases dramatically relative to the graph size.(b) and (d) reflect longer response time of exact methods, induced by random access to massive information.Table 1: Overall comparison of approaches to SP representation on DBLP dataset (A.8.4).PTC: preprocessing time complexity, PSC: preprocessing space complexity, RTC: response time complexity, TSC: the total storage cost for answering online distance queries, RT: real response time, AL: accuracy loss which is measured by mRE (see Equation 1).N : the number of nodes in the graph, L(N ): the average label size of each node which increases along with N , D: the amortized degree on each node.\u03b1 0 , L, n, d, w, l, c and \u03b2 are hyperparameters in corresponded models.Categories Method PTC PSC RTC TSC RT AL Hub-labeling PLL (Akiba et al., 2013) O(N 1+log L(N ) ) O(N L(N )) O( L(N )) 611.2 MB 2104.4 ns -Oracle-based ADO (Thorup & Zwick, 2004) O(\u03b10N 1+ 1 \u03b10 ) O(\u03b10N 1+ 1 \u03b10 ) O(\u03b10) 5, 980 MB 8, 598 ns 0.4985 Landmark-based LS (Potamias et al., 2009) O(lN ) O(lN ) O(l) 334.6 MB 12, 094 ns 0.3939 Learning-based Orion (Xiaohan et al., 2010) O(n 2 + nN ) O(dN ) O(d) 19.35 MB 82.25 ns 1.1897 Rigel (Xiaohan et al., 2011) O(n 2 + nN ) O(dN ) O(d + D) 35.37 MB 5, 657 ns 1.0662 DADL (Rizi et al., 2018) O((|L| + wl)N ) O(dN + c) O(d + D) 35.37 MB 7, 562 ns 0.2016 Path2Vec (Kutuzov et al., 2019) O((|L| + wl)N ) O(dN + c) O(d + D) 35.37 MB 7, 700 ns 0.6097 HALK (Schl\u00f6tterer et al., 2019) O((|L| + wl)N ) O(dN + c) O(d + D) 35.37 MB 7, 704 ns 0.3077 CatBoost-SDP (Jiang et al., 2021a) O((|L|N ) O(|L|N + c) O(|L|c) 44.16 MB 9, 270 ns 0.0890 BCDR (ours.)O((|L| + wl \u03b2 )N ) O(|L|N + c) O(|L|c + D) 39.19 MB 7, 247 ns 0.0798 BCDR-FQ (ours.)O((|L| + wl \u03b2 )N ) O(dN + c) O(d) 19.35 MB 58.82 ns 0.1840Several competitive works in learning-based methods (Rizi et al., 2018;Schl\u00f6tterer et al., 2019) heuristically leverage truncated random walk and optimization of node-cooccurrence likelihood on the arbitrary linkage to learn SP representations, which once achieved the state-of-the-art performance on approximation quality.However, they are not without limitations on efficiency and interpretability.On one side, a random walk is an unstrained node sequence from the root, possessing a limited exploration range of distance, thus resulting in uncaught distance relations with remote nodes.This is because each transition on nodes is not implied for a specific direction to move towards or beyond the root, especially after several walk steps, which restricts it from visiting remote nodes under limited walk steps (see Figure 2a).On the other side, the optimization on arbitrary linkage reflects excessively versatile local similarity among nodes, which preserves inaccurate distance relations from original graphs to the embedding space.In fact, it exerts a too-general metric over nodes' correlation, wherein the more edges or paths exist between two nodes, the stronger correlation they share.That means there are many ways to simulate a strong correlation for two nodes (e.g., add mutual edges, delete an edge to other nodes) even if some of the operations do not influence their actual SP distance (see Figures 2c and2d).A detailed statement of related works on SP representation and motivation for estimating accurate SP distance can be found in Appendix A.1.In this paper, we address the above shortcomings by proposing an efficient and interpretable SP representation method called Betweenness Centrality-based Distance Resampling (BCDR).It improves the approximation quality of SP representations with two components.The first is betweenness centrality (BC)-based random walk which explores a wider range of distance correlation on the graph due to its awareness of high-order path structures.To our best knowledge, there is no existing method that combines betweenness centrality and random walk to learn SP representations.We prove that BC-based transition is prone to jump out of local neighborhoods compared to random transition.The second is distance resampling which preserves accurate SP distance relations via implicitly decomposing an SP distance-based similarity matrix.In essence, it simulates the observation of random SPs from original walk paths and exerts desirable constraints on node representations to preserve distance relations over the graph.We summarize the major contributions as follows: i) We propose BC-based random walk as an efficient strategy for exploring a wider range of SP distance within limited walk steps (see Section 3.1).ii) We propose distance resampling to preserve accurate distance relations among nodes to learn an interpretable SP representation (see Section 3.2).iii) We evaluate BCDR with a broad class of real-world and synthetic graphs, and it yields an average improvement of 25% accuracy and 25-30% query speed compared to all existing methods (see Section 4)."}
{"paper_id": 441, "introduction": "Neural network training has gained a tremendous amount of attention in machine learning in recent years.This is primarily due to the success of backpropagation and stochastic gradient descent for firstorder optimization.However, there has also been a strong line of work on second-order optimization for neural network training; see [1] and references therein.While these second-order optimization methods (such as Newton's method and natural gradient descent) exhibit improved convergence rates and therefore require fewer training steps, they have two major limitations [2], namely (i) computing the curvature (or its inverse) for a large and deep neural network is computationally substantially more expensive than simply computing the gradient with backpropagation, which makes second-order methods practically inapplicable in most cases; (ii) networks trained with second-order information exhibit reduced generalization capabilities [3].In this work, we propose a novel method for incorporating second-order information of the loss function into training, while training the actual neural network with gradient descent.As loss functions are usually substantially cheaper to evaluate than a neural network, the idea is to apply second-order optimization to the loss function while training the actual neural network with first-order optimization.For this, we decompose the original iterative optimization problem into a two-stage iterative optimization problem, which leads to Newton losses.This is especially interesting for intrinsically hard-to-optimize loss functions, i.e., where second-order optimization of the inputs to the loss is superior to first-order optimization.Such loss functions have recently become increasingly popular, as they allow for solving more specialized tasks such as inverse rendering [4]- [6], learning-to-rank [7]- [13], self-supervised learning [14], differentiation of optimizers [15], [16], and top-k supervision [9], [11], [17].In this paper, we summarize these loss functions exceeding typical classification and regression under the umbrella of algorithmic losses [18] as they introduce algorithmic knowledge into the training objective.We evaluate the proposed Newton losses for various algorithmic losses on two popular benchmarks: the four-digit MNIST sorting benchmark and the Warcraft shortest-path benchmark.We found that Newton losses improve the performance in case of hard-to-optimize losses and maintain the original performance in the case of easy-to-optimize losses.Contributions.The contributions of this work are (i) introducing a mathematical framework for splitting iterative optimization methods into two-stage schemes, which we show to be equal to the original optimization methods; (ii) introducing Newton losses as combinations of first-order and second-order optimization methods; and (iii) an extensive empirical evaluation on two algorithmic supervision benchmarks using an array of algorithmic losses."}
{"paper_id": 442, "introduction": "Sequential prediction tasks have a wide range of applications in real-world, e.g., Online Transaction (Wang et al., 2017;Zhang et al., 2018;Weber et al., 2018;Tam et al., 2019;Zhu et al., 2020;Chen & Lai, 2021) and Sequential Recommendation (Quadrana et al., 2017;Tang & Wang, 2018;Sun et al., 2019;Shen et al., 2021;Cui et al., 2022), since sequences contain continuous signals which are important for model predictions.With the development of deep learning technique, sequence-based models have achieved a desirable performance in recent years (Hidasi et al., 2015;Quadrana et al., 2017;Wang et al., 2017;Tang & Wang, 2018;Zhang et al., 2018;Sun et al., 2019;Zhu et al., 2020;Qiao & Wang, 2022).However, the complicated sequential data and increased model complexity make it hard for humans to understand the prediction of models.Indeed, for security and trust considerations, it is essential to develop effective explainable artificial intelligence (XAI) methods for sequence-based models in scenarios like fraud detection and medical care, so that end-users could understand how model predictions are produced with these complicated sequential data and models.In recent years, considerable efforts have been made on the model explanation algorithms (Ribeiro et al., 2016;Shrikumar et al., 2017;Lundberg & Lee, 2017;Selvaraju et al., 2017;Wachter et al., 2017;Alvarez-Melis & Jaakkola, 2018;Mothilal et al., 2020;Slack et al., 2021;Ghalebikesabi et al., 2021;Ali et al., 2022).Among these works, feature attribution methods (Ribeiro et al., 2016;Shrikumar et al., 2016;2017;Lundberg & Lee, 2017) are a popular family of post-hoc XAI methods.They calculate an attribution score for each feature to capture those important features for model predictions.However, most existing methods mainly pay attention to explain tabular data or images.And when dealing with the data and models in sequential scenarios, the complex input sequences make the element-wise explanations produced by these methods less explainable.The high-dimensional features and abundant interactions bring difficulty to existing element-wise XAI methods to provide explanations.Separately assigning attribution scores to individual feature cells in the sequence is not informative enough for users to understand the predictions.In addition, the great amount of features in a sequence could bring an extensive execution cost for existing methods, since the time complexity of them are mostly related to the number of features to be explained.In this paper, we propose SeqSHAP, a Shapley value based method to explain model predictions in sequential scenarios.SeqSHAP provides explanations at a unique subsequence level, which is more intuitive in sequential scenarios for humans compared to the element-wise explanations.Meanwhile, we propose a distribution-based segmentation method to split the sequence into reasonable subsequences which utilizes the distribution information of sequential features.With obtained subsequences, we group the feature elements under each subsequence as independent units.Then Shapley value estimations for feature units are calculated, to capture the important features that strongly influence the model prediction.Extensive experiments on two large-scale online transaction datasets collected from real-world are carried out.We analyze the local explanations produced by SeqSHAP and prove that our method provides intuitive explanations with meaningful subsequences, compared to existing feature attribution methods in sequential scenarios.Our contribution could be summarized as follows:\u2022 We propose an effective XAI method to explain sequential predictions at a subsequence level, which is a unique and intuitive view in sequential scenarios.\u2022 We propose a distribution-based segmentation method characterizing the distribution information of sequential features to capture the context information and obtain reasonable subsequences.\u2022 Extensive experiments on two real-world transaction datasets are provided to evaluate the validity of our segmentation method and subsequence-level explanations produced by Se-qSHAP."}
{"paper_id": 443, "introduction": "Chinese Word Segmentation (CWS) is an essential step in the Chinese NLP processing pipeline.In languages like Chinese, there are no evident word boundaries in a sentence.Unlike English, sentences are naturally split into separate words by spaces in the text.When we do a Chinese NLP task, we are required to segment the sentence into words at first and then feed words into a downstream model.Although character-level models have achieved good results on many NLP tasks in recent years, many studies have shown that models incorporating word information can improve their performance Tian et al. (2020); Liu et al. (2021); Zhang & Yang (2018).So a good word segmentation model is significant for Chinese NLP tasks.Although the effect of the Chinese word segmentation model has been dramatically improved with the recent development of the pre-training model, there are still multitudinous problems in CWS.For example, the model usually performs poorly on difficult segmentation words, and the OOV words are one of them.OOV words are words for which the model was not taught how to segment them out in the training phase.Many studies show that the model often has a poorer segmentation effect for OOV words than common words.To alleviate and solve this problem to a certain extent, we propose the boundary-enhanced decoder (BED) in this paper.Our proposed method is inspired by the process of humans doing word segmentation tasks.The difficulty of word segmentation is different for each word in one sentence.For some words in a sentence, it is easy for us to figure out how to segment them, while others need repeated scrutiny and pondering to be correctly segmented.Therefore, when people perform word segmentation, they tend to preliminarily segment easy and relatively specific words' boundary, such as punctuation and some transition words, so that the sentence is roughly divided into large blocks.Then, the content in large blocks is more finely segmented.For example, when we segment the sentence in Figure 1, \"\u738b \u949f \u7ff0/\u8bba \u8bc1/\u540e \u91d1/\u5728/\u8fdb \u5165/\u8fbd \u6c88/\u524d/\uff0c/\u5df2/\u5c5e/\u5974\u96b6\u5236/\u793e\u4f1a/\u3002(Wang Zhonghan argues that Post-Jin had already belonged to a slave society before entering Liao and Shen.)\".We can split it into coarse-grained parts effortlessly, like the second layer in Figure 1.Blocks with a red background are words.All words can be determined in the first segmentation, except for the \"\u8bba\u8bc1\u540e\u91d1 (argues Post-Jin)\" part with yellow background needs to be more fine-grained segmentated.There are two ways to segment it, one way is \"{\u8bba\u8bc1, \u540e, \u91d1} ({argues, after, Jin})\" and another is \"{\u8bba\u8bc1, \u540e\u91d1}({argues, Post-Jin})\" whichFine-grained segmentation[Wang Zhonghan] 1 (Wang Zhonghan argues that Post-Jin had already belonged to a slave society before entering Liao and Shen.) [argues that] [Post-Jin] [Liao and Shen] [before] [entering] [ a slave] [already] [society] . 2 [belonged to] \u5c5e Figure 1: A human CWS case.is the correct segmentation.It is demanding for us to segment these words correctly.However, if we segment the relatively easy blocks of the sentence into words and then the hard block could be easy to be segmented, because of the excluding of other information's interference.Therefore, we propose BED, which imitates the process mentioned above.The model looks for easily tokenized positions, divides a whole sentence into several parts first, and then tokenizes each piece into more fine-grained words.This more intuitive approach can further improve the performance of the word segmentation model, especially the performance of OOV words.To our best knowledge, it is the first study trying to optimize the decoder part in a CWS model.In this paper, we will first introduce the related work of Chinese word segmentation in section 2 .Then we define the problem of CWS and introduce our proposed decoder and the entire model structure.We conduct exhaustive experiments, and the results and analysis will be described in Section 5."}
{"paper_id": 444, "introduction": "Denoising diffusion probabilistic models (DDPM) (Sohl-Dickstein et al., 2015;Ho et al., 2020) have recently become an object of great research interest in the generative modelling community since they often outperform the alternative approaches both in terms of the realism of individual samples and their diversity (Dhariwal & Nichol, 2021).The most impressive successes of DDPM were demonstrated in the domain of natural images (Dhariwal & Nichol, 2021;Saharia et al., 2022;Rombach et al., 2022), where the advantages of diffusion models are successfully exploited in applications, such as colorization (Song et al., 2021), inpainting (Song et al., 2021), segmentation Baranchuk et al. (2021), super-resolution (Saharia et al., 2021;Li et al., 2021), semantic editing (Meng et al., 2021) and others.Beyond computer vision, the DDPM framework is also investigated in other fields, such as NLP (Austin et al., 2021;Li et al., 2022), waveform signal processing (Kong et al., 2020;Chen et al., 2020), molecular graphs (Jing et al., 2022;Hoogeboom et al., 2022), time series (Tashiro et al., 2021), testifying the universality of diffusion models across a wide range of problems.The aim of our work is to understand if the universality of DDPM can be extended to the case of general tabular problems, which are ubiquitous in various industrial applications that include data described by a set of heterogeneous features.For many such applications, the demand for highquality generative models is especially acute because of the modern privacy regulations, like GDPR, which prevent publishing real user data, while the synthetic data produced by generative models can be shared.Training a high-quality model of tabular data, however, can be more challenging compared to computer vision or NLP due to the heterogeneity of individual features and relatively small sizes of typical tabular datasets.In our paper, we show that despite these two intricacies, the diffusion models can successfully approximate typical distributions of tabular data, leading to state-of-the-art performance on most of the benchmarks.In more detail, the main contributions of our work are the following:1. We introduce TabDDPM -the simplest design of DDPM for tabular problems that can be applied to any tabular task and can work with mixed data, which includes both numerical and categorical features.2. We demonstrate that TabDDPM outperforms the alternative approaches designed for tabular data, including GAN-based and VAE-based models from the literature, and illustrate the sources of this advantage for several datasets.3. We show that data produced by TabDDPM appears to be a \"sweet spot\" for privacyconcerned scenarios when synthetics are used to substitute the real user data that cannot be shared.The source code of TabDDPM is publicly availablefoot_0 ."}
{"paper_id": 445, "introduction": "Generative flow networks (GFlowNets; Bengio et al., 2021a) are generative models that construct objects lying in a target space X by taking sequences of actions sampled from a learned policy.GFlowNets are trained so as to make the probability of sampling an object  \u2208 X proportional to a given nonnegative reward ().GFlowNets' use of a parametric policy that can generalize to states not seen during training makes them a competitive alternative to methods based on local exploration in various probabilistic modeling tasks (Bengio et al., 2021a;Malkin et al., 2022;Zhang et al., 2022;Jain et al., 2022;Deleu et al., 2022).GFlowNets solve the variational inference problem of approximating a target distribution over X with the distribution induced by the sampling policy, and they are trained by algorithms reminiscent of reinforcement learning (although GFlowNets model the diversity present in the reward distribution, rather than maximizing reward by seeking its mode).In most past works (Bengio et al., 2021a;Malkin et al., 2022;Zhang et al., 2022;Jain et al., 2022), GFlowNets are trained by exploratory sampling from the policy and receive their training signal from the reward of the sampled object.The flow matching (FM) and detailed balance (DB) learning objectives for GFlowNets proposed in Bengio et al. (2021a;b) resemble temporal difference learning (Sutton & Barto, 2018).A third objective, trajectory balance (TB), was proposed in Malkin et al. (2022) to address the problem of slow temporal credit assignment with the FM and DB objectives.The TB objective propagates learning signals over entire episodes, while the temporal difference-like objectives (FM and DB) make updates local to states or actions.It has been hypothesized by Malkin et al. (2022) that the improved credit assignment with TB comes at the cost of higher gradient variance, analogous to the bias-variance tradeoff seen in temporal difference learning (TD() or TD()) with different eligibility trace schemes (Sutton & Barto, 2018;Kearns & Singh, 2000;van Hasselt et al., 2018;Bengio et al., 2020).This hypothesis is one of the starting points for the present paper.In this paper, we propose a new learning objective for GFlowNets, called subtrajectory balance (SubTB, or SubTB() when its real-valued hyperparameter  is specified).Building upon theoretical results of Bengio et al. (2021b); Malkin et al. (2022), we show how the SubTB() objective allows the flexibility of learning from partial experiences of any length.Experiments on two synthetic and four real-world domains support the following empirical claims:(1) SubTB() improves convergence of GFlowNets in previously studied environments: models trained with SubTB() approach the target distribution in fewer training iterations and are less sensitive to hyperparameter choices.(2) SubTB() enables training of GFlowNets in environments where past approaches perform poorly due to sparsity of the reward function or length of action sequences.(3) The benefits of SubTB() are explained by lower variance of the stochastic gradient, with the parameter  allowing interpolation between the high-bias, low-variance DB objective and the low-bias, high-variance TB objective."}
{"paper_id": 446, "introduction": "Consider a (black-box) image classifier that is trained on a dataset to output probability estimates for L classes given an input feature vector x \u2208 R d .This classifier is typically a deep neural network with a softmax layer at the end.Conformal prediction algorithms are wrapped around such a black-box classifier to generate a set of classes that contain the correct label with a user-specified desired probability based on the output probability estimates.Let x \u2208 R d be a feature vector with associated label y \u2208 {1, . . ., L}.We say that a set-valued function C generates valid prediction sets for the distribution P ifwhere 1\u03b1 is the desired coverage level.Conformal prediction methods generate valid set generating functions by utilizing a calibration set consisting of labeled examples {(x 1 , y 1 ), . . ., (x n , y n )} drawn from the distribution P.An important caveat of conformal prediction methods is that they assume that the examples from the calibration set and the test set are exchangeable, i.e., samples are identically distributed, or more broadly, are invariant to permutations across the two sets.The exchangeability assumption is difficult to satisfy and verify in applications and potentially limits the applicability of conformal prediction methods in practice.In fact, in practice one usually expects a distribution shift between the calibration set and the examples at inference (or the test set), in which case the coverage guarantees provided by conformal prediction methods are void.For example, the new CIFAR-10.1 and ImageNetV2 test sets were created in the same way as the original CIFAR-10 and ImageNet test sets, yet Recht et al. (2019) found a notable drop in classification accuracy for all classifiers considered.Ideally, a conformal predictor is recalibrated on a distribution before testing, otherwise the coverage guarantees are not valid (Cauchois et al., 2020).However, in real-world applications, while distribution shifts are ubiquitous, labeled data from new distributions is scarce or non-existent.We therefore consider the problem of recalibrating a conformal predictor only based on unlabeled data from the new domain.This is an ill-posed problem: it is in general impossible to calibrate a conformal predictor based on unlabeled data.Yet, we propose a simple calibration method that gives excellent performance for a variety of natural distribution shifts.Organization and contributions.We start with concrete examples on how conformal predictors yield miscalibrated uncertainty estimates under natural distribution shifts.We next propose a simple recalibration method that only uses unlabeled examples from the target distribution.We show that our method correctly recalibrates a popular conformal predictor (Sadinle et al., 2019) on a theoretical toy model.We provide empirical results for various natural distribution shifts of ImageNet showing that recalibrating conformal predictors using our proposed method significantly reduces the performance gap.In certain cases, it even achieves near oracle-level coverage.Related work.Several works have considered robustness of conformal prediction to distribution shift.Tibshirani et al. (2019) considers covariate shifts and proposes calibrating conformal predictors by estimating the amount of covariate shift from unlabeled target data.Similarly, Park et al. (2022) considers the problem of estimating the covariate shift from unlabeled target data, and aims at constructing PAC prediction sets instead of the standard unconditionally valid prediction sets.In contrast, we focus on complex image datasets for which covariate shift is not well defined.In Appendix B, we provide a comparison of our method to the above covariate shift based methods for a setting where we have access to labeled examples from multiple domains during training/calibration, one of which correspond to the target distribution.We are not aware of other works studying calibration of conformal predictors under distribution shift based on unlabeled examples.However, prior works propose to make conformal predictors robust to various distribution shifts from the source distribution of the calibration set (Cauchois et al., 2020;Gendler et al., 2022), via calibrating the conformal predictor to achieve a desired coverage in the worse case scenario of the considered distribution shifts.Cauchois et al. (2020) considers covariate shifts and calibrates the conformal predictor to achieve coverage for the worst-case distribution within the f -divergence ball of the source distribution.Gendler et al. (2022) considers adversarial perturbations as distribution shifts and calibrates a conformal predictor to achieve coverage for the worst-case distribution obtained through 2 -norm bounded adversarial noise.While making the conformal predictor robust to a range of worst-case distributions at calibration time allows maintaining coverage worst-case distributions, this approaches has two shortcomings: 1. Natural distribution shifts are difficult to capture mathematically, and models like covariate-shifts or adversarial perturbations do not seem to model natural distribution shifts (such as that from ImageNet to ImageNetV2) accurately.2. Calibrating for a worst-case scenario results in an overly conservative conformal predictor that tends to yield much higher coverage than desired for test distributions that correspond to a less severe shift from the source, which comes at the cost of reduced efficiency (i.e., larger set size, or larger confidence interval length).In contrast, our method does not compromise the efficiency of the conformal predictor on easier distributions as we recalibrate the conformal predictor separately for any new dataset.A related problem is to predict the accuracy of a classifier on new distributions from unlabeled data sampled from the new distribution (Deng & Zheng, 2021;Chen et al., 2021;Jiang et al., 2021;Deng et al., 2021;Guillory et al., 2021;Garg et al., 2022).In particular, Garg et al. (2022) proposed a simple method that achieves state-of-the-art performance in predicting classifier accuracy across a range of distributions.However, the calibration problem we consider is fundamentally different than estimating the accuracy of a classifier.While predicting the accuracy of the classifier would allow making informed decisions on whether to use the classifier for a new distribution, it doesn't provide a solution to recalibrate."}
{"paper_id": 447, "introduction": "The graph as a relational data structure is widely used to model real-world entity relations such as citation networks Yang et al. (2016a), recommended systems Wu et al. (2022), drug discovery Gaudelet et al. (2021), particle physics Shlomi et al. (2021), etc.However, due to the collection agents and privacy concerns, generally, the global domain-specific graph consists of many subgraphs collected by multiple institutions.In order to analyze the local subgraph, each client maintains a powerful graph mining model such as graph neural networks (GNNs), which have achieved stateof-the-art performance in many graph learning tasks Zhang et al. (2022b); Hu et al. (2021); Zhang & Chen (2018).Despite its effectiveness, the limited data provide sub-optimal performance in most cases.Motivated by the success of federated learning (FL), a natural idea is to combine the GNNs with FL to utilize the distributed subgraphs.Recently, federated graph learning (FGL) He et al. (2021); Wang et al. (2022b) is proposed to achieve collaborative training without directly sharing data, yet an essential concern is the heterogeneity of the distributed subgraphs.Notably, graph heterogeneity is different from the heterogeneity of labels or features in the fields of computer vision or natural language processing, we suggest that it depends on the graph structure.However, The existing FGL methods simulate the federated subgraph distributions through community split, which follows the cluster homogeneity assumption as shown in Fig. 1(a).Specifically, community split leads to the subgraph structure being consistent and the same as the original graph, e.g., connected nodes are more likely to have the same labels.Obviously, it is overly desirable and hard to satisfy in reality, hence we consider a more reasonable setting shown in Fig. 1(c).We first refer to the above problem as structure non-independent identical distribution (Non-IID).The motivation behind it is due to graph structure directly related to node labels and feature distributions.Meanwhile, the challenges of structure heterogeneity are ubiquitous in the real world Zheng et al. (2022b).For instance, in citation networks, we consider research teams focused on computers and intersectional fields (e.g., AI in Science) Shlomi et al. (2021); Gaudelet et al. (2021) as clients.In online transaction networks, fraudsters are more likely to build connections with customers instead of other fraudsters Pandit et al. (2007).We consider different regions as clients to detect financial fraudsters by analyzing online transaction subgraphs.Specifically, graph structure can be divided into two types: homogeneity means that connected nodes are more likely to have the same label and similar feature distributions and heterogeneity is the opposite.In order to explain it intuitively, we visualize the 3 clients partitioning result on Cora in Table . 1 andTable. 2, where Homo represents the homogeneity degree of the local subgraph, and it is computed by a popular metric Pei et al. (2020).Obviously, compared to community split, which follows the cluster homogeneity assumption and uniform distribution principle, structure Non-IID brings challenges to the existing FGL methods.Based on this, we investigate the above issues through empirical analysis shown in Fig. 2. According to the results, we observe that in case the original graph satisfies the homogeneity assumption then the label distributions satisfy Non-IID.It is the opposite when the original graph satisfies the heterogeneity.This is due to the fact that the nodes partitioned into the same clients are communities and follow the uniform distribution principle.In addition, the local accuracy indicates that the subgraph structure performs a more important role in FGL compared to the label distributions, which also supports our motivation.In model performance, we observe that the GGCN improves the structure Non-IID problem, and FedSage+ trains NeighGen to implement local subgraph augmentation by sharing node embeddings.However, the above methods fail to achieve competitive results as SGC on the homogeneous subgraphs while considering heterogeneity.In order to efficiently analyze distributed subgraphs with both homogeneity and heterogeneity.We propose a simple pipeline called Adaptive Federated Graph Learning (AdaFGL) for more general federated data settings, which consists of three main parts.Specifically, it starts by analyzing the subgraph structure through non-params label propagation and selects the appropriate base model: (i) the federated global knowledge extractor (e.g., MLP, powerful GNNs, or any reasonable embedding models), which does not rely on any learning over the subgraph.Then, the base predictor is trained based on the global data, which can be done offline or in parallel with local training, benefiting from the flexibility of our approach.Finally, the local client implements two adaptive propagation mechanisms: (ii) homogeneity propagation module or (iii) heterogeneity propagation module based on the local subgraph.Notably, with non-params label propagation, the above process is adaptive.To summarize, the contributions of this paper are as follows: (1)To the best of our knowledge, we are the first to analyze the structure Non-IID problem in FGL, which is a more general federated data setting and brings new challenges.(2) We propose AdaFGL, a new paradigm for structure Non-IID subgraph learning, which shows its flexibility in FGL with impressive performance.(3) Extensive experiments demonstrate the effectiveness of AdaFGL.Specifically, our approach achieves state-ofthe-art performance in the above two data settings.Compared to the best prediction accuracy in the 0 2 4 6 8 C la s s 1 C la s s 2 C la s s 3 C la s s 4 C la s s 5 C la s s 6 11 24 24 19 27 14 28 26 40 51 37 44 52 49 69 32 99 58 77 73 219 27 52 97 54 56 28 60 36 39 33 38 162 18 53 96 45 100 71 85 10 162 14 19 115 39 100 63 45 29 23 38 28 130 15 96 33 26 63 56 0 2 4 6 8 C la s s 1 C la s s 2 C la s s 3 C la s s 4 C la s s 5 C la s s 6 33 21 4 17 10 49 5 66 41 18 58 46 0 47 24 132 17 143 89 34 64 62 1 30 19 164 15 137 100 76 70 39 2 40 23 156 19 178 126 48 46 45 0 32 24 161 17 128 91 52 67 27 4 22 24 101 12 122 90 39 0 2 4 6 8 Community Split C la s s 1 C la s s 2 C la s s 3 C la s s 4 C la s s 5 58 107 125 166 119 90 112 98 84 83 78 84 128 92 125 104 135 89 98 107 95 105 116 81 102 93 116 102 116 113 151 112 85 66 107 113 89 115 108 94 138 112 66 115 67 120 68 116 115 123 0 2 4 6 8 Structure Non-IID C la s s 1 C la s s 2 C la s s 3 C la s s 4 C la s s 5 45 207 83 166 69 35 38 308 5 86 35 211 84 155 88 31 23 305 5 103 34 207 86 167 73 39 34 316 6 77 44 190 78 161 78 22 28 343 2 94 39 184 67 173 73 28 26 342 3 105 Client Local Accuracy Structure Challenges in CiteSeer & Squirrel 0.5 0.6 0.7 0.8 0.9 FedGGCN FedSGC FedSage+ 1 2 3 4 5 6 7 8 9 10 0.2 0.3 0.4 0.5 FedGGCN FedSGC FedSage+ 1 2 3 4 5 6 7 8 9 10 0.5 0.6 0.7 0.8 0.9 FedGGCN FedSGC FedSage+ 1 2 3 4 5 6 7 8 9 10 0.2 0.3 0.4 0.5 FedGGCN FedSGC FedSage+ baselines, our method achieves performance gains of 4.67% and 2.65% in structure Non-IID and community split data settings, respectively."}
{"paper_id": 448, "introduction": "Neural networks (NN) are known for their great performance in solving learning problems.However, these excellent results are almost always achieved at the price of human explainability.This problem is addressed in research and practice from different standpoints.There are calls to refrain from using NN for important problems and to rely on explainable methods, even if they give worse results in terms of accuracy (Rudin, 2019).The second major direction is to develop methods for explaining NN models.Such explanations can be classified as local explanations, i.e., why a particular data point was treated in a specific manner (Ribeiro et al., 2016), and global explanations, i.e., approaches for explaining the whole NN model.The latter can be achieved, e.g., by mapping the NN to an explainable surrogate.A common approach for locally explaining NN models is to highlight activation at some hidden layer (Fong & Vedaldi, 2018) or, if possible, project this inversely.For flat data, e.g., images, this is a viable approach since an essential explanatory component, the human, can be integrated into the process.This is not the case for high-dimensional or complex data.Global approaches are more difficult, in particular for high-dimension, and therefore less frequent.A typical idea is to find an (explainable) surrogate for a NN, e.g., symbolic regression (Alaa & van der Schaar, 2019).We answer to the still growing interest for global explanations procedures for NN models by introducing a novel intermediate space, called (symbolic) conceptual views.We demonstrate how NN models can represented in these views and how surrogate training, e.g., with decision trees, can profit from this.We further demonstrate how to compare NN models, e.g., when derived from diverse architectures, using Gromov-Wasserstein (M\u00e9moli, 2011) distance within the views.Moreover, we demonstrate how symbolic conceptual views can be used to represent NN models with formal concept lattices (Ganter & Wille, 1999) and profit from its human-centered approach for explainable data analysis.Finally, we show by an application of subgroup discovery how human-comprehensible propositional statements can be derived from NN models with the use of background knowledge.This allows us to extract global rules in form of propositional statements using the neurons of the NN."}
{"paper_id": 449, "introduction": "(Stochastic) bilevel optimization is a widely confronted problem in machine learning with various applications such as meta-learning (Finn et al., 2017;Bertinetto et al., 2018;Rajeswaran et al., 2019), hyper-parameter optimization (Franceschi et al., 2018;Shaban et al., 2019;Baydin et al., 2017;Bergstra et al., 2011;Luketina et al., 2016), reinforcement learning (Hong et al., 2020), and few-shot learning (Koch et al., 2015;Santoro et al., 2016;Vinyals et al., 2016).The basic form of this problem can be defined as followswhere f : R d1 \u00d7 R d2 \u2192 R and g : R d1 \u00d7 R d2 \u2192 R are two continuously differentiable loss functions with respect to x and y.Problem (1) has an optimization hierarchy of two levels, where the outer-level objective function f depends on the minimizer of the inner-level objective function g.Due to its importance, the above bilevel optimization problem has received considerable attention in recent years.A natural way to solve problem (1) is to apply alternating stochastic gradient updates with approximating \u2207 y g(x, y) and \u2207f (x, y), respectively.Briefly speaking, previous efforts mainly examined two types of methods to perceive an approximate solution that is close to the optimum y * (x).One is to utilize the single-timescale strategy (Chen et al., 2021;Guo et al., 2021;Khanduri et al., 2021;Hu et al., 2022), where the updates for y and x are carried out simultaneously.The other one is to apply the two-timescale strategy (Ghadimi & Wang, 2018;Ji et al., 2021; REFERENCE STABILITY BOUNDS IN VARIOUS SETTINGS SC-SC C-C NC-NC NC-SC SSGD (THIS WORK) O(1/m1) O(\u03ba1 K/2 /m1) O(K \u03ba 2 /m1) O(K \u03ba 3 /m1) TSGD (THIS WORK)O((\u03ba4) K /m1) O((\u03ba4) K /m1) O(T 1-\u03ba 5 K \u03ba 5 /m1) O(T 1-\u03ba 6 K \u03ba 6 /m1)Table 1: Summary of main results.\u03bai: a constant for all i above; T : inner iterations; K: outer iterations; m1: size of outer dataset.SSGD and TSGD stand for Algorithm 1 and Algorithm 2, the single-timescale and two-timescale methods, via stochastic gradient descent.Hong et al., 2020;Pedregosa, 2016), where the update of y is repeated multiple times to achieve a more accurate approximation before conducting the update of x.While there is a long list of work on bilevel optimization, most of the existing work only focuses on either analyzing its convergence behaviors (Ghadimi & Wang, 2018;Hong et al., 2020;Ji et al., 2021) or improving its convergence rate, based on the convexity and the smoothness properties of f (\u2022, \u2022) and/or g(\u2022, \u2022) (Liu et al., 2020;Li et al., 2020).Contrarily, only little effort is devoted to understanding the generalization behavior of the problem.To the best of our knowledge, there is only one recent work on the generalization analysis for bilevel problems (Bao et al., 2021), which presents the first expected uniform stability bound.However, there are still several undesirable issues in this work: (1) Their result is only for the uniform stability (which could be deduced from argument stability with certain conditions, see Definition 4 for details), leaving the analysis of other stronger definitions of algorithmic stability open; (2) Additionally, the UD algorithm allows the outer level parameters to be updated continuously but needs to reinitialize the inner level parameters before each iteration in the inner loop, which is not commonly used in practice due to their inefficiency (see line 4 in Algorithm 3).(3) The proof of Theorem 2 in their work is unclear to show whether the update of outer level parameters is argument dependent on the inner level parameters, where may exist some gap in the analysis of UD algorithm (see Appendix E for detailed discussions).( 4)Their experiments take only hyper-parameter optimization into consideration and neglect other applications in the bilevel optimization instances.To address all the aforementioned issues, we give in this paper a thorough analysis on the generalization behaviors of first-order (gradient-based) methods for general bilevel optimization problem.We employ the recent advances of algorithmic stability to investigate the generalization behaviors in different settings.Specifically, our main contributions can be summarized as follows:\u2022 Firstly, we establish a fundamental connection between generalization gap and different notations of algorithmic stability (argument stability and uniform stability) for any randomized bilevel optimization algorithms in both expectation and high probability forms.Specifically, we show that the high probability form of the generalization gap bound can be improved from O( \u221a n) to O(log n) compared with the result in Bao et al. (2021).\u2022 Next, we present the stability bounds for gradient-based methods with either singletimescale or two-timescale update strategy under different standard settings.To the best of our knowledge, this work provides the first stability bounds for the two-timescale (double loop) algorithms, which allows the accumulation of the sub-sampled gradients in the inner level.In detail, we consider the settings of strongly-convex-strongly-convex (SC-SC), convex-convex (C-C), and nonconvex-nonconvex (NC-NC), and further extend our analysis to a particular nonconvex-strongly-convex (NC-SC) setting that is widely appeared in practice.Table 1 is the summary of our main results.\u2022 Thirdly, we provide the first generalization bounds for the case where both the outer and inner level parameters are subject to continuous (iterative) changes.Compared to the previous work (Bao et al., 2021), our work does not need the reinitialization step before each iteration in the inner level and hence our algorithm can carry over the last updated inner level parameters, which is more general and practical.\u2022 Finally, we conduct empirical studies to corroborate our theories via meta-learning and hyperparameter optimization, which are two applications of bilevel optimization.Due to space limitations, all the proofs and additional experiments are included in Appendix."}
{"paper_id": 450, "introduction": "Transformer has achieved great breakthroughs in natural language processing (Devlin et al., 2019;Radford et al., 2019;Brown et al., 2020), computer vision (Dosovitskiy et al., 2020;Liu et al., 2021b), speech processing (Schneider et al., 2019;Ren et al., 2021), and biology (Jumper et al., 2021;Brandes et al., 2022).A major drawback of the transformer architecture is its quadratic complexity in both time and memory.The problem has been more evident with the ever-increasing need in applying transformers for longer sequence modeling in different domains.Recent research on efficient attention mechanisms seeks to respond to this problem by improving attention efficiency while preserving efficacy (Wang et al., 2020;Kitaev et al., 2019;Zaheer et al., 2020;Choromanski et al., 2020;Zheng et al., 2022).The commonly adopted test bed for benchmarking efficient transformers in the context of processing long sequences, is the Long Range Arena (Tay et al., 2020a;LRA), consisting of both synthetic probing tasks and real-world tasks.However, all of these tasks focus on the self attention setting, ignoring cross attention and causal attention, which are equally important and often more challenging.In other words, the transformer model is only used as a sequence encoder in LRA, while in real applications, cross attention is essential for conditionality modeling tasks such as sequence-to-sequence (Bahdanau et al., 2015), data-to-text (Du\u0161ek et al., 2020) and knowledge-enhanced models (Liu et al., 2021a), and causal attention is critical for causality modeling tasks such as text generation (Vaswani et al., 2017;Zhang et al., 2020), language modeling (Radford et al., 2019;Brown et al., 2020) and speech synthesis (Li et al., 2019).Another potential drawback of LRA as discovered by researchers recently, is that with proper tuning, the performance gap between different transformer variants in these tasks can be insignificant (Xiong et al., 2022;Ivgi et al., 2022;Shaham et al., 2022), impairing its effectiveness as a standard benchmark.To address these problems, we propose a comprehensive attention benchmark (CAB) for long sequence modeling.Above all, we present a fine-grained attention taxonomy, considering attentive functionality on conditionality and causality modeling.We present four patterns of attentions, namely, noncausal self, causal self, noncausal cross, and causal cross, representing the distinguishable attentive functionality to sequence modeling ( \u00a72).With that in mind, we then collect seven real-world tasks from diverse fields of computer vision, natural language processing, speech processing, and time series forecasting ( \u00a73).Among these tasks, CAB includes rich backbone architectures to evaluate the attention mechanisms, testing their performances and generalization abilities.Given four attention patterns defined by the attention taxonomy, we advocate a pattern-wise comparison between attention mechanisms, evaluating their attentive functionality respectively.We conduct exhaustive experiments on CAB, assessing nine widely-used efficient attentions designed with different philosophies ( \u00a74).The experimental results reveal several insights into designing efficient attention architectures.First, we show that existing efficient transformers claiming comparable or even superior performances to vanilla attention, often achieve less competitive results in the causal cross scenario, indicating efficient attentions' modeling capability cannot always generalize across different attention patterns.Second, by quantifying the efficiency of attentions in long sequence contexts using efficiency length (i.e., the minimum length for a sub-quadratic efficient model to surpass vanilla attention in efficiency), we disclose the underlying inefficiency problem of existing efficient attention methods in modeling relatively short sequences.Third, we investigate interpolation and extrapolation on a long-context language model, and find that it is promising for efficient attention, such as local attention and LongShort Transformer, to scale to long-context language modeling.We hope CAB and elaborated experimental efforts can shed light on the fundamental problems of efficient attentions, and inspire the design of advanced attention mechanisms.CAB and all the related codes will be released at https://github.com/Anonymous."}
{"paper_id": 451, "introduction": "Recent advances in Reinforcement Learning have yielded many promising research achievements Vinyals et al. (2019); Berner et al. (2019); Schrittwieser et al. (2019).However, the complexity of action spaces still prevents us from directly utilizing advanced RL algorithms to real-world scenarios, such as high-dimensional continuous control in robot manipulation Lillicrap et al. (2016) and structured hybrid action decision-making in strategy games Kanervisto et al. (2022).Complex action spaces lead to extensive challenges in designs of policy optimization Xiong et al. (2018b), efficiency of exploration Seyde et al. (2021b) and behaviour stability of learned agents Bester et al. (2019).To handle these issues, some existing work first elaborately design particular reinforcement learning methods in original complex action spaces.Specifically, deterministic policy gradient methods Lillicrap et al. (2016); Fujimoto et al. (2018) are designed to handle continuous control problems.And Xiong et al. (2018b); Fan et al. (2019b) propose some techniques to extract the relationship between different action dimensions, which is important in hybrid action spaces.However, these designs often suffer from low exploration efficiency and unstable training due to the infinite action spaces and interference between different sub-actions Bester et al. (2019), respectively.Action space shaping Kanervisto et al. (2020) is another way to tackle these problems.Particularly, many RL applications Kanervisto et al. (2022); Wei et al. (2022) design specific action discretization mechanisms to simplify the decision-making spaces, leading to the promising performance improvement, but it requires intensive investigations about the corresponding environments.Moreover, the combination of many manually discretized sub-actions will result in the exponential explosion of action numbers, which is incompatible with large action spaces.Recently, some works propose to learn abstract action representations to boost RL training.HyAR Li et al. (2021) designs a special training scheme with VAE Kingma & Welling (2014) to map the original hybrid action space to a continuous latent action space.Some other methods Dadashi et al. (2022); Shafiullah et al. (2022); Jiang et al. (2022) build prior sets of discrete actions to from expert demonstrations, and then deploy RL agents on this fixed discrete action sets.To preserve the necessary attributes of environments, all the above discretiza-tion techniques require related domain knowledge to discard redundant information about actions, which means that they are unsuitable for different environments with arbitrary action spaces.In this paper, we focus on how to learn a unified discrete action representations from scratch without any domain knowledge.Based on previous analysis and our investigations (as shown in Figure 1), we summarize the following advantages of discretization for the complexity of the action space:\u2022 Unified action discretization provides a powerful and general approach to dealing with reinforcement learning in complex action spaces.It is equivalent to split the entire pipeline into two parts: (1).representation learning and (2).decision-making.The former focus on intrinsic properties and data distributions of the action space, then transform various action spaces into standard discrete action sets, while the latter only needs to solve core decision-making problems.\u2022 Effective discretization can improve sample efficiency by reducing the overhead in repeating sub-optimal, useless, and semantically similar actions.RL agent can just explore and exploit the necessary subsets of the original action space during training.Then, we introduce Neural Discrete Reinforcement Learning (NDRL) framework.Specifically, inspired by VQ-VAE van den Oord et al. (2017), we propose a action representation method called Action Discretization Variational Auto-Encoder (AD-VAE) to learn latent discrete action space from the original environment, and conduct RL on the learned space utilizing any classical RL techniques about the discrete action.It is essential to capture the intrinsic properties of the original action space, which is beneficial to learn a compact latent action space while keeping necessary information of the original action space.Therefore, we design a state-conditioned action encoder and decoder, and utilize graph neural network (Kipf & Welling, 2016) and soft-argmax operation Luvizon et al. (2019)to improve the capability of AD-VAE for the relationships between different action dimensions and boundary action values.Furthermore, we find a core issue of parallel optimization of AD-VAE and RL agents: the online updates of AD-VAE may lead to semantic changes of latent actions (i.e. the non-stationary of decision spaces), resulting in severe data staleness and Q-value over-estimation.To solve this problem, we introduce action remapping and ensemble Q-learning.Concretely, we apply the classic DQN as an instance to our framework, named Action Discretization Q-learning (ADQ), which can be deployed for most complex action spaces.Compared with pioneer works (Chandak et al., 2019a;Zhou et al., 2020;Dadashi et al., 2022), to our best knowledge, our proposed framework is the first online RL paradigm capable of employing in discrete action spaces learned from different continuous and hybrid decision-making environments.To demonstrate the efficiency and stability of our NDRL framework and AD-VAE method, we evaluate it on the classic continuous control benchmark MuJoCo Todorov et al. (2012), showing that ADQ can achieve excellent performance operating in high-dimensional continuous space even with a small number of actions.To evaluate the generality, we test it on the hybrid action environments Gym Hybrid thomashirtz (2021), HardMove from HyAR and GoBigger Zhang (2021).The results show that ADQ outperforms current state-of-the-art hybrid action algorithms in both sample efficiency and final performance.Besides, we also conduct a series of ablation study experiments and interpret more details about NDRL by visualization on the latent space."}
{"paper_id": 452, "introduction": "Exciting developments in computational resources with a significant rise in data size have led deep neural networks (DNNs) to be widely used in various tasks, for example image classification.Despite their excellent performance in prediction, DNNs are seen as black boxes as their decision process generally includes a huge number of parameters and nonlinearities (Gilpin et al., 2018;Hagras, 2018;Zeiler & Fergus, 2014).The lack of explanation in these black boxes hinders their direct implementation in important and sensitive domains such as medicine and autonomous driving, where human life may directly be affected (Loyola-Gonzalez, 2019;Lipton, 2018).An example would be the DNNs trained to detect coronavirus.Although many works have been conducted and claimed to have a high predictive performance in detecting COVID-19 cases, a Turing Institute's recent report (Heaven, 2021) disappointingly finds that Artificial Intelligence (AI) used to detect coronavirus had little to no benefit and may even be harmful, mainly due to unnoticed biases in the data and its inherent black-box nature (also see e.g.Roberts et al. 2021).Another example is a woman who was hit and killed by an autonomous car.An investigation showed that the death was caused by the incapability of the car in detecting a human unless they are near a crosswalk (McCausland, 2019).In addition to these life-related examples, there are plenty of others where bias in training data or the model itself causes unwanted discriminations that may immensely affect people's lives.Amazon's AI-enabled recruitment tool is an example of how discriminative these models could be by only recommending men and directly eliminating resumes including the word \"woman\"; the company later announced that this tool had never been used to recruit people due to the detected bias (Olavsrud, 2022).These examples clearly show that for machine learning models to gain acceptance, it is critical to be able to reason why a certain decision has been made to prevent any unwanted consequences.Explanations delivered by explainable AI (XAI) can help machine learning practitioners debug their models by for example investigating the misclassification cases (Adadi & Berrada, 2018) and detecting bias in data (Tan et al., 2017).There have been a number of works in this context to reveal the reasoning of the black-box models (Simonyan et al., 2014;Springenberg et al., 2014;Zhou et al., 2016;Chattopadhay et al., 2018;Petsiuk et al., 2018;Ribeiro et al., 2016;2018).However, the most widely used techniques, creating class-wise saliency maps (e.g.see left of Figure 1) to indicate the areas that contribute to the prediction the most, have severe innate limitations.The first is the validation process of these maps, which is mostly qualitative or requires labour intensive object-wise annotations (Goebel et al., 2018;Park et al., 2018).A recent study in (Bearman et al., 2016) showed that a full supervision of object segmentation by humans takes around 78 seconds per instance while higher error rate bounding boxes take 10 seconds per instance to produce, which are much more expensive than 1 second per instance image level annotations.Moreover, requiring a higher level of annotation by experts is rather impractical.Another limitation stems from the discrepancy between these maps and human-like explanations.Humans naturally explain their reasoning using discriminative words (e.g.domestic vs wild or weak vs strong to differentiate a cat from a lion) together with pointing to where those words lie in the given image if visually permitting (Park et al., 2018;Goebel et al., 2018) (cf.our results on the right of Figure 1).To produce human-like explanations, this multilevel (i.e., visual and linguistic) manner is crucial, which also inspires the work in this article.In this article we propose a new methodology called multilevel XAI to delve into DNNs by leveraging visual and linguistic attributes.Our approach exploits per-class attributes (rather than per-image attributes, which are too expensive and generally impractical) to interpret DNNs in e.g.classifying raw images.By creating multilevel explanations, i.e., linguistic salient attributes and attribute-wise saliency maps, our method can achieve towards human-like explanations (e.g.see right of Figure 1).This is a big step forward in XAI and this new methodology does not suffer from the abovementioned limitations existing in current XAI solutions.The proposed setting adds a tiny extra cost to the training set, i.e., per-class attributes, which can be easily obtained if needed using for example online search engines or some autonomous tools (e.g.GPT-3 API Brown et al. 2020), and once acquired they can always be in use since in most cases they are time and image invariant.Our main contributions lie in: i) proposing a multilevel XAI methodology which is easy to use and can achieve towards human-like explanations; ii) implementing extensive experiments on both coarse-grained and fine-grained datasets to validate the performance of the proposed approach; and iii) conducting insightful discussions in XAI and future paths."}
{"paper_id": 453, "introduction": "Physics-informed neural networks (PINNs) have been proposed as alternatives to traditional numerical partial differential equations (PDEs) solvers (Raissi et al., 2019;2020;Sirignano & Spiliopoulos, 2018;Tripathy & Bilionis, 2018).In PINNs, a PDE which describes the physical domain knowledge of a problem is added as a regularization term to an empirical loss function.Although PINNs has shown remarkable performance in solving a wide range of problems in science and engineering (Cai et al., 2022;Kharazmi et al., 2019;Sun et al., 2020;Kissas et al., 2020;Tartakovsky et al., 2018), regardless of the simplicity of a PDE itself, they often fail to converge to accurate solutions when the target function contains high-frequency features (Krishnapriyan et al., 2021;Wang et al., 2021).This phenomenon known as the spectral bias exists in even the simplest linear PDEs (Wang et al., 2021;Moseley et al., 2021;Krishnapriyan et al., 2021).Spectral bias is not limited to PINNs.Rahaman et al. (2019) empirically showed that all fullyconnected feed-forward neural networks (NNs) are biased against learning complex components of target functions.Furthermore, Cao et al. (2019) theoretically proved that in training infinitely-wide networks with squared loss, the corresponding eigenvalues of the neural tangent kernel (NTK) (Jacot et al., 2018) indicate the exact convergence rate for different components of the target functions.Thus, spectral bias happens when the absolute values of some of the eigenvalues of the NTK are large while others are small.Recently, utilizing the NTK of infinitely-wide PINNs, Wang et al. (2022) examined the gradient flow of these networks during training.They proved that the training error decays based on e -\u03bait , where \u03ba i are the eigenvalues of the NTK.Thus, the components of the target function corresponding to the smaller eigenvalues have a slower rate of decay, which causes spectral bias.To tackle the issue of spectral bias, they proposed to assign a weight to each term of the loss function and dynamically update it.Although the results showed some improvements, as the frequency of the target function increased, their proposed PINN still failed to converge to solutions of PDEs.Moreover, as assigning weights can result in indefinite kernels, the training process could become extremely unstable.Of note, compared to the typical NNs, analyzing the effect of spectral bias for PINNs is more challenging as the loss function is regularized by means of adding the PDE equation.Thus, Wang et al. (2022)'s study was limited to training the model only based on GD.Some studies proposed an alternative approach in which instead of modifying the loss function terms, a high-frequency PDE is solved in a few successive steps.In these methods, it is assumed that the optimal solution of low-frequency PDEs is close to the optimal solution of high-frequency PDEs.Hence, instead of randomly initializing weights they are being initialized using the optimal solution of low-frequency PDEs.Moseley et al. (2021) implemented a finite element approach where PINNs were trained to learn basis functions over several small, overlapping subdomains.Similarly, Krishnapriyan et al. (2021) proposed a learning method based on learning the solution over small successive chunks of time.Moreover, they proposed another sequential learning scheme in which the model was gradually trained on target functions with lower frequencies, and, at each step, the optimized weights were used as the warm initialization for higher-frequency target functions.In a similar approach, Huang & Alkhalifah (2021) proposed to use the pre-trained models from lowfrequency functions and to increase the size of the network (neuron splitting) as the frequency of the target function is increased.Although these methods showed good performance on some PDEs, as the frequency terms became larger, the process became much slower as the required time steps would significantly grow.In this work, we study the spectral bias of PINNs from an optimization perspective.Existing studies only have focused on effect of the vanilla GD (Wang et al., 2022) or they are limited to some weak empirical evidence indicating that Adam might learn high feature faster (Taylor et al., 2022).We prove that an infinitely-wide PINN under the vanilla GD optimization process will converge to the solution.However, for high-frequency modes, the learning rate needs to become very small, which makes the convergence extremely slow, and hence not possible in practice.Moreover, we prove that for infinitely-wide networks, using the GD with momentum (GDM) optimizer can reduce the effect of spectral bias in the networks, while significantly outperforming vanilla GD.We also investigate why the Adam optimizer can also accelerate the optimization process while decreases the effect of spectral bias in PINNs.To the best of our knowledge this is the first time that the gradient flow of the output of PINNs under the GDM, and Adam are being analyzed, and their relation to solving spectral bias is discussed.Finally, our extensive numerical experiments on sufficiently wide networks confirm our theoretical findings."}
{"paper_id": 454, "introduction": "Inspired by biological neural networks, the theory of artificial neural networks has largely focused on pointwise (or \"local\") nonlinear layers (Rosenblatt, 1958;Cybenko, 1989), in which the same function \u03c3 : R \u2192 R is applied to each coordinate independently:R n \u2192 R n , v = (v 1 , . . ., v n ) \u2192 (\u03c3(v 1 ) , \u03c3(v 2 ) , . . ., \u03c3(v n )).(1.1)In networks with pointwise nonlinearities, the standard basis vectors in R n can be interpreted as \"neurons\" and the nonlinearity as a \"neuron activation.\"Research has generally focused on finding functions \u03c3 which lead to more stable training, have less sensitivity to initialization, or are better adapted to certain applications (Ramachandran et al., 2017;Misra, 2019;Milletar\u00ed et al., 2018;Clevert et al., 2015;Klambauer et al., 2017).Many \u03c3 have been considered, including sigmoid, ReLU, arctangent, ELU, Swish, and others.However, by setting aside the biological metaphor, it is possible to consider a much broader class of nonlinearities, which are not necessarily pointwise, but instead depend simultaneously on many coordinates.Freedom from the pointwise assumption allows one to design activations that yield expressive function classes with specific advantages.Additionally, certain choices of non-pointwise activations maximize symmetry in the parameter space of the network, leading to compressibility and other desirable properties.In this paper, we introduce radial neural networks which employ non-pointwise nonlinearities called radial rescaling activations.Such networks enjoy several provable properties including high model compressibility, symmetry in optimization, and universal approximation.Radial rescaling activations are defined by rescaling each vector by a scalar that depends only on the norm of the vector:where \u03bb is a scalar-valued function of the norm.Whereas in the pointwise setting, only the linear layers mix information between different components of the latent features, for radial rescaling, all coordinates of the activation output vector are affected by all coordinates of the activation input vector.The inherent geometric symmetry of radial rescalings makes them particularly useful for designing equivariant neural networks (Weiler & Cesa, 2019;Sabour et al., 2017;Weiler et al., 2018a;b).We note that radial neural networks constitute a simple and previously unconsidered type of multilayer radial basis functions network (Broomhead & Lowe, 1988), namely, one where the number of hidden activation neurons (often denoted N ) in each layer is equal to one.Indeed, pre-composing equation 1.2 with a translation and post-composing with a linear map, one obtains a special case of the local linear model extension of a radial basis functions network.In our first set of main results, we prove that radial neural networks are in fact universal approximators.Specifically, we demonstrate that any asymptotically affine function can be approximated with a radial neural network, suggesting potentially good extrapolation behavior.Moreover, this approximation can be done with bounded width.Our approach to proving these results departs markedly from techniques used in the pointwise case.Additionally, our result is not implied by the universality property of radial basis functions networks in general, and differs in significant ways, particularly in the bounded width property and the approximation of asymptotically affine functions.In our second set of main results, we exploit parameter space symmetries of radial neural networks to achieve model compression.Using the fact that radial rescaling activations commute with orthogonal transformations, we develop a practical algorithm to systematically factor out orthogonal symmetries via iterated QR decompositions.This leads to another radial neural network with fewer neurons in each hidden layer.The resulting model compression algorithm is lossless: the compressed network and the original network both have the same value of the loss function on any batch of training data.Furthermore, we prove that the loss of the compressed model after one step of gradient descent is equal to the loss of the original model after one step of projected gradient descent.As explained below, projected gradient descent involves zeroing out certain parameter values after each step of gradient descent.Although training the original network may result in a lower loss function after fewer epochs, in many cases the compressed network takes less time per epoch to train and is faster in reaching a local minimum.To summarize, our main contributions and headline results are:\u2022 Radial rescaling activations are an alternative to pointwise activations: We provide a formalization of radial neural networks, a new class of neural networks; \u2022 Radial neural networks are universal approximators: Results include a) approximation of asymptotically affine functions, and b) bounded width approximation; \u2022 Radial neural networks are inherently compressible: We prove a lossless compression algorithm for such networks and a theorem providing the relationship between optimization of the original and compressed networks.\u2022 Radial neural networks have practical advantages: We describe experiments verifying all theoretical results and showing that radial networks outperform pointwise networks on a noisy image recovery task."}
{"paper_id": 455, "introduction": "Federated Learning (FL) (McMahan et al., 2017;Li et al., 2020a;Wang et al., 2021;Lin et al., 2020;Li et al., 2022b) is a promising distributed optimization paradigm where clients' data are kept local, and a central server aggregates clients' local gradients for collaborative training.Although a lot of FL algorithms with deep neural networks (DNNs) are emerging in recent years (Lin et al., 2020;Chen & Chao, 2021a;Li et al., 2020b;Acar et al., 2020;Chen & Chao, 2021b), there are few works about the underlying training dynamics in FL with DNNs (Yan et al., 2021;Yuan et al., 2021), which hinders us to go further into the link between generalization and optimization in FL.In the meanwhile, an interesting analogy exists between centralized mini-batch SGD and FL.The server-client training framework of FL (from the server perspective) learns a global model by iteratively sampling a cohort of clients and updating the global model with the sum local gradient of the cohort.While in centralized mini-batch SGD, a model is learned by iteratively sampling a mini-batch of data and updated by summing the corresponding gradients.In the analogy, the clients in FL refer to the data samples in mini-batch SGD, the cohort of clients refers to the mini-batch of data samples, and the communication round refers to the iteration step.The interesting analogy makes us wonder:Can we leverage the insights of mini-batch SGD to better understand the training dynamics in FL?Following this question and considering the key techniques in mini-batch SGD (as well as its generalization), in this paper, we focus on two aspects of training dynamics in FL: client coherence (refers to sample coherence in mini-batch SGD) and global weight shrinking (GWS) regularization (refers to weight decay in mini-batch SGD).Firstly, sample coherence explains how the relations between data samples affect the generalization of DNN models (Chatterjee, 2019;Chatterjee & Zielinski, 2020;Fort et al., 2019).As an analogy, here we extend the concept of sample coherence to the client case in FL with partial participation for studying the effect and training dynamics jointly caused by heterogeneous client data and local updates.Secondly, in a different line of works, weight decay methods (Lewkowycz & Gur-Ari, 2020;Zhang et al., 2018;Loshchilov & Hutter, 2018;Xie et al., 2020)-by decaying the weights of the model parameter in each iteration step-are the key techniques in the mini-batch SGD based optimization to guard the generalization performance of deep learning tasks.We similarly examine the effects of weight decay in FL, in which we shrink the aggregated global model on the server in each communication round (i.e.global weight shrinking).Note that we take the server-side aggregation weight optimization as a tool framework to derive the insights of the training dynamics in FL.Though the idea of aggregation weight optimization was appeared in previous FL works to match similar peers in decentralized FL (Li et al., 2022a) or improve performances in FL with medical tasks (Xia et al., 2021), all prior works assume normalized aggregation weights of clients' models (i.e.\u03b3 = 1 in Equation 1), failing to dive into understand the FL's dynamics from the learned weights for further insights, e.g., identifying the significance of adaptive global weight shrinking.Specifically, our contributions are three-folded.\u2022 We first make an analogy between centralized mini-batch SGD and FL, in which it enables us to derive a principled tool framework to understand the training dynamics in FL, by leveraging the learnt aggregation weights a global-objective-consistent proxy dataset.\u2022 As our main contribution, we identify some interesting findings (see below take-away) to unveil the training dynamics of FL, from the aspects of client coherence (cf.section 3) and global weight shrinking (cf.section 4)foot_0 .These insights are crucial to the FL community and can inspire better practical algorithm design in the future.\u2022 We showcase the effectiveness of these insights, and devise a simple yet effective method FEDAWO, for server-side aggregation weight optimization (cf.section 5).It can perform adaptive global weight shrinking and optimize attentive aggregation weights simultaneously to improve the performance of the global model.We summarize our key take-away messages of the understandings as follows.\u2022 Our novel concept of client coherence undermines the training dynamics of FL, from the aspects of local gradient coherence and heterogeneity coherence.-Local gradient coherence refers to the averaged cosine similarities of clients' local gradients.A critical point (from positive to negative) exists in the curves of local gradient coherence during the training.The optimization quality of the initial phase (before encountering the point) matters: Assigning larger weights to more coherent clients in this period boosts the final performance.-Heterogeneity coherence refers to the distribution consistency between the global data and the sampled one (i.e.data distribution of a cohort of sampled clients) in each round.The value of heterogeneity coherence is proportional to the IID-ness of clients as well as the client participation ratio; the higher, the better.Increasing the heterogeneity coherence by reweighting the sampled clients could also improve the training performance.\u2022 Global weight shrinking regularization effectively improves the generalization performance of the global model.-When the number of local epochs is larger, or the clients' data are more IID, a stronger global weight shrinking is necessary.-The magnitude of the global gradient (i.e.uniform average of local updates) determines the optimal weight shrinking factor.A larger norm of the global gradient requires stronger regularization.-In the late training of FL, where the global model is near convergence, the effect of global weight shrinking gradually saturates.-The effectiveness of global weight shrinking is stemmed from flatter loss landscapes of the global model as well as the improved local gradient coherence after the critical point.foot_1"}
{"paper_id": 456, "introduction": "Many real world applications require message classification and regression, such as handling spam emails Karim et al. (2020), ticket routing Han et al. (2020), article sentiment review Medhat et al. (2014) and more.Accurate message classification could improve critical scenarios such as in call centers (routing tickets based on topic) Han et al. (2020), alert systems (flagging highly important alert messages) Gupta et al. (2012), and categorizing incoming messages (automatically unclutter emails) Karim et al. (2020); Klimt & Yang (2004).The main distinction between text and message classification is the availability of additional attributes, such as the sender information, timestamps, attached image, audio, affiliations, and more.New message classification contests often appear in the prominent platforms (i.e., Kaggle Kaggle), showing how this topic is sought after.There are already many data-sets to explore in this field, but no clear winner algorithm that fits all scenarios with high accuracy, efficiency and simplicity (in terms of implementation and interpretation).A notable advancement in the field of NLP is the attention based transformers architecture Vaswani et al. (2017).This family of methods excels in finding local connections between words, and better understanding the meaning of a sentence.A leading example is the Bidirectional Encoder Representations from Transformers (BERT) Devlin et al. (2018) as well as its variations Liu et al. (2019); Lan et al. (2019); Sanh et al. (2019), winning certain benchmarks Rajpurkar et al. (2018); Wang et al. (2019).Several packages, such as Huggingface Transformers Wolf et al. (2019), make such models accessible and easy to use as well as provide pre-trained versions.In addition, one can use transfer learning Pan & Yang (2009) to further train BERT on their on data, creating a tailored model for the specific task at hand.BERT, and often other transformer based models, are designed to handle text.They operate on the words of a given text by encoding them into tokens, and by the connections between the tokens they learn the context of sentences.This approach is limited, since sometimes more information can be extracted and used, not necessarily textual.Throughout this paper we refer to this information as meta-data to distinguish it from the main stream of textual content (though one may recognize it as the core data, depending on the application).For example, a meta-data could be the time stamp of when the text was written, sent, published, etc.Another example is the writer of the text, when dealing with a small list of writers of a corpus.There have been some attempts to incorporate these into BERT models, for example by assigning artificial tokens for writers or for temporal segments (token per month for example) Zhang et al. (2021).This approach is limited since not all metadata entries are suitable for encoding by tokenization.In the example of temporal segments, more segments introduce more tokens, leading to large computational resources consumption, and less segments cause loss of information.Another approach is to concatenate the embeddings, created by the transformer module, with the outputs of an embedding module for the meta-data.In this approach, a transformer for the text is trained (using direct or transfer learning) on the text, and other separate modules (time series embedding, senders embeddings, etc.) are used to embed the meta-data.All the embeddings are then concatenated and used as inputs to a classification network.A drawback of this approach is that the internal network features are not trained from a combination of diffident input streams, and therefore avoid cross dependent features (e.g. the importance of an email is not only determined by its content, but also by who sent it, when, to whom else, attachments, etc.).To bridge these gaps, we implement a transformer based model that is able to train with both the text (transformer architecture) and meta-data.We create a new architecture of a blocks based network.Each block handles different kind of inputs.Splitting to blocks enables the flexibility to handle different kind of inputs.We present results of the method with a main block based on a transformer that handles the text, and an additional block that handles the pre-processed meta-data inputs individually.This method can be extended to support more complex blocks, such as an advanced DL model for images Wang et al. (2017), a temporal analysis block to extract information from temporal metadata Ienco & Interdonato (2020), additional transformer blocks for multiple text inputs (for example, subject and body of an email), categorical data, and more.To demonstrate the performance of the method we run multiple experiments on publicly available data-sets to show the advantages of using the block architecture, and compare them to the transformer benchmark (BERT), Random Forest (RF) classifier, and Multi-Layer Perceptron (MLP) networks.We achieve competitive results, and in most cases lead those benchmarks, showcasing that there is much to extract from the meta-data compared to just using text for classification tasks."}
{"paper_id": 457, "introduction": "Video-language pre-training (VLP) (Lei et al., 2021;Li et al., 2020a;Miech et al., 2020) that jointly learns video and language representations in a self-supervised manner has become the most popular practice to cope video-language retrieval (Lee et al., 2018;Liu et al., 2019a).Recently, end-to-end methods (Bain et al., 2021;Zellers et al., 2021) that learn video representations from raw pixels have dominated due to their strong performance on downstream tasks.Despite significant progress, these methods are quite data-hungry due to a large number of model parameters and uncurated raw inputs.The pre-training stage turns out to be inefficient and expensive with massive pre-training data and long pre-training time, making it difficult for researchers to pursue research in VLP.Previous work (Lei et al., 2021) attempts to lower the barrier for VLP via removing visual redundancy.They point out that video clips with sparsely sampled frames are sufficient enough to capture key semantics for pre-training, since adjacent frames often contain similar scenes.The effort enables more efficient VLP with competitive downstream performances.Besides the temporal visual redundancy, we argue that, in contrast to the text with highly abstract semantics, each frame of the video clips also has heavy spatial redundancy.Towards this end, we further propose to remove the redundant spatial information in sparsely sampled video clips via the claim that a frame is actually worth around 30 objects (based experiments in Section 4.4).Specifically, we revitalize offline region features that were all the rage in imagelanguage tasks (Liu et al., 2019a) to encourage efficient VLP.Region features are generally preextracted by a pre-learned object detector (Anderson et al., 2018).Rather than the dense and continuous visual signal of the raw pixels, the region features are sparsely distributed with the compact information of salient visual contents, which are the most useful for video-text understanding.The sparse sampling significantly reduce the complexity of attention mechanism, which enables our model to have larger capacity with less FLOPs.We further advocate \"less is more\" for one more step towards democratizing VLP research.As is known, methods using off-the-shelf features (Lee et al., 2018) have been phased out in visuallanguage tasks due to the inferior downstream performances.Previous work (Lei et al., 2021)   tributes the unsatisfactory pre-training performance of pre-extracted features to their disconnections with the current domain and language modality.We would like to clarify that such disconnections can be properly eliminated by imposing fine-grained cross-modality alignment regularization.Specifically, besides the common late fusion regularization on the global visual-text representations (Bain et al., 2021), we introduce a novel bidirectional region-word alignment regularization under the observation that objects extracted from video frames are naturally associated with certain words in the corresponding sentences.For instance, as demonstrated in Fig. 1, the keywords \"people\", \"car\" and \"bicycle\" share high-level semantics with cropped regions (highlighted with bounding boxes), respectively.To model and promote such a detailed cross-modality relationship, we build bidirectional connections between extracted regions and words.In the Region\u2192Word manner, we estimate the region-to-sentence similarity resorting to the similarities between each region and all the words in a sentence.The average region-to-sentence similarity over all the regions of a video clip is treated as the video-to-sentence similarity, which is further maximized for positive pairs.Similarly, the Word\u2192Region manner is conducted to measure and optimize the sentence-to-video similarity according to the similarities between each word and the corresponding regions.We surprisingly find that the proposed fine-grained region-word alignment constraints can also be seamlessly integrated into end-to-end VLP methods (Bain et al., 2021), achieving promising performance gains.In summary, our contributions are three-fold: (1) We revitalize region features towards democratizing VLP via removing both temporal and spatial visual redundancy.Specifically, our efficient VLP model can maintain state-of-the-art performance on multiple downstream tasks with 80% fewer data and 85% less pre-training time than ClipBERT, which is the most efficient end-to-end VLP method so far.(2) We clarify that the inferior performance of off-the-shelf features in previous attempts (Li et al., 2020a;Zhu & Yang, 2020;Sun et al., 2019;Yu et al., 2018;Gabeur et al., 2020) lies in the sub-optimal learning regularization.We tackle the challenge with a newly proposed bidirectional region-word constraint, which optimizes fine-grained visual-text relations and properly eliminates the domain/modality disconnections of the region features.(3) Our method shows competitive results on four downstream video-language retrieval tasks.We surprisingly observe that the introduced region-word alignment regularization can also effectively boost the end-to-end method (Bain et al., 2021) with noticeable improvements."}
{"paper_id": 458, "introduction": "Vertical federated learning (VFL) attracts increasing attention due to the emerging concerns over data privacy in multi-party collaborative learning Hardy et al. (2017); Vepakomma et al. (2018); Liu et al. (2019b); Hu et al. (2019); Zhang et al. (2021b;c).Currently, extensive VFL methods have gained success in various applications, such as medical study, financial risk, and targeted marketing Cheng et al. (2019); Hu et al. (2019); Liu et al. (2019a;b); Li et al. (2021); Zhang et al. (2021c).However, these methods are designed only for machine learning (ML) problems with single-level structure and are not applicable to those falling under bilevel programming, such as hyper-representation learning and hyperparameter tuning, which are becoming popular in practical VFL applications.Thus, it is desirable to design methods solving ML problems with bilevel structures under the setting of VFL.A desirable solution is adopting bilevel optimization (BO) Willoughby (1979) into VFL because extensive stochastic BO methods have been proposed to well address various machine learning tasks falling under bilevel programming Grazzi et al. (2020); Ji et al. (2021); Rajeswaran et al. (2019); Ji & Liang (2021); Tarzanagh et al.; Gao (2022).However, it is challenging to achieve this because it is difficult to compute the hypergradient of BO problems under the setting of VFL (defined as vertical federated bilevel optimization problems, VFBO problems) with privacy-preserving and computation-efficient.On-shelf BO methods Chen et al. (2021); Yang et al. (2021); Ji & Liang (2021); Grazzi et al. (2020) use the second-order derivatives to approximate Chen et al. (2021); Yang et al. (2021); Ji & Liang (2021) or directly compute Grazzi et al. (2020) the inverse Hessian matrix used for computing the hypergradient.However, besides the high computation complexity, applying existing methods to VFBO problems will cause feature privacy leakage.Specifically, 1) directly computing the second-order derivatives requires each party to access data of all features (not only its own features but also features of other parties), which will lead to feature privacy leakage, 2) approximating or directly computing the inverse Hessian matrix has a high computation complexity.Thus, it is challenging to design VFBO methods with privacy-preserving and computation efficiency.In this paper, we address these challenges by proposing the novel stochastic Bilevel optimizAtion Method with a desirable JacoBian estImator (BAMBI).Specifically, BAMBI ingeniously adopts the zeroth-order (ZO) estimation technique to approximate the Jacobian matrix, which enables all parties to collaboratively compute the hypergradient with privacy-preserving and computation efficiency.We theoretically prove that BAMBI still has the convergence rate of 1/ \u221a K for nonconvex-stronglyconvex problems, where K is the total number of upper-level iterations.This convergence rate matches those of BO algorithms not using ZO estimation, which justifies the advantage in privacy preservation without sacrificing convergence rate.In BAMBI, only the intermediate gradients (rather than raw labels) are transmitted (please refer to Fig. 1 and its explanation for details) from the server to parties not holding labels (denoted as passive parties), which enables passive parties to optimize models locally without directly accessing the labels.Therefore, it seems BAMBI can preserve label privacy.However, existing works have shown that transmitting intermediate gradient is vulnerable to label privacy leakage Li et al. (2021); Sun et al. (2022); Yang et al. (2022).Particularly, raw labels often contain highly sensitive information (e.g., important demographic information Ghazi et al. (2021) or disease diagnosis results Vepakomma et al. (2018)).Thus, it is also important to preserve the label privacy of BAMBI.To address this important problem, we design the BAMBI-DP by leveraging differential privacy (DP) technique to further preserve the label privacy.Specifically, BAMBI-DP adds well designed noises to the intermediate gradients, which is proved to guarantee ( , 0)-differentially private with respect to (w.r.t.) the label.We summarize the contributions of this paper as follows.\u2022 To our best knowledge, we are the first to propose methods, i.e.BAMBI and BAMBI-DP, for solving VFL problems falling under bilevel programming.\u2022 We design a desirable Jacobian estimator in BAMBI, which enables all parties to collaboratively compute the hypergradient with privacy-preserving and computation efficiency.We also derive the convergence rate of BAMBI for nonconvex-strongly-convex problems, which justifies our advantage in privacy preservation without sacrificing convergence rate.\u2022 We further design the BAMBI-DP to preserve the label privacy, which is proved to be ( , 0)differentially private w.r.t. the label.Notations a i \u2208 R d denotes features of sample i, and b i denotes its label.Given a positive integer l, [l] denotes the set {1, \u2022 \u2022 \u2022 , l}.We use subscript m \u2208 [l] to denote notations associated with party m, e.g., a i m \u2208 R dm , x m \u2208 R pm and y m \u2208 R qm denote the features, upper-and lower-level variables on party m, respectively.We use superscript t = 0, \u2022 \u2022 \u2022 , N -1, k = 1, \u2022 \u2022 \u2022 , K to denote the timestamp of variables, where N and K are the total number of low-and upper-level iterations."}
{"paper_id": 459, "introduction": "Decentralized multi-agent online learning concerns agents that, simultaneously, learn to behave over time in order to achieve their goals.Compared to the single-agent setup, novel challenges are present as agents may not share the same objectives, the environment becomes non-stationary, and information asymmetry may exist between agents (Yang & Wang, 2020).Traditionally, the multi-agent problem has been addressed by either relying on a central controller to coordinate the agents' actions or to let the agents learn independently.However, access to a central controller may not be realistic and independent learning suffers from convergence issues (Zhang et al., 2019).To circumvent these issues, a common approach is to drop the central coordinator and allow information exchange between agents (Zhang et al., 2018;2019;Cesa-Bianchi et al., 2021).Decision-making that involves multiple agents is often modeled as a game and studied under the lens of game theory to describe the learning outcomes. 1Herein, we consider games with a leaderfollower structure in which players act consecutively.For two players, such games are known as Stackelberg games (Hicks, 1935).Stackelberg games have been used to model diverse learning situations such as airport security (Balcan et al., 2015), poaching (Sessa et al., 2020), tax planning (Zheng et al., 2020), and generative adversarial networks (Moghadam et al., 2021).In a Stackelberg game, one is typically concerned with finding the Stackelberg equilibrium, sometimes called Stackelberg-Nash equilibrium, in which the leader uses a mixed strategy and the follower is bestresponding.A Stackelberg equilibrium may be obtained by solving a bi-level optimization problem if the reward functions are known (Sch\u00e4fer et al., 2020;Aussel & Svensson, 2020) or, otherwise, it may be learnt via online learning techniques (Bai et al., 2021;Zhong et al., 2021), e.g., no-regret algorithms (Shalev-Shwartz, 2012;Deng et al., 2019;Goktas et al., 2022).No-regret algorithms have emerged from the single-player multi-armed bandit problem as a means to alleviate the exploitation-exploration trade-off (Bubeck & Slivkins, 2012).An algorithm is called no-regret if the difference between the cumulative rewards of the learnt strategy and the single best action in hindsight is sublinear in the number of rounds (Shalev-Shwartz, 2012).In the multi-armed bandit problem, rewards may be adversarial (based on randomness and previous actions), oblivious adversarial (random), or stochastic (independent and identically distributed) over time (Auer et al., 2002).Different assumptions on the bandit rewards yield different algorithms and regret bounds.Indeed, algorithms tailored for one kind of rewards are sub-optimal for others, e.g., the EXP3 algorithm due to Auer et al. (2002) yields the optimal scaling for adversarial rewards but not for stochastic rewards.For this reason, best-of-two-worlds algorithms, able to optimally handle both the stochastic and adversarial rewards, have recently been pursued and resulted in algorithms with close to optimal performance in both settings (Auer & Chiang, 2016;Wei & Luo, 2018;Zimmert & Seldin, 2021).Extensions to multiplayer multi-armed bandit problems have been proposed in which players attempt to maximize the sum of rewards by pulling an arm each, see, e.g., (Kalathil et al., 2014;Bubeck et al., 2021).No-regret algorithms are a common element also when analyzing multiplayer games.For example, in continuous two-player Stackelberg games, the leader strategy, based on a no-regret algorithm, converges to the Stackelberg equilibrium if the follower is best-responding (Goktas et al., 2022).In contrast, if also the follower adopts a no-regret algorithm, the regret dynamics is not guaranteed to converge to a Stackelberg equilibrium point (Goktas et al., 2022, Ex. 3.2).In (Deng et al., 2019), it was shown for two-player Stackelberg games that a follower playing a, so-called, mean-based no-regret algorithm, enables the leader to achieve a reward strictly larger than the reward achieved at the Stackelberg equilibrium.This result does, however, not generalize to n-player games as demonstrated by D'Andrea (2022).Apart from studying the Stackelberg equilibrium, several papers have analyzed the regret.For example, Sessa et al. ( 2020) presented upper-bounds on the regret of a leader, employing a no-regret algorithm, playing against an adversarial follower with an unknown response function.Furthermore, Stackelberg games with states were introduced by Lauffer et al. (2022) along with an algorithm that was shown to achieve no-regret.As the follower in a Stackelberg game observes the leader's action, there is information exchange.A generalization to multiple players has been studied in a series of papers (Cesa-Bianchi et al., 2016;2020;2021).In this line of work, players with a common action space form an arbitrary graph and are randomly activated in each round.Active players share information with their neighbors by broadcasting their observed loss, previously received neighbor losses, and their current strategy.The goal of the players is to minimize the network regret, defined with respect to the cumulative losses observed by active players over the rounds.The players, however, update their strategies according to their individually observed loss.Although we consider players connected on a graph, our work differs significantly from (Cesa-Bianchi et al., 2016;2020;2021), e.g., we allow only actions to be observed between players and player strategies are updated based on a common bandit reward.We introduce the joint pseudo-regret, defined with respect to the cumulative reward where all the players observe the same bandit-reward in each round.We provide an online learning-algorithm for general consecutive-play games that relies on no-regret algorithms developed for the single-player multi-armed bandit problem.The main novelty of our contribution resides in the joint analysis of players with coupled rewards where we derive upper bounds on the joint pseudoregret and prove our algorithm to be no-regret in the stochastic and adversarial setting.Furthermore, we quantify the penalty incurred by our decentralized setting in relation to the centralized setting."}
{"paper_id": 460, "introduction": "The explosion of data for embedding the physical world is reshaping the ways we understand, model, and control dynamical systems.Though control theory has been classically rooted in a model-based design and solving paradigm, the demands of model reusability, and the opacity of complex dynamical systems call for a rapprochement of modern control theory, machine learning, and optimization.Recent years have witnessed emerging trends of control theories with successful applications to engineering and scientific research, such as robotics (Krimsky & Collins, 2020), aerospace technology (He et al., 2019), and economics and management (Lapin et al., 2019) etc.We consider the well-established formulation of optimal control (Kirk, 2004) in finite time horizon T = [t 0 , t f ].Denote X and U as two vector-valued function sets, representing state functions and control functions respectively.Functions in X (resp.U ) are defined over T and have their outputs in R dx (resp.R du ).State functions x \u2208 X and control functions u \u2208 U are governed by a differential equation.The optimal control problem (OCP) is targeted at finding a control function that minimizes the cost functional f (Lions, 1992;Kirk, 2004;Vinter & Vinter, 2010;Lewis et al., 2012):where d is the dynamics of differential equations; p evaluates the cost alongside the dynamics and h evaluates the cost at the termination state x(t f ); and x 0 is the initial state.We restrict our discussion to differential equation-governed optimal control problems, leaving the control problems in stochastic networks (Dai & Gluzman, 2022), inventory management (Abdolazimi et al., 2021), etc. out of the scope of this paper.The analytic solution of Eq. 1 is usually unavailable, especially for complex dynamical systems.Thus, there has been a wealth of research towards accurate, efficient, and scalable numerical OCP solvers (Rao, 2009) and neural network based solvers (Kiumarsi et al., 2017) in recent years.However, both classic and modern numerical OCP solvers are facing challenges, especially emerging in the big data era, which we briefly discuss as follows.1) Opacity of Dynamical Systems.Existing works (B\u00f6hme & Frank, 2017a;Effati & Pakdaman, 2013;Jin et al., 2020) assume the dynamical systems a priori and exploit their explicit forms to ease Table 1: Comparison of modern optimal control approaches.The proposed OptCtrlOP naturally covers all the merits in the sense of performing a single-phase direct-mapping paradigm that does not rely on known system dynamics, and supports arbitrary input-domain queries.4) Control Solution Continuity.Control functions are defined on a continuous domain (typically time), although being intractable for numerical solvers.Hence resorting to discretization in the input domain gives rise to the trade-off in the precision of the control solution and the computational cost, as well as the truncation errors.While the discrete solution can give point-wise queries, learning a control function for arbitrary time queries is much more valued.5) Running Phase.A two-phase model (Chen et al., 2018;Wang et al., 2021a;Hwang et al., 2022) can (partially) overcome the above issues at the cost of introducing an auxiliary dynamics inference phase.This thread of works first approximates the state dynamics by a differentiable surrogate model and then, in its second phase, solves an optimization problem for control variables (more explanation in Appx.B).However, the two-phase paradigm increases computational cost and manifests inconsistency between the two phases.A motivating example in Fig. 1 shows the two-phase paradigm leads to crucial failures.When the domain of phase-2 optimization goes outside the training distribution in phase-1, this method might collapse.Table 1 compares the methods regarding the above aspects.We propose an instance-solution operator perspective for learning to solve OCPs, thereby tackling the issues above.The contributions are:1. We propose the operator perspective and solve OCPs by learning direct mappings from OCPs to their solutions.The design holds the following merits.The system dynamics is implicitly learned during the training, which relies on neither any explicit form of the system nor the optimization process at test time.As such the operator can be reused and generalized to similarly-formed OCPs without retraining, and such generalization ability is even missing for learning-free solvers.The single-phase direct mapping paradigm avoids iterative processes with substantial speedup.2. We theoretically validate the instance-solution mapping perspective by leveraging Pontryagin's Maximum Principle, thereby converting Eq. 1 to a boundary value problem.We instantiate a neural solver: OptCtrlOP (Optimal Control OPerator), and derive bounds on its approximation error.3. Experiments on both synthetic and real systems show that OptCtrlOP is versatile for various forms of OCPs.It achieves about 100x speedup against MLP baseline, and 10Kx speedup (on synthetic environments) over classical direct method solvers.It also generalizes well on both inand out-of-distribution OCP instances.Related Works.Most OCPs can not be solved analytically, and numerical methods are developed, which can be mainly categorized into three groups: direct methods, indirect methods, and dynamic programming.Our work bears a resemblance to indirect methods.Due to page limitation, we leave the detailed discussion on related works to Appendix B."}
{"paper_id": 461, "introduction": "Federated learning (FL) (McMahan et al., 2017) has emerged as an attractive distributed learning paradigm that leverages a large number of clients to collaboratively learn a joint model with decentralized training data under the coordination of a centralized server.In contrast with centralized learning, the FL architecture allows for preserving clients' privacy and reducing the communication burden caused by transmitting data to the server.While there is a rich literature in distributed optimization in the context of machine learning, FL distinguishes itself from traditional distributed optimization in two key challenges: high degrees of system and statistical heterogeneity (Kairouz et al., 2019).In an attempt to address the heterogeneity and improve the efficiency of FL, various optimization methods have been developed for FL.In particular, the federated averaging algorithm (FedAvg) (McMahan et al., 2017) is the current state-of-the-art method for FL.In each communication round, FedAvg leverages local computation at each client and employs a centralized server to aggregate and update the global model parameter.While FedAvg has demonstrated empirical success in heterogeneous settings, it fails to fully address the underlying challenges associated with heterogeneity.For example, FedAvg randomly selects a subset of clients in each iteration regardless of their statistical heterogeneity, which has been shown to diverge empirically in settings where data samples of each client follow a non-identical and independent distribution (non-IID).A recent trend of improving FL efficiency focuses on adaptive client selection during the FL training process, such as (Ruan et al., 2021;Karimireddy et al., 2020;Li et al., 2020a;Wang et al., 2020c;b;Cho et al., 2020;Wang et al., 2020a;Rothchild et al., 2020;Lai et al., 2021).However, these studies implicitly assume that all learning phases during the FL training process are equally important.Unfortunately, this assumption has recently been revealed to be invalid due to the existence of critical learning (CL) periods, i.e., the final quality of a deep neural network (DNN) model is determined by the first few training epochs, in which deficits such as low quality or quantity of training data will cause irreversible model degradation.Notably, this phenomenon was revealed in the latest series of works (Achille et al., 2019;Jastrzebski et al., 2019;Golatkar et al., 2019;Jastrzebski et al., 2021) for centralized learning, and in (Yan et al., 2022) for FL settings.Despite their insightful findings, there Algorithm 1 FedAvg Input: M, \u03b7, E, \u03b8 (0) , T 1: for t = 0, 1,Server selects a subset M (t) of M clients at random 3:Server sends \u03b8 (t) to all selected clients 4:Client k \u2208 M (t) updates \u03b8 (t) via E iterations of SGD on D k with stepsize \u03b7 to obtain \u03b8 (t+1) k 5:Each selected client k \u2208 M (t) sends \u03b8 Server aggregates the \u03b8's as \u03b8 (t+1) := k\u2208M (t)N k k\u2208M (t) N k \u03b8 (t+1) k 7: end for in FL, we refer interested readers to (Kairouz et al., 2019).Unlike these works that are agnostic to the existence of CL periods, we design a novel CL periods-aware FL framework.Importantly, we remark that our CL periods-aware FL framework, FedCL is orthogonal to these methods, since FedCL merely augments a state-of-the-art FL method to adaptively determine the number of clients that participate in each FL training round, rather than changing the way how the FL method selects clients."}
{"paper_id": 462, "introduction": "In recent years, object-centric representations have emerged as a paradigm shift in machine perception.Intuitively, inference or prediction tasks in down-stream applications are significantly simplified by reducing the dimensionality of the hypothesis space from raw perceptual inputs, such as pixels or point-clouds, to something more akin to a traditional state-space representation.While reasoning over objects rather than pixels has long been the aspiration of machine vision research, it is the ability to learn such a representation in an unsupervised, generative way that unlocks the use of large-scale, unlabelled data for this purpose.As consequence, research into object-centric generative models (OCGMs) is rapidly gathering pace.Central to the success of an OCGM are the inductive biases used to encourage the decomposition of a scene into its constituent components.With the field still largely in its infancy, much of the work to date has confined itself to 2D scene observations to achieve both scene inference (e.g.Eslami et al., 2016;Burgess et al., 2019;Greff et al., 2016;2017;Locatello et al., 2020;Engelcke et al., 2019;2021) and, in some cases, generation (e.g.Engelcke et al., 2019;2021).In contrast, unsupervised methods for object-centric scene decomposition operating directly on 3D inputs remain comparatively unexplored (Elich et al., 2022;Stelzner et al., 2021) -despite the benefits due to the added information contained in the input.As a case in point, Stelzner et al. (2021) recently established that access to 3D information significantly speeds up learning.Another benefit is that, for the parts of an object visible to a 3D sensor, object shape is readily accessible and does not have to be inferred, either from a single view (e.g.Liu et al., 2019;Kato et al., 2018) or from multiple views (e.g.Yu et al., 2021;Xie et al., 2019).We conjecture that object shape can serve as a highly informative inductive bias for object-centric learning.As we elaborate below, we reason that the asymmetry of a shape can be used to discover an object's pose, and pose can help to identify and locate an object in space.Here we present OBPOSE, an unsupervised OCGM that takes RGB-D images (or video) as input and learns to segment the underlying scene into its constituent 3D objects, as well as into an explicit background representation.As we will show, OBPOSE can also be used for scene generation and editing.Inspired by prior art in 2D settings (Eslami et al., 2016;Crawford & Pineau, 2019;Lin et al., 2020;Kosiorek et al., 2018;Jiang et al., 2019;Wu et al., 2021), OBPOSE factorises its latent embedding into a component capturing an object's location and appearance (where and what components, respectively).This factorisation provides a strong inductive bias, helping the model to disentangle its input into meaningful concepts for downstream use.A key contribution of OBPOSE is the introduction of pose (i.e.location and orientation) as a novel inductive bias.OBPOSE is not a pose-estimation model.Rather, OBPOSE infers pose information from an object's shape to reduce apparent variance and to simplify the learning of the model's what component in 3D (see fig. 11 in the appendix) -ultimately for use in downstream tasks like segmentation and scene editing.In an OGGM, the intuition is that the variance we observe in an object's pose should be made invariant in the latent space.OBPOSE infers pose information without supervision, using a minimum volume principle defined using the tightest bounding box that constrains the object.Effectively, the tightest bounding box will reveal asymmetries in an object's shape, if there are any, which can be used to constrain the object's orientation.We further propose a voxelised approximation approach that recovers an object's shape in a computationally tractable way from a neural radiance field (NeRF) (Mildenhall et al., 2020).Although the recovery of an object's shape from a NeRF can be prohibitively expensive, our approach allows this to be integrated efficiently into the OBPOSE training loop.In a series of experiments, OBPOSE outperforms the current state-of-the-art in 3D scene inference, ObSuRF (Stelzner et al., 2021), by significant margins.Evaluations are performed on the CLEVR dataset (Johnson et al., 2017), the MultiShapeNet dataset (Stelzner et al., 2021;Chang et al., 2015), and the YCB dataset for unsupervised scene segmentation (Calli et al., 2015), in the latter case using both RGB-D moving-objects (video) and multi-view static scenes.An ablation study on the OBPOSE encoder serves to validate the design decisions that distinguish its use of attention from alternative attention mechanisms represented by Slot Attention (Locatello et al., 2020) and GENESIS-v2 (Engelcke et al., 2021).In summary, the key contributions of this paper are: (1) a new state-of-the-art unsupervised scene segmentation model for 3D, OBPOSE, together with insights into its design decisions; (2) a novel inductive bias for 3D OCGMs, pose, together with its motivation; and (3) a general method for fast shape evaluation from NeRFs.Object-NeRFWorld coordinate frame RGB-D Object coordinate frame Coordinate Origin Coordinate Origin Coordinate Origin Coordinate Origin Coordinate Origin .Each point within a slot is coloured to represent a specific object.Note that while an object may move in the world frame (middle row), it appears stationary when viewed from the perspective of its normalised frame (bottom row)."}
{"paper_id": 463, "introduction": "Crowdsourcing labels for supervised learning has become quite common in the last two decades, notably for image classification with datasets such as CIFAR-10 and Imagenet.Using a crowd of workers is fast, simple (see Figure 1) and less expensive than using experts.Furthermore, aggregating crowdsourced labels instead of working directly with a single one enables modeling the sources of possible ambiguities and directly take them into account in the training pipeline (Aitchison, 2021).With deep neural networks nowadays common in many applications, both the architectures and data quality have a direct impact on the model performance (M\u00fcller et al., 2019;Northcutt et al., 2021b) and on calibration (Guo et al., 2017).Yet, depending on the crowd and platform's control mechanisms, the obtained label quality might vary and harm generalization (Snow et al., 2008).Popular label aggregation schemes take into account the uncertainty related to workers' abilities: for example by estimating confusions between classes, or using a latent variable representing each worker trust (Dawid & Skene, 1979;Kim & Ghahramani, 2012;Sinha et al., 2018;Camilleri & Williams, 2019).This leads to scoring workers without taking into account the inherent difficulty of a task at stake.Inspired by the Item Response Theory (IRT) (Birnbaum, 1968), Whitehill et al. (2009) combined both the task difficulty and the worker's ability in a feature-blind fashion for label aggregation.They only require labels but not the associated featuresfoot_0 .In the classical supervised learning setting, the labels are said to be hardi.e., a Dirac mass on one class.Multiple crowdsourced labels induce soft labelsi.e., probability distributions over the classes -for each task.Our motivation is to identify ambiguous tasks from their associated features, hence discarding hurtful tasks (such as the one illustrated on Figure 2b).Recent work on data-cleaning in supervised learning (Han et al., 2019;Pleiss et al., 2020;Northcutt et al., 2021a) has shown that some images might be too corrupted or too ambiguous to be labeled by humans.Hence, one should not consider these tasks for label aggregation and learning since they might be harmful for generalization.In this work, we combine task difficulty scores with worker abilities scores, but we measure the task difficulty by incorporating feature information.We thus introduce the Weighted Area Under the Margin (WAUM), a generalization to the crowdsourcing setting of the Area Under the Margin (AUM) (Pleiss et al., 2020).The AUM is a confidence indicator in an assigned label defined for each training task.It is computed as an average of margins over scores obtained along the learning steps, and reflects how a learning procedure struggles to classify a task to an assigned label (see Figures 3 and5 to visualize how the AUM is connected to the classical margin from the kernel literature).The AUM is well suited when training a neural network (where the steps are training epochs) or other iterative methods.For instance, it has led to better network calibration (Park & Caragea, 2022) using MixUp strategy (Zhang et al., 2018), i.e., mixing tasks identified as simple and difficult by the AUM.The WAUM identifies harmful data points in crowdsourced datasets, so one can prune ambiguous tasks that degrade the generalization.It is a weighted average of workers AUM, where the weights reflect trust scores based on tasks difficulty and workers ability."}
{"paper_id": 464, "introduction": "Learning without labels is the most common way for humans to get to know the world (DiCarlo et al., 2012), and it has also been widely studied in machine learning for developing intelligent agents.In particular, many researchers focus on self-supervised learning (SSL) from dynamic visual input data, i.e., videos (Hurri & Hyv\u00e4rinen, 2003;Mobahi et al., 2009;Srivastava et al., 2015), which comes closest to the natural data perceived by humans.Recently, deep learning based video SSL methods have also shown superior performance over traditional non-deep learning methods (Wang et al., 2021a;Duan et al., 2022).However, there is still a large room for improvement considering the gap between the unsupervised learning abilities of deep learning models and humans.One notable difference between deep video SSL methods and human unsupervised learning is that the former typically learn discriminative representations by considering the inherent data properties, such as the clip order (Misra et al., 2016), the spatiotemporal coherence (Vondrick et al., 2018), the transformations exerted (Jenni et al., 2020), etc., and propose various pretext tasks accordingly.While for humans, the self-awareness of the semantic change or consistency in the input stimuli is essential for learning without labels (Melcher & Colby, 2008).Besides, the encoded representations in the brain are not left unchanged but kept being reorganized to yield a representation structure with strengthened associations among perceptually similar representations (Diekelmann & Born, 2010).Fig. 1 shows an overall comparison.This discrepancy inspired us to propose a new video SSL method by taking inspiration from cognitive science and neuroscience on human visual perception.Recently, Illing et al. (2021) proposed a bio-inspired unsupervised learning rule that treats the presence of saccades as a global synaptic modulator.However, it is less powerful due to the inherent difficulty in optimizing deep networks with layer-wise optimization.Human visual perception is mainly accomplished by alternating saccade and fixation when the heads are relatively still.The former is the rapid foveal motion from one target of interest to another, We further encourage the semantic consistency within a fixation duration by minimizing the prediction error (PE) when using the current state to predict that of another time point in the same fixation duration.In this way, PE can serve as an extra supervision signal to avoid semantic discrepancy during semantic-change-aware contrastive learning.This is also biorational, as PE is known as an important modulator in perception, attention, and motivation control (Den Ouden et al., 2012).To enhance the association among the previously learned finer-grained semantics, inspired by the reorganization in human representation learning (Diekelmann & Born, 2010), we incorporate prototypical contrastive learning (Li et al., 2020) to gradually redistribute the representations.The learned representations are pulled towards their corresponding prototypes and pushed away from other prototypes.Such post-learning reorganization facilitates grouping unseen input stimuli into meaningful categories based on similarity, which leads to improved Top-1 retrieval accuracy compared with previous contrastive-based video SSL methods.In summary, we propose a video SSL framework by taking inspiration from cognitive science and neuroscience on human visual perception.We first exploit the presence of saccades as an indicator of semantic change in a contrastive learning framework for modeling the role of self-awareness in human representation learning.Then, we model the semantic consistency in the input by minimizing PE between a predicted and the true states of different time points during a fixation.Third, we incorporate prototypical contrastive learning to reorganize the learned representations such that the associations among perceptually similar ones would be strengthened after redistribution.Experiments show that the proposed bio-inspired video SSL method significantly improves the Top-1 video retrieval accuracy on UCF101, and achieves superior performance on downstream tasks such as action recognition.The code and the pre-trained models will be released."}
{"paper_id": 465, "introduction": "Multi-agent reinforcement learning (MARL) has long been a go-to tool in complex robotic and strategic domains (RoboCup, 2019;OpenAI, 2019).However, learning effective policies with sparse reward from scratch for large-scale multi-agent systems remains challenging.One of the challenges is that the joint observation-action space grows exponentially with varying numbers of agents.Meanwhile, the sparse reward signal requires a large number of training trajectories.Hence, applying existing MARL algorithms directly to complex environments with a large number of agents is not effective.In fact, they may produce agents that do not collaborate with each other even when it is of significant benefit (Zhang et al., 2021;Yang & Wang, 2020).There are several lines of work related to the large-scale MARL problem with sparse reward, including: reward shaping (Hu et al., 2020), curriculum learning (Chen et al., 2021), and learning from demonstrations (Huang et al., 2021).Among these approaches, the curriculum learning paradigm, in which the difficulty of experienced tasks and the population of training agents progressively grow, shows particular promise.In automatic curriculum learning (ACL), a teacher (curriculum generator) learns to adjust the complexity and sequencing of tasks faced by a student (curriculum learner).Several works have even proposed multi-agent ACL algorithms, based on approximate or heuristic approaches to teaching, such as DyMA-CL (Wang et al., 2020c), EPC (Long et al., 2020), and VACL (Chen et al., 2021).However, DyMA-CL and EPC rely on a framework of an off-policy student with replay buffer, and ignore the forgetting problem that arises when the agent population size grows.ACL relies on the strong assumption that the value of the learned policy does not change when agents switch to a different task.Moreover, the teacher in these approaches is still facing an unmitigated non-stationarity problem due to the ever-changing student strategies.In addition, if we somewhat expand the ACL paradigm and presume that the teacher may have another purpose for the sequence of tasks performed by the student, another class of larger-scale MARL solutions should be mentioned.Namely, hierarchical MARL, which learns temporal abstraction with more dense rewards, including: skill discovery (Yang et al., 2019), option as response (Vezhnevets et al., 2019), role-based MARL (Wang et al., 2020b), and two levels of abstraction (Pang et al., 2019).Alas, hierarchical MARL mostly focuses on one specific task with a fixed number of agents and does not consider the transfer ability of learned complementary skills.In this paper, we informally give our answer to a question:Can an elaborate combination of ACL and hierarchical principles learn complex cooperation with sparse reward in MARL?Specifically, we present a novel automatic curriculum learning algorithm, Skilled Population Curriculum (SPC), which learns cooperative behaviors from scratch.The core idea of SPC is to encourage the student to learn skills from tasks with different numbers of agents.Motivation from the real world is team sports, where players often train their skills by gradually increasing the difficulty of tasks and the number of coordinating players.In particular, we implement SPC with three key components with the teacher-student framework.First, to solve the final complex cooperative tasks, we model the teacher as a contextual bandit, where we utilize an RNN-based (Hochreiter & Schmidhuber, 1997) imitation model to represent student policies and use this to generate the bandit's context.Second, to handle the varying number of agents across these tasks, motivated by the transformer (Vaswani et al., 2017), which can process sentences of varying lengths, we implement population-invariant communication by treating each agent's message as a word.Thus, a self-attention communication channel is used to support an arbitrary number of agents sharing their messages.Third, to learn transferable skills in the sparse reward setting, we utilize the skill framework in the student.Agents communicate on the high level about a set of shared low-level policies.Empirical results show that our method achieves state-of-the-art performance in several tasks in the multi-particle environment (MPE) (Lowe et al., 2017) and the challenging 5vs5 competition in Google Research Football (GRF) (Kurach et al., 2019)."}
{"paper_id": 466, "introduction": "Deep convolutional neural networks (CNNs) are particularly successful in certain tasks such as image classification.Such tasks generally entail the approximation of functions of a large number of variables, for instance the number of pixels which determine the content of an image.Learning a generic high-dimensional function is plagued by the curse of dimensionality: the rate at which the generalisation error \u03f5 decays with the number of training samples n vanishes as the dimensionality d of the input space grows, i.e. \u03f5(n) \u223c n -\u03b2 with \u03b2 = O(1/d) (Wainwright, 2019).Therefore, the success of CNNs in classifying data whose dimension can be in the hundreds or more (Hestness et al., 2017;Spigler et al., 2020) points to the existence of some underlying structure in the task that CNNs can leverage.Understanding the structure of learnable tasks is arguably one of the most fundamental problems in deep learning, and also one of central practical importance-as it determines how many examples are required to learn up to a certain error.A popular hypothesis is that learnable tasks are local and hierarchical: features at any scale are made of sub-features of smaller scales.Although many works have investigated this hypothesis (Biederman, 1987;Poggio et al., 2017;Kondor & Trivedi, 2018;Zhou et al., 2018;Deza et al., 2020;Kohler et al., 2020;Poggio et al., 2020;Schmidt-Hieber, 2020;Finocchio & Schmidt-Hieber, 2021;Giordano et al., 2022), there are no available predictions for the exponent \u03b2 for deep CNNs trained on tasks with a varying degree of locality or a truly hierarchical structure.In this paper we perform such a computation in the overparameterised regime, where the width of the hidden layer of the neural networks diverges and the network output is rescaled so as to converge to that of a kernel method (Jacot et al., 2018;Lee et al., 2019).Although the deep networks deployed in real scenarios do not generally operate in such regime, the connection with the theory of kernel regression provides a recipe for computing the decay of the generalisation error with the number of training examples.Namely, given an infinitely wide neural network, its generalisation abilities depend on the spectrum of the corresponding kernel (Caponnetto & De Vito, 2007;Bordelon et al., 2020): the main challenge is then to characterise this spectrum, especially for deep CNNs whose kernels are rather cumbersome and defined recursively (Arora et al., 2019).This characterisation is the main result of our paper, together with the ensuing study of generalisation in deep CNNs.1.1 OUR CONTRIBUTIONS More specifically, this paper studies the generalisation properties of deep CNNs with nonoverlapping patches and no pooling (defined in Sec. 2, see Fig. 1 for an illustration), trained on a target function f * by empirical minimisation of the mean squared loss.We consider the infinitewidth limit (Sec.3) where the model parameters change infinitesimally over training, thus the trained network coincides with the predictor of kernel regression with the Neural Tangent Kernel (NTK) of the network.Due to the equivalence with kernel methods, generalisation is fully characterised by the spectrum of the integral operator of the kernel: in simple terms, the projections on the eigenfunctions with larger eigenvalues can be learnt (up to a fixed generalisation error) with fewer training points (see, e.g., Bach (2021)).Spectrum of deep hierarchical kernels (Thm.3.1).Due to the network architecture, the hidden neurons of each layer depend only on a subset of the input variables, known as the receptive field of that neuron (highlighted by coloured boxes in Fig. 1, left panel).We find that the eigenfunctions of the NTK of a hierarchical CNN of depth L + 1 can be organised into sectors l = 1, . . ., L associated with the hidden layers of the network (Thm.3.1).The eigenfunctions of each sector depend only on the receptive fields of the neurons of the corresponding hidden layer: if we denote with d eff (l) the size of the receptive fields of neurons in the l-th hidden layer, then the eigenfunctions of the l-th sector are effectively functions of d eff (l) variables.We characterise the asymptotic behaviour of the NTK eigenvalues with the degree of the corresponding eigenfunctions (Thm.3.1) and find that it is controlled by d eff (l).As a consequence, the eigenfunctions with the largest eigenvalues-the easiest to learn-are those which depend on small subsets of the input variables and have low polynomial degree.This is our main technical contribution and all of our conclusions follow from it.Adaptivity to the spatial structure of the target (Cor.4.1).We use the above result to prove that deep CNNs can adapt to the spatial scale of the target function (Sec.4).More specifically, by using rigorous bounds from the theory of kernel ridge regression (Caponnetto & De Vito, 2007) (reviewed in the first paragraph of Sec. 4), we show that when learning with the kernel of a CNN and optimal regularisation, the decay of the error depends on the effective dimensionality of the target f * -i.e., if f * only depends on d eff adjacent coordinates of the d-dimensional input, then \u03f5 \u223c n -\u03b2 with \u03b2 \u2265 O(1/d eff ) (Cor.4.1, see Fig. 1 for a pictorial representation).We find a similar picture in ridgeless regression by using non-rigorous results derived with the replica method (Bordelon et al., 2020;Loureiro et al., 2021) (Sec. 5).Notice that for targets which are spatially localised (or sums of spatially localised functions), the rates achieved with deep CNNs are much closer to the Bayes-optimal rates-realised when the architecture is fine-tuned to the structure of the targetthan \u03b2 = O(1/d) obtained with the kernel of a fully-connected network.Moreover, we find that hierarchical functions generated by the output of deep CNNs are too rich to be efficiently learnable in high dimensions (Lemma 5.1).We confirm these results through extensive numerical studies and find them to hold even if the nonoverlapping patches assumption is relaxed (Subsec.G.4)."}
{"paper_id": 467, "introduction": "Traditional instance segmentation Lin et al. (2014); Cordts et al. (2016) methods often assume that objects in images can be categorized into a finite set of predefined classes (i.e., closed-world).Such an assumption, however, can be easily violated in many real-world applications, where models will encounter many new object classes that never appeared in the training data.Therefore, researchers recently attempted to tackle the problem of Open-World Instance Segmentation (OWIS) Wang et al. (2021), which targets class-agnostic segmentation of all objects in the image.Prior to this paper, most existing methods for OWIS are of two-stage Wang et al. (2022); Saito et al. (2021), which detect bounding boxes of objects and then segment them.Despite their promising performances, such a paradigm cannot handle and recover if object bounding boxes are not detected.In contrast, a transformer-based approach called Mask2Former Cheng et al. (2022) has recently been introduced, yet only for closed-world instance segmentation.Based on the Mask2Former, we propose a Transformer-based Open-world Instance Segmentation method named TOIS.Note that our work is not just a straightforward adaptation of Mask2Former from close-world to open-world.This is because unlike closed-world segmentation, where the object categories can be clearly defined before annotation, the open-world scenario makes it challenging for annotators to label all instances completely or ensure annotation consistency across different images because they cannot have a well-defined finite set of object categories.As shown in Figure 1(a), annotators miss some instances.It still remains challenging that how to handle such incomplete annotations (i.e.some instances missed).Visualization results of our TOIS on UVO dataset.Here, the proposed TOIS is trained on COCO dataset and tested on UVO dataset.Our methods correctly segments many objects that are not labeled in COCO.(d -f).The AP 100 % of our TOIS vs.SOTA methods on COCO\u2192UVO, Cityscapes\u2192Mapillary, COCO\u2192UVO.(g).The AR 100 % of our TOIS vs. baseline Mask2Former on COCO.From right to left, with the total number of classes decreases (i.e. more instance annotations missed), the gain of our TOIS over baseline becomes larger, thanks to the capability of our model to handle incomplete annotations.Recent work LDET Saito et al. (2021) addresses this problem by generating synthetic data with a plain background, but based on a decoupled training strategy that can only be used in the two-stage method while our method is of single-stage.Another work called GGN Wang et al. (2022) handles such incomplete instance-level annotation issue by training a pairwise affinity predictor for generating pseudo labels.But training such an additional predictor is complicated and time-consuming.In contrast, our proposed TOIS method is end-to-end and simpler.We address this incomplete annotation issue via a novel regularization module, which is simple yet effective.Specifically, it is convenient to concurrently predict not only (1) instance masks but also a (2) foreground map.Ideally, as shown in Figure 1(b), the foreground region should be consistent with the union of all instance masks.To penalize their inconsistency, we devise a cross-task consistency loss, which can down-weight the adverse effects caused by incomplete annotation.This is because when an instance is missed in annotation, as long as it is captured by both our predictions of instance masks and foreground map, the consistency loss would be low and hence encourage such prediction.Experiments in Figure 1(g) show that such consistency loss is effective even when annotations miss many instances.And as in Figure 1(c), novel objects which are unannotated in training set have been segmented successfully by our method.So far, like most existing methods, we focus on the fully-supervised OWIS.In this paper, we further extend OWIS to the semi-supervised setting, where some training images do not have any annotations at all.This is of great interest because annotating segmentation map is very costly.Notably, our proposed regularization module can also benefit semi-supervised OWIS -consider an unlabeled image as an extreme case of incomplete annotation where all of the instance annotations are missed.Specifically, we perform semi-supervised OWIS by first warming up the network on the labeled set and then continuing training it with the cross-task consistency loss on the mixture of labeled and unlabeled images.Contributions.In a nutshell, our main contributions could be summarized as:1. Building upon a recently-proposed close-world segmentation method of Mask2Former, we propose a Transformer-based Open-world Instance Segmentation (TOIS) method.2. We propose a novel cross-task consistency loss that mitigates the issue of incomplete mask annotations, which is a critical issue for open-world segmentation in particular.3. We further extend the proposed method into a semi-supervised OWIS model, which effectively makes use of the unlabeled images to help the OWIS model training .4. Our extensive experiments demonstrate that the proposed method reaches the leading OWIS performance in the fully-supervised learning.(Figure 1(d-f)), and that our semi-supervised extension can achieve remarkable performance with a much smaller amount of labeled data."}
{"paper_id": 468, "introduction": "In many practical reinforcement learning (RL) applications, it is critical for an agent to meet certain constraints on utilities/costs while maximizing the reward.This problem is usually modeled as the constrained Markov decision processes (CMDPs) (Altman, 1999).Consider a CMDP with state space S, action space A, transition kernel P = {p a s \u2208 \u2206 S 1 : s \u2208 S, a \u2208 A}, reward and utility functions: r, c i : S \u00d7 A \u2192 [0, 1], 1 \u2264 i \u2264 m, and discount factor \u03b3. The goal of CMDP is to find a stationary policy \u03c0 : S \u2192 \u2206 A that maximizes the expected reward subject to constraints on the utility:where \u03c1 is the initial state distribution, b i 's are some thresholds and E \u03c0,P denotes the expectation when the agent follows policy \u03c0 and the environment transits following P.In practice, it is often the case that the environment on which the learned policy will deploy (the test environment) possibly deviates from the training one, due to, e.g., modeling error of the simulator, adversarial attack, and non-stationarity.This could lead to a significant performance degradation in reward, and more importantly, constraints may not be satisfied anymore, which is severe in safetycritical applications.For example, a drone may run out of battery and crash due to mismatch between training and test environments.This hence motivates the study of robust constrained RL in this paper.In this paper, we take a pessimistic approach in face of uncertainty.Specifically, consider a set of transition kernels P, which is usually constructed in a way to include the test environment with high probability (Iyengar, 2005;Nilim & El Ghaoui, 2004;Bagnell et al., 2001).The learned policy should satisfy the constraints under all these environments in P, i.e., \u2200P \u2208 P,which is equivalent to min P\u2208P E \u03c0,P [ \u221e t=0 \u03b3 t c(S t , A t )|S 0 \u223c \u03c1] \u2265 b i .At the same time, we aim to optimize the worst-case reward performance over P:1 \u2206 X denotes the probability simplex supported on the set X.(3)On one hand, a feasible solution to eq. ( 3) always satisfies eq. ( 2), and on the other hand, the solution to eq. ( 3) provides a performance guarantee for any P \u2208 P. We note that our approach and analysis can also be applied to the optimistic approach in face of uncertainty.In this paper, we design and analyze a robust primal-dual algorithm for the problem of robust constrained RL.In particular, the technical challenges and our major contributions are as follows.\u2022 We take the Lagrange multiplier method to solve the constrained policy optimization problem.A first question is that whether the primal problem is equivalent to the dual problem, i.e., whether the duality gap is zero.For non-robust constrained RL, the Lagrange function has a zero duality gap (Paternain et al., 2019;Altman, 1999).However, we show that this is not necessarily true in the robust constrained setting.Note that the set of visitation distribution being convex is one key property to show zero duality gap of constrained MDP (Altman, 1999;Paternain et al., 2019).In this paper, we constructed a novel counter example showing that the set of robust visitation distributions for our robust problem is non-convex.\u2022 In the dual problem of non-robust CMDPs, the sum of two value functions is actually a value function of the combined reward.However, this does not hold in the robust setting, since the worstcase transition kernels for the two robust value functions are not necessarily the same.Therefore, the geometry of our Lagrangian function is much more complicated.In this paper, we formulate the dual problem of the robust constrained RL problem as a minimax linear-nonconcave optimization problem, and show that the optimal dual variable is bounded.We then construct a robust primaldual algorithm by alternatively updating the primal and dual variables.We theoretically prove the convergence to stationary points, and characterize its complexity.\u2022 In general, convergence to stationary points of the Lagrangian function does not necessarily imply that the solution is feasible (Lin et al., 2020;Xu et al., 2020).We design a novel proof to show that the gradient belongs to the normal cone of the feasible set, based on which we further prove the robust feasibility of the obtained policy.\u2022 We apply and extend our results on an important uncertainty set referred to as \u03b4-contamination model (Huber, 1965).Under this model, the robust value functions are not differentiable and we hence propose a smoothed approximation of the robust value function towards a better geometry.We further investigate the practical online and model-free setting and design an actor-critic type algorithm.We also establish its convergence, sample complexity, and robust feasibility.We then discuss works related to robust constrained RL.Robust constrained RL.In (Russel et al., 2020), the robust constrained RL problem was studied, and a heuristic approach was developed.The basic idea is to estimate the robust value functions, and then to use the vanilla policy gradient method (Sutton et al., 1999) with the vanilla value function replaced by the robust value function.However, this approach did not take into consideration the fact that the worst-case transition kernel is also a function of the policy (see Section 3.1 in (Russel et al., 2020)), and therefore the \"gradient\" therein is not actually the gradient of the robust value function.Thus, its performance and convergence cannot be theoretically guaranteed.The other work (Mankowitz et al., 2020) studied the same robust constrained RL problem under the continuous control setting, and proposed a similar heuristic algorithm.They first proposed a robust Bellman operator and used it to estimate the robust value function, which is further combined with some non-robust continuous control algorithm to update the policy.Both approaches in (Russel et al., 2020) and(Mankowitz et al., 2020) inherit the heuristic structure of \"robust policy evaluation\" + \"non-robust vanilla policy improvement\", which may not necessarily guarantee an improved policy in general.In this paper, we employ a \"robust policy evaluation\" + \"robust policy improvement\" approach, which guarantees an improvement in the policy, and more importantly, we provide theoretical convergence guarantee, robust feasibility guarantee, and complexity analysis for our algorithms.Constrained RL.The most commonly used method for constrained RL is the primal-dual method (Altman, 1999;Paternain et al., 2019;2022;Liang et al., 2018;Stooke et al., 2020;Tessler et al., 2018;Yu et al., 2019;Zheng & Ratliff, 2020;Efroni et al., 2020;Auer et al., 2008), which augments the objective with a sum of constraints weighted by their corresponding Lagrange multipliers, and then alternatively updates the primal and dual variables.It was shown that the strong duality holds for constrained RL, and hence the primal-dual method has zero duality gap (Paternain et al., 2019;Altman, 1999).The convergence rate of the primal-dual method was investigated in (Ding et al., 2020;2021;Li et al., 2021b;Liu et al., 2021;Ying et al., 2021).Another class of method is the primal method, which is to enforce the constraints without resorting to the Lagrangian formulation (Achiam et al., 2017;Liu et al., 2020;Chow et al., 2018;Dalal et al., 2018;Xu et al., 2021;Yang et al., 2020).The above studies, when directly applied to robust constrained RL, cannot guarantee the constraints when there is model deviation.Moreover, the objective and constraints in this paper take min over the uncertainty set (see eq. ( 4)), and therefore have much more complicated geometry than the non-robust case.Robust RL under model uncertainty.Model-based robust RL was firstly introduced and studied in (Iyengar, 2005;Nilim & El Ghaoui, 2004;Bagnell et al., 2001;Satia & Lave Jr, 1973;Wiesemann et al., 2013;Lim & Autef, 2019;Xu & Mannor, 2010;Yu & Xu, 2015;Lim et al., 2013;Tamar et al., 2014), where the uncertainty set is assumed to be known, and the problem can be solved using robust dynamic programming.It was then extended to the model-free setting, where the uncertainty set is unknown, and only samples from its centroid can be collected (Roy et al., 2017;Wang & Zou, 2021;2022;Zhou et al., 2021;Yang et al., 2021;Panaganti & Kalathil, 2021;Ho et al., 2018;2021).There are also empirical studies on robust RL, e.g., (Vinitsky et al., 2020;Pinto et al., 2017;Abdullah et al., 2019;Hou et al., 2020;Rajeswaran et al., 2017;Huang et al., 2017;Kos & Song, 2017;Lin et al., 2017;Pattanaik et al., 2018;Mandlekar et al., 2017).These works focus on robust RL without constraints, whereas in this paper we investigate robust RL with constraints, which is more challenging.There is a related line of works on (robust) imitation learning (Ho & Ermon, 2016;Fu et al., 2017;Torabi et al., 2018;Viano et al., 2022), which can be formulated as a constrained problem.But their problem settings and approaches are fundamentally different from ours."}
{"paper_id": 469, "introduction": "Reinforcement learning (RL), powered by the generalization ability of machine learning structures, has been fairly successful in classical control tasks (Hafner & Riedmiller, 2011;Lillicrap et al., 2016) and problems like Atari (Mnih et al., 2013) and Go (Silver et al., 2016).RL aims to train a policy to achieve maximum reward or minimize the costfoot_1 , which is similar to control theoretic approaches on designing a controller.However, different from classical optimal control which requires full knowledge of transition dynamics, RL can learn the optimal policy from the past data directly by solving an optimization problem without the knowledge of the underlying dynamics.One of the mainstream methods to solve an RL problem is policy optimization via gradient descent.However, the convergence of the policy optimization algorithm heavily relies on an unapparent yet critical assumption of the system dynamics itself: stabilityfoot_2 .In addition, to ensure the convergence of policy optimization, it requires the Lipschitz property of the cost function and its gradient.In fact, in many of the existing RL benchmark examples such as OpenAI's classical control environments, the state space/actions/costs are clipped to ensure that the policy would never move to extreme conditions and costs/states are bounded, in order to reduce the error derivatives (Mnih et al., 2015).Unfortunately, similar formulations are not directly applicable to unstable systems, such as a LQR with an unstable state matrix (i.e. the spectral radius of the state matrix is outside the unit circle), as the standard policy gradient based methods are likely to fail.Motivated by the above issue, in this paper we aim to enable and speed up the convergence of policy gradient methods for unstable RL problems, we propose a logarithmic mapping method on loss functions supported by rigorous theoretical proofs and experimental results.The key contributions are summarized as follows.\u2022 We formally define the unstable RL problem in the scope of \"input-to-output\" stability, with the input actions leading to a temporal growing effect against cost output.This is the first time the convergence issue of unstable RL problems is studied, we demonstrate that a major issue for policy gradient methods on unstable RL problems is the slow convergence rate, which is the due to large spectral radius of the Hessian matrix.\u2022 We propose a simple yet effective logarithmic mapping to alleviate this issue and speed up the convergence.We show both theoretical advantage and experimental results to support the contribution of faster convergence rate.Notably, our finite horizon problem setup does not require the bounded assumption of cost function.The experiments cover LQR examples to customized nonlinear cases with neural network based policy.\u2022 We provide an efficient method to find a better initialization of control policy by optimizing it over the spectral norm of controlled system.We use it as a fast pre-processing step to effectively save the computation cost and allow larger learning rate for fast convergence."}
{"paper_id": 470, "introduction": "Understanding the decision of a deep learning model is becoming more and more important.Especially for safety-critical applications such as the medical domain or autonomous driving, it is often either legally (Bibal et al., 2021) or by the practitioners required to be able to trust the decision and evaluate its reasoning (Molnar, 2020).Due to the high dimensionality of images, most previous work on interpretable models for computer vision combines the deep features computed by a deep neural network with a method that is considered interpretable, such as a prototype based decision tree (Nauta et al., 2021).While approaches for measuring the interpretability without humans exist for conventional machine learning algorithms (Islam et al., 2020), they are missing for methods including deep neural networks.In this work, we propose a novel sparse and low-dimensional SLDD-Model which offers measurable aspects of interpretability.The key aspect is a heavily reduced number of features, out of which only very few are considered per class.Humans can only consider 7 \u00b1 2 aspects at once (Miller, 1956) and could therefore follow a decision that uses that many features.To be intelligible for all humans, we aim for an average of 5 features per class.Having a reduced number of features makes it feasible to investigate every single feature and understand its meaning: We are able to align several of the learned features with human concepts post-hoc.The combination of reduced features and sparsity therefore increases both global How does the model behave?and local interpretability Why did the model make this decision?, demonstrated in Figure 1.Our proposed method generates the SLDD-Model by utilizing glm-saga (Wong et al., 2021) to compute a sparse linear classifier for selected features, which we then finetune to the sparse structure.We apply feature selection instead of a transformation to reduce the computational load and preserve the original semantics of the features, which can improve interpretability (Tao et al., 2015), especially if a more interpretable model like B-cos Networks (B\u00f6hle et al., 2022) is used.Additionally, we propose a novel loss function for more diverse features, which is especially relevant when one class depends on very few features, since using more redundant features limits the total information available for the decision.Our main contributions are as follows:\u2022 We present a pipeline that ensures a model with increased global and local interpretability which identifies a single class with just few, e.g. 5, features of its low-dimensional representation.We call the resulting model SLDD-Model.\u2022 Our novel feature diversity loss ensures diverse features.This increases the accuracy for the extremely sparse case.\u2022 We demonstrate the competitive performance of our proposed method on four common benchmark datasets in the domain of fine-grained image classification as well as ImageNet-1K (Russakovsky et al., 2015), and show that several learned features for algorithmic decision-making can be directly connected to attributes humans use.\u2022 Code will be published upon acceptance."}
{"paper_id": 471, "introduction": "UAVs have gained considerable attention and are widely used for various purposes because of their high manoeuvrability and flexibility.For example, quadrotors are widely deployed for inspection, reconnaissance, and rescue.As control strategies evolve, novel scenarios for UAVs, such as aerial grasping, transporting, and bridge inspection (Ruggiero et al., 2018), require more precise trajectory tracking.Especially in the outdoor environment, unpredictable and changing wind field conditions pose substantial challenges to the stability of UAVs.Rotor blades are affected by induced airflow caused by the wind, which creates complex and non-stationary aerodynamic interactions (see Appendix B.6.3).From security and policy perspectives, demonstrating that UAVs can operate safely and reliably in unpredictable environments with various distributions is an essential requirement.It is also the premise for future medical robots, autonomous cars, and manned aerial vehicles to be widely accepted.Many areas have benefited from data-driven approaches.However, they are susceptible to performance degradation after generalization.And the majority of deep learning algorithms heavily rely on the I.I.D assumption for data, which is generally violated in practice due to domain generalization (Zhou et al., 2022).Nevertheless, neural networks may lose their robustness when confronted with OoD data.Many cases of failure in DNN originate from shortcut learning in the learning process (Geirhos et al., 2020).The damage to the UAV is undoubtedly considerable if the UAV cannot adjust to the changing environment, i.e., it is unstable or even crashes in an OoD situation.One significant objective of this paper is to propose a control algorithm to enable UAVs to maintain accurate control even in the case of environment domain shifts.Our Contributions.UAVs interact with the changing environment, resulting in complex environment-dependent uncertain aerodynamics, called unknown dynamics, that are tricky to model and significantly impact precise control.Previous data-driven controllers attempt to solve the problem by estimating the unknown dynamics, while the estimation accuracy and the performance of the controllers are limited by the environment domain shifts in tests.This paper presents a methodology for adaptive flight control problems, focusing on enabling UAVs to fly under unknown environments.Compared with previous works, our proposed OoD-Control algorithm can provide performance guarantees under domain shifts of the environment distribution.Compared with previous state-of-the-art work (Shi et al., 2021), the proposed OoD-Control method does not require strong assumptions, for example, e-ISS stability and a fully actuated system.Additionally, our algorithm has a greater capacity for generalization.For different distributions of the environment, we show theoretically that the bound on the prediction error of the unknown dynamics remains constant over a certain range of perturbations.Besides, simulated results under challenging aerodynamic conditions indicate that the OoD-Control algorithm achieves better control performance than the SOTA deep learning algorithms."}
{"paper_id": 472, "introduction": "In this work, we aim to address the problem of generating a realistic talking head video given one still source image and one dynamic driving video, which is widely known as talking head video generation.A high-quality talking head generation model needs to imitate vivid facial expressions and complex head movements, and should be applicable for different facial identities presenting in the source image and the target video.It has been attracting rapidly increasing attention from the community, and a wide range of realistic applications remarkably benefits from this task, such as digital human broadcast, AI-based human conversation, and virtual anchors in films.Significant progress has been achieved on this task in terms of both quality and robustness in recent years.Existing works mainly focus on learning more accurate motion estimation and representation in 2D and 3D to improve the generation.More specifically, 2D facial keypoints or landmarks are learned to model the motion flow (see Fig. 1(c)) between the source image and any target image in the driving video ( Zhao et al. (2021); Zakharov et al. (2019); Hong et al. (2022)).Some works also consider utilizing 3D facial prior model (e.g. 3DMMBlanz & Vetter (1999)) with decoupled expression codes (Zhao et al., 2021;Zakharov et al., 2019) or learning dense facial geometries in a self-supervised manner (Hong et al., 2022) to model complex facial expression movements to produce more fine-grained facial generation.However, no matter how accurately the motion can be estimated and represented, highly dynamic and complex motions in the driving video cause ambiguous generation from the source image (see Fig. 1(d)), because the still source image cannot provide sufficient appearance information for occluded regions or delicate expressions, which severely produces artifacts and significantly degrades the generation quality.Intuitively, we understand that human faces are symmetrical and highly structured, and many regions of the human faces are essentially not discriminative.For instance, only blocking a very small eye region of a face image makes a well-trained facial recognition model largely drop the recognition performance (Qiu et al., 2021), which indicates to a certain extent that the structure and appearance representations of human faces crossing different face identities are generic and transferable.Therefore, learning global facial priors on spatial structure and appearance from all available training face images, and utilizing the learned facial priors for compensating the dynamic facial synthesis are critically important for high-fidelity talking head generation.However, existing works did not explore these beneficial facial priors to address facial ambiguities in generation from large head motions.In this paper, to effectively deal with the ambiguities in the dramatic appearance changes from the still source image, we propose an implicit scale conditioned Memory Compensation Network, coined as MCNet, to learn and transfer global facial representations to compensate ambiguous facial details and guarantee completeness for a high-fidelity generation.Specifically, we design and learn a global spatial meta memory bank.The optimization gradients from all the training images during training contribute together to the updating of the meta memory, and thus it can capture the global and most common facial appearance and structure representations for the transferring.Since the different source face images contain distinct scales, to more effectively query the learned meta memory bank, we propose an implicit scale conditioned memory module (ISCM) (see Fig. 3).As the detected discrete facial keypoints inherently contain the scale information of the face, we first learn an implicit scale representation from the discrete keypoint coordinates, and further use it to condition on the query of the meta memory bank to obtain a scale-aware memory bank, which can more effectively compensate the feature of faces with different scales.The compensation is performed through a memory compensation module (MCM) (see Fig. 4).The warped feature map generated from the estimated motion field queries the scale-aware memory bank through a dynamic cross-attention mechanism to output a refined compensated feature map for the final generation.We conduct extensive experiments to evaluate the proposed MCNet on two competitive talking head generation datasets (i.e.VoxCeleb (Nagrani et al., 2017) and CelebV (Wu et al., 2018).Experimental results demonstrate the effectiveness of learning global facial memory to tackle the appearance ambiguities in the talking head generation, and also show clearly improved generation results from both qualitative and quantitative perspectives, achieving state-of-the-art performances.In summary, our main contribution is three-fold:\u2022 We propose to learn a global facial meta memory bank to transfer representative facial representations to handle the appearance and structure ambiguities caused by the highly dynamic generation from a still source image.To the best of our knowledge, it is the first exploration in the literature to model global facial representations for effectively improving the ambiguities in talking head generation.\u2022 We propose a novel implicit scale conditioned memory compensation network (MCNet) for talking head video generation, in which an implicit scale conditioned memory module (ISCM) and a facial memory compensation module (MCM) are designed to respectively perform the scale-aware memory learning and the feature compensation tasks.\u2022 Qualitative and quantitative experiments extensively show the effectiveness of the learned meta memory bank for addressing the ambiguities in generation, and our framework establishes a clear state-of-the-art performance on the talking head generation.The generalization experiment also shows that the proposed modules can effectively boost the performance of different talking head generation models."}
{"paper_id": 473, "introduction": "An essential goal of reinforcement learning (RL) is to solve tasks humans cannot solve, such as those with innumerable action spaces.For instance, recommender systems have a set of millions of discrete items, and robotic control requires acting in continuous dimensions.In such domains, all the actions cannot be enumerated feasibly to perform RL.Thus, actions are associated with continuous parameterizations or representations that enable generalized learning.We consider continuous and large parameterized discrete action spaces as problems of innumerable actions.Typically innumerable action space tasks are solved in two phases: retrieval and selection (See Fig. 1).QT-Opt (Kalashnikov et al., 2018) learns robotic manipulation by retrieving actions from a distribution fitted using the cross entropy method and selecting the action that maximizes a learned Q-function.Similarly, Dulac-Arnold et al. (2015) performs large discrete action RL by acting in the space of action representations, retrieving k-nearest-neighbors, and selecting with a Q-function.Continuous actor-critic approaches like DDPG and SAC (Lillicrap et al., 2015;Haarnoja et al., 2018) directly learn an actor with a learning objective of retrieving an action that maximizes a Q-function.However, action retrieval in these approaches takes the form of a single action, a distribution, or a local neighborhood defined on the action space.While these forms of retrieval work when the selection Q-function is smooth over the actions, it becomes a limiting factor in complex action spaces.The action space can be imprecise, noisy, high-dimensional, or independently derived from the task (such as in recommender systems), leading to a mismatch between the action representations and their effects on the task.Therefore, to enable efficient reinforcement learning in complex and innumerable action spaces, we aim to address the goal of performing robust action retrieval.Our critical insight is to perceive the problem of action retrieval as one of listwise reinforcement learning (Sunehag et al., 2015).Concretely, the retrieval network is considered an RL agent with a modified action space of picking a list of k action candidates, trained to enable the selection Qfunction to maximize the environment reward (Fig. 1).Listwise retrieval improves the efficiency of RL in two ways.Firstly, retrieval lags behind selection during initial training because it is trained with RL loss.By hedging or diversifying the retrieved actions, our selection phase gets better candidates to maximize, enabling directed exploration of the task.Second, listwise retrieval can learn to adapt the list composition (e.g., diverse or similar) over training because the actions are not! Selection QAction Retrieval as Listwise RLFigure 1: Large discrete action set and continuous action space tasks cannot be solved with a single Q-function because it is infeasible to enumerate all the actions.A common approach is to (a) retrieve k actions, and (b) select only from those actions.We posit that the retrieval task can be generally seen as listwise RL, where the retrieval agent must learn to output the list of candidates as a whole.This enables flexible learning in complex innumerable action space RL.constrained to a predefined distribution or local neighborhood form.We show that the flexibility of listwise retrieval is crucial in complex action spaces.To this end, we propose a novel framework FLAIR, Flexible Listwise ActIon Retrieval, that can incrementally build a list of k candidate actions without enumerating all possible list combinations.We extend cascaded DQN (Chen et al., 2019c;Jain et al., 2021) to continuous action space resulting in a cascaded DDPG framework with k actors and k critics.Specifically, an actor outputs a candidate action for each list index while considering the state and the partially built list as input.Each cascaded actor is trained to maximize its associated critic's value, while the critics are trained to maximize their environment reward.Overall, each actor-critic pair learns to retrieve a candidate action that can optimize the environment reward when combined with the current list of candidates.Our primary contribution is introducing the problem of complex innumerable action spaces in reinforcement learning.We make the retrieval-selection approach flexible by incorporating listwise RL.We demonstrate our proposed method FLAIR learns to perform listwise action retrieval that enables flexible and efficient decision-making in innumerable action space tasks, such as recommender systems, a novel mine-world environment, and continuous control."}
{"paper_id": 474, "introduction": "Word embedding is one of the critical technologies in natural language processing (Pennington et al., 2014;Goldberg & Levy, 2014;Tang et al., 2014).It statistics the co-occurrence frequency between pairs of words within a given context in a large-scale training corpus to learn an encoder that can infer vectors for any words in a learned embedded space.A well-trained word embedding model is usually regarded as a knowledge graph (Matthews & Matthews, 2001;Wang et al., 2018), in which the meaning of a word is determined by its relationship to other words in the learned vector space.That is, analogies and correlations between words can be presented by the learned vectors (Hohman et al., 2018;Chersoni et al., 2021), which help the model associate seen objects with unseen objects.Recently, several works have explored using semantic word/text embedding as supervision signs for zero-shot learning and visual-linguistic pre-training, and have achieved impressive successes in various AI tasks (Qiao et al., 2017;Wang et al., 2018;Radford et al., 2021;Wang et al., 2022).On the other hand, deep neural networks are usually vulnerable to adversarial examples (Szegedy et al., 2014;Goodfellow et al., 2015;Madry et al., 2018;Bhojanapalli et al., 2021), which severely limits their applications in many security scenarios.Fortunately, some studies (Radford et al., 2021;Yu et al., 2022) have shown that the visual model trained with semantic supervised information has much more robust to distribution shift and adversarial examples than standard trained models.As a result, these preliminaries raise a natural question:What is the impact of semantic informations on adversarial robustness?To answer this question, we explore the relationship between semantic information and model robustness from two aspects: distribution and structural relevance.Firstly, we apply the canonical correlation analysis (CCA) (Hotelling, 1992), which can reflect connections between two random variables.to analyze the distribution relevance between the visual representation and the corresponding semantic word vector.We mainly analyze the correlation coefficient of natural and adversarial image representation with semantic word vector under non-robust and robust models (Madry et al.,   2018).The results in Figure 1 indicate that, for the non-robust model, the representation of the natural image has a high correlation with its corresponding word vector.In contrast, the adversarial image has a lower correlation.This result means that the adversarial attack will destroy the semantic information from the non-robust model, which responds to the previous observations in (Zhang & Zhu, 2019;Ilyas et al., 2019) via a new perspective.For the robust model trained on adversarial examples (Madry et al., 2018), the correlation between the visual representation and word vector has a significant enhancement.As a result, we can summarize a novel intriguing property: the more robust model, the stronger the correlation.Secondly, to verify the semantic word vectors could present the analogies and correlations between words, we visualize the similarity matrix of word vectors generated by a trained Glove (Pennington et al., 2014) on CIFAR-10, which is shown in Figure 2 (c), As can be seen from the figure, the correlation between category 3 (Cat)foot_0 and category 5 (Dog) is stronger than the correlation between category 3 (Cat) and category 9 (Truck).We further visualize the similarity between different categories of non-robust features , and the similarity of robust features.which are shown in Figure 2 (a) and (b) respectively.We can observe that the robustness feature can also reflect the relatedness between categories, and it is similar to the relatedness reflected by the semantic word vector.However, the non-robust features cannot reflect the association between categories.Recently, CLIP (Radford et al., 2021) uses large-scale image-text pairs to jointly learn semantic representations.Therefore, we also visualize the semantic representation correlation matrix learned by CLIP.which is shown in Figure 2 (d). the semantic representations learned by CLIP present analogies and correlations between categories, but there is a certain gap with the real semantics.Taking our analysis into consideration, we introduce the semantic information learned by word embedding into model training, which aims at improving the robustness of the current neural networks (He et al., 2016a).We follow an information-theoretical perspective to bridge the information gap between visual representations and semantic word vectors, which consists of two key techniques.First, we use mutual information to enhance semantic information in the visual representation, which aims to enhance the correlations via distributional information.Second, we introduce geometric constraints to align the manifold information from the visual representation space to the word vector space, which aims to enhance the correlations via structural information.Finally, we propose the Semantic Constraint Adversarial Robust Learning (SCARL) framework, which combines the above two techniques with adversarial training.Our contributions are summarized as follows:\u2022 We are the first to explore the correlation between semantic word information and the deep model via the classical CCA method.We find that, the more robust the model, the stronger the correlation between visual representation and semantic word vector.\u2022 We analyze the correlations between different categories of image features, and find that the robust features can reflect the semantic association between categories, which is consistent with the word vector, but the not-robust features can not.\u2022 We introduce a Semantic Constraint Adversarial Robust Learning (SCARL) framework that captures the distributional and structural information from semantic word vectors via mutual information optimization and geometry constraints, to promote robustness.\u2022 We conduct extensive experiments on three widely-used benchmarks.The results show that the proposed SCARL behaves more robust than several state-of-the-art techniques, which demonstrates semantic information indeed helps improve robustness."}
{"paper_id": 475, "introduction": "Large, pretrained, foundation models (e.g., CLIP (Radford et al., 2021), DALL-E 2 (Ramesh et al., 2022) and GPT-3 (Brown et al., 2020)) are capable of many complex tasks such as zero-shot prediction -the ability of models to predict the classes to which the input samples belong during testing without previous exposure to samples from that classes during training, generating images according to text prompts, generating images inspired by their originals, translating, reading comprehension, etc.However, the scales, or the numbers of parameters that these models contain are so large that it would be difficult to deploy such models to devices with limited computing power such as mobile phones, tablets, and laptops.In addition, even though these foundation models are versatile, demonstrating great competence in abundant tasks that are considered to be challenging for regular neural networks, in some situations, however, instead of using all the functions that these models are capable of, we may only need to use parts of or even a derivative of their functions.These facts indicate that deploying a full foundation model in all use cases could be a waste of computational resource and memory, and such intentions could be even impractical in some situations.Therefore, the study of techniques that could be applied to compress or enable the utilization of a portion of the functions of such foundation models would be valuable and necessary.To use a portion or derivatives of the functions of these huge, pretrained foundation models, one promising mechanism is to transfer the knowledge from the foundation models to lightweight, taskspecific models.In (Hinton et al., 2014), a knowledge distillation (KD) algorithm is proposed, which is able to improve the task-specific performance of a model with a smaller scale (the student network) by transferring knowledge from another model with a larger scale and better performance specific to the task to it.The KD algorithm proposed by Hinton et al. (2014) (HKD) aims at minimizing both the Kullback-Leibler divergence (KL divergence) loss between the outputs of the teacher network and the student network along with the cross entropy loss between the student network and class labels.However, given the differences in network architectures along with pretraining methods between foundation models and conventional models, applying HKD directly on foundation models may not be an effective approach to exploit the knowledge within the foundation models to benefit the performance of lightweight models designed for definite tasks.Firstly, the teacher network, in this case, a foundation model, is not pretrained to optimize its performance on the particular task that the student network is designed for.Moreover, the intrinsic properties of the dataset utilized for pretraining foundation models could be different from that of the dataset we adopt for a specific task.In addition, in (Cho & Hariharan, 2019;Mirzadeh et al., 2020), the existence of \"capacity gap\" between a teacher and a student is believed to be the major factor that prevents the performance of a student network from further improving when the teacher network contains more parameters and have better task-specific performance.When a foundation model, which contains a considerably larger quantity of parameters compared to conventional models, is adopted as a teacher network for knowledge distillation, this problem could become even more severe.In this paper, we focus on the image classification task, exploring and investigating knowledge distillation-related properties of a pretrained foundation model CLIP (Radford et al., 2021) under various experimental settings.Our contributions are:\u2022 We notice that naively distilling knowledge from CLIP (Radford et al., 2021) to student networks does not lead to satisfactory results, meaning such student networks do not outperform those distilled from more commonly adopted teacher networks (e.g., ResNet 34, 50 (He et al. (2016))).We hence propose a process to improve the accuracy of the teacher network on image classification before knowledge distillation, which is the fine tuning of CLIP.This accuracy is the upper bound of that of the student network.\u2022 We find that distilling from CLIP is not vulnerable to the \"capacity gap\" issue even when the difference in the number of parameters between the teacher network and the student network reaches more than a thousand times.Moreover, when there are only limited training samples available, the superiority of CLIP in knowledge distillation increases.Our experimental results suggest the reason may well be related to the training recipe of CLIP instead of the network architecture.Our further quantitative analysis of the output of teacher networks reveals that it is more probable for image classifying models trained with crossentropy criterion to give a high score to a wrong label on misclassification, which can later mislead the student network in knowledge distillation.On the contrary, giving a relatively high score to wrong labels is less likely for models trained under CLIP paradigm.This can have a profound impact on the understanding of the capacity gap issue\u2022 Based on these findings, we assign our finetuned CLIP to supervise the training of the lightweight model MobileNetV3 (Howard et al., 2019), a network designed for CPU deployments.The achieved performance turned out to be notably higher than that of those trained from scratch or under the supervision of regular networks."}
{"paper_id": 476, "introduction": "Deep learning technology is being employed with increasing frequency in recent years LeCun et al. (2015) Schmidhuber (2014).Various deep learning models have achieved remarkable results in computer vision Krizhevsky et al. (2017), remote sensing Zhu et al. (2017), target classification in SAR images Chen et al. (2016), and speech recognition Graves et al. (2013) Hinton et al. (2012).In the domain of natural language processing (NLP), deep learning methods are used to learn word vector representations through neural language models Mikolov et al. (2013) and performing composition over the learned word-vectors for classification Collobert et al. (2011).Convolutional neural networks (CNNs), for example, utilize layers with convolving filters that are applied to local features.CNN is widely used for image tasks and is currently state-of-the-art for object recognition and detection.Originally invented for computer vision, CNN models have subsequently been shown to be effective for NLP and have achieved excellent results in semantic parsing Yih et al. (2015), search query retrieval Shen et al. (2014), sentence modelling Kalchbrenner et al. (2014), and other traditional NLP tasks Collobert et al. (2011).However, the success of deep learning comes at a cost.The first and foremost is its reliance on large amounts of labeled data, which are often difficult to collect and entail a slow learning process.Second, deep models are brittle in the sense that a trained network that performs well on one task often performs very poorly on a new task, even if the new task is very similar to the one it was originally trained on.Third, they are strictly reactive, meaning that they do not use high-level processes such as planning, causal reasoning, or analogical reasoning.Fourth, human expertise cannot be used which can often reduce the burden of acquiring training data which is often expensive to collect.Purely data-driven learning can lead to uninterpretable and sometimes counter-intuitive results Nguyen et al. (2014) Szegedy et al. (2013).The sub-symbolic neural approaches allow us to mimic human cognitive thought processes by extracting features at various levels of abstraction from direct observation and thereby facilitate learning.But humans also learn from general high-level knowledge expressed declaratively in logical syntax.A representation language allows recursive structures to be easily represented and manipulated, which is usually difficult in a neural learning environment.But a symbolic reasoning system is not good to adapt to new environments by learning and reasoning based on traditional theorem-proving, which can be computationally expensive.Moreover, a purely symbolic system based on traditional AI requires enormous human effort as knowledge are manually programmed and not learned.Central to classical AI is the use of language-like propositional representations to encode knowledge.The symbolic elements of a representation in classical AI -the constants, functions, and predicates -are typically hand-crafted.Inductive Logic Programming Muggleton (1990) methods learn hypothesis rules given background knowledge, and a set of positive and negative examples.The systems we have discussed until now do not model uncertainty which is essential in practical applications.Various probabilistic logics Halpern (2005) and Markov Logic Networks Richardson & Domingos (2006) (MLNs) handle uncertainty using weight attached to every rule.Practical applications of these networks have been limited as inference is not scalable to a large number of rules.It is, therefore, desirable to develop a hybrid approach, embedding declarative representation of knowledge, such as domain and commonsense knowledge, within a neural system.In this paper, hybrid approach is applied to indoor scene classification, which has been extensively studied in field of computer vision Chen et al. (2018).However, compared with outdoor scene classification, this is an arduous issue due to the large variety of density of objects within a typical scene.In addition, high-accuracy models already exist for outdoor scene classification while indoor scene classification is not.In order to accomplish our objective, the acquisition, representation, and utilization of visual commonsense knowledge represents a set of critical opportunities in advancing computer vision past the stage where it simply classifies or identifies which objects occur in imagery Davis et al. (2015).The contributions of this paper is summarized as followed:\u2022 A joint representation multimodal fusion framework is applied to exploit the early fusion of vectorized logical knowledge and images for the task of indoor scene classification.Experiments show that higher classification accuracy is obtained compared to traditional image classification methods.\u2022 A 'if-then' logical knowledge system is built based on reviews of each indoor scene class which are scraped from Google open source, through Word2Vec and BERT embedding.This helps to get a better contextual representation of words detected by object detection.\u2022 A unique rules embedding approach is proposed, which allows to converge 'if-then' logic of probability with image representation.The embedding approach has different representations during training and inference process.The rest of the paper is organized as follows.The next section 2 surveys the related work.The hybrid framework is explained in section 3. Section 4 details implementation and evaluation of experiments.Finally, we conclude with some future directions in 5."}
{"paper_id": 477, "introduction": "Video recognition methods has evolved rapidly due to the increasing number of online videos and success of advanced deep neural networks.Even if 3D networks (Feichtenhofer et al., 2019;Carreira & Zisserman, 2017;Tran et al., 2015) provide straightforward solutions for video recognition, 2D based methods (Wang et al., 2016;Lin et al., 2019;Li et al., 2020;Wang et al., 2021) still arouse researchers' interests because of their efficiency in both computing and storage.However, 2D networks still suffer from overfitting issue.For instance, on Something-Something V1 (Goyal et al., 2017) dataset which contains strong temporal dependency, the training and validation accuracy of TSM (Lin et al., 2019) is 81.22% and 45.34%, respectively.Besides, its Expected Calibration Error (ECE) (Guo et al., 2017) is 25.83% which means the model gives overconfident predictions and brings negative impact when deploying the model in real scenarios.There are two main reasons for overfitting in video: 1) video recognition benchmarks usually have fewer training samples compared with image classification datasets (e.g., ImageNet (Deng et al., 2009) with 1.2 million training samples compared to Kinetics (Kay et al., 2017) with 240K videos).Furthermore, spatial-temporal modeling for video is harder than recognizing static images, which should require even more samples.2) 2D based methods average the logits of all frames to vote for final decision which increases the tendency to overfit frames which contain less semantic information (e.g., background scene).In this view, these frames can be regarded as noise in optimization because they do not provide motion information for temporal modeling.To alleviate overfitting, many attempts have been proposed.Dropout (Srivastava et al., 2014) and Label Smoothing (Szegedy et al., 2016) are widely used in deep networks because of the regularization effects they bring.Designing data augmentation methods (Zhang et al., 2017;Yun et al., 2019;DeVries & Taylor, 2017;Cubuk et al., 2020) to relieve overfitting is another line of research.Although some methods have shown effectiveness in image-level tasks, directly employing them on video tasks may result in detrimental temporal representations as these methods are not specifically designed for video data and some transformations will break the motion pattern.In our work, we propose a data augmentation approach Ghost Motion (GM) which shifts channels along the temporal dimension to propagate motion information to adjacent frames.Specifically, all channels will be shifted for one step along the temporal axis leading to misaligned channel orders, and we interpolate between the original video and the misaligned one to form the new video, named ghost video.It diffuses motion patterns from salient frames into other frames to enhance the overall representative capacity.In this way, we can enforce the model to focus more on the informative frames and prevent it from overfitting non-informative frames to relieve overfitting.Although the shifting operation results in mismatched RGB orders, we surprisingly find the disorder is beneficial to improve generalization as well where it is elaborated in the analysis of Sec.4.5.Ghost Motion effectively enlarges the input space without crashing the motion pattern and offers continuous variance in the input space which is beneficial to improve the generalization abilities of video recognition methods.Moreover, we find utilizing a hyperparameter Temperature to scale the logits before Softmax can further mitigate overfitting and reduce the calibration error, especially on challenging datasets such as Something-Something V1&V2.The proposed GM can be easily plugged into existing methods to improve their generalization abilities with a few line codes and brings negligible computational costs.Shown in Fig. 1 (a), GM results in consistent improvements over various 2D models.In Fig 1 (b), GM continuously improves the performance of baseline model with different number of sampled frames.In addition, our method is compatible with existing image-level augmentations.We can jointly apply them to relieve overfitting and further boost the performance.The main contributions are summarized as follow:\u2022 We propose video recognition data augmentation method Ghost Motion (GM) which can effectively improve the generalization of current video benchmark models and is compatible with existing image-level data augmentation approaches.\u2022 We find smoothing the logits can prevent overconfident predictions to further alleviate overfitting for temporal-dependent datasets such as Something-Something. \u2022 We conduct comprehensive experiments to validate the strength of Ghost Motion on various datasets and methods.Extensive ablation with detailed analysis illustrate the motivation and intuition about our GM strategy."}
{"paper_id": 478, "introduction": "Graph Neural Networks (GNN) Hamilton et al. (2017); Kipf & Welling (2017) have become de facto models for representation learning on graph structured data.Hence they have started being deployed in production systems Ying et al. (2018); Niu et al. (2020).These models iteratively update the node embeddings by passing messages along the direction of the edges in the given graph with nonlinearities in between different layers.With l layers, the computed node embeddings contain information from the l-hop neighborhood of the seed vertex.In the production setting, the GNN models need to be trained on billion-scale graphs (Ching et al., 2015;Ying et al., 2018).The training of these models takes hours to days even on distributed systems Zheng et al. (2022b;a).As in general Deep Neural Networks (DNN), it is more efficient to use minibatch training (Bertsekas, 1994) on GNNs, even though it is a bit trickier in this case.The node embeddings in GNNs depend recursively on their set of neighbors' embeddings, so when there are l layers, this dependency spans the l-hop neighborhood of the node.Real world graphs usually have a very small diameter and if l is large, the l-hop neighborhood may very well span the entire graph, also known as the Neighborhood Explosion Phenomenon (NEP) (Zeng et al., 2020).To solve these issues, researchers proposed sampling a subgraph of the l-hop neighborhood of the nodes in the batch.There are mainly three different approaches: Node-based, Layer-based and Subgraph-based methods.Node-based sampling methods (Hamilton et al., 2017;Chen et al., 2018a;Liu et al., 2020;Zhang et al., 2021) sample independently and recursively for each node.It was noticed that node-based methods sample subgraphs that are too shallow, i.e., with a low ratio of number of edges to nodes.Thus layer-based sampling methods were proposed (Chen et al., 2018b;Zou et al., 2019;Huang et al., 2018;Dong et al., 2021), where the sampling for the whole layer is done collectively.On the other hand subgraph sampling methods (Chiang et al., 2019;Zeng et al., 2020;Hu et al., 2020b;Zeng et al., 2021) do not use the recursive layer by layer sampling scheme used in the node-and layer-based sampling methods and instead tend to use the same subgraph for all of the layers.Some of these sampling methods take the magnitudes of embeddings into account (Liu et al., 2020;Zhang et al., 2021;Huang et al., 2018), while others, such as Chen et al. (2018a); Cong et al. (2021), cache the historical embeddings to reduce the variance of the computed approximate embeddings.There are methods sampling from a vertex cache Dong et al. (2021) filled with popular vertices.Most of these approaches are orthogonal to each other and they can be incorporated into other sampling algorithms.Node-based sampling methods suffer the most from the NEP but they guarantee a good approximation for each embedding by ensuring each vertex gets k neighbors which is the only hyperparameter of the sampling algorithm.Layer-based sampling methods do not suffer as much from the NEP because number of vertices sampled is a hyperparameter but they can not guarantee that each vertex approximation is good enough and also their hyperparameters are hard to reason with, number of nodes to sample at each layer depends highly on the graph structure (as the numbers in Table 2 show).Subgraph sampling methods usually have more bias than their node-and layer-based counterparts.Hence, in this paper, we focus on the node-and layer-based sampling methods and combine their advantages.The major contributions of this work can be listed as follows:\u2022 We propose a new sampling algorithm called LABOR, combining advantages of neighbor and layer sampling approaches using Poisson Sampling.LABOR correlates the sampling procedures of the given set of seed nodes so that the sampled vertices from different seeds have a lot of overlap, resulting into a 7\u00d7 reduction in computation, memory and communication.Furthermore, LABOR has the same hyperparameters as neighbor sampling to use as a drop-in replacement and can speed up training by upto 2.6\u00d7.\u2022 We experimentally verify our findings, show that our proposed sampling algorithm LABOR outperforms both neighbor sampling and layer sampling approaches.LABOR can enjoy a batch-size of upto 112\u00d7 larger than NS while sampling the same number of vertices."}
{"paper_id": 479, "introduction": "Padding, one of the most fundamental components in neural network architectures, has received much less attention than other modules in the literature.In convolutional neural networks (CNNs), zero padding is frequently used perhaps due to its simplicity and low computational costs.This design preference remains almost unchanged in the past decade.Recent studies (Islam* et al., 2020;Islam et al., 2021b;Kayhan & Gemert, 2020;Innamorati et al., 2020) show that padding can implicitly provide a network model with positional information.Such positional information can cause unwanted side-effects by interfering and affecting other sources of position-sensitive cues (e.g., explicit coordinate inputs (Lin et al., 2022;Alsallakh et al., 2021a;Xu et al., 2021;Ntavelis et al., 2022;Choi et al., 2021), embeddings (Ge et al., 2022), or boundary conditions of the model (Innamorati et al., 2020;Alguacil et al., 2021;Islam et al., 2021a)).Furthermore, padding may lead to several unintended behaviors (Lin et al., 2022;Xu et al., 2021;Ntavelis et al., 2022;Choi et al., 2021), degrade model performance (Ge et al., 2022;Alguacil et al., 2021;Islam et al., 2021a), or sometimes create blind spots (Alsallakh et al., 2021a).Meanwhile, simply ignoring the padding pixels (known as no-padding or valid-padding) leads to the foveal effect (Alsallakh et al., 2021b;Luo et al., 2016) that causes a model to become less attentive to the features on the image border.These observations motivate us to thoroughly analyze the phenomenon of positional encoding including the effect of commonly used padding schemes.Conducting such a study requires reliable metrics to detect the presence of positional information introduced by padding, and more importantly, quantify its strength consistently.We observe that the existing methods for detecting and quantifying the strength of positional information yield inconsistent results.In Section 3, we revisit two closely related evaluation methods, PosENet (Islam* et al., 2020) andF-Conv (Kayhan &Gemert, 2020).Our extensive experiments demonstrate that (a) metrics based on PosENet are unreliable with an unacceptably high variance, and (b) the Border Handling Variants (BHV) test in F-Conv suffers from unaware confounding variables in its design, leading to unreliable test results.In addition, we observe all commonly-used padding schemes actually encode consistent patterns underneath the highly dynamic model features.However, such a pattern is rather obscure, noisy, and visually imperceptible for most paddings (except zeros-padding), which makes recognizing and analyzing it difficult.Fortunately, we show that such patterns can be consistently revealed with a sufficient number of samples by defining an optimal padding scheme (see Section 2.1 and Figure 1).The source codes and data collection scripts will be made publicly available.We propose a method that can consistently and effectively extract PPPs through the distributional difference between optimallypadded (gray-scale surfaces) and algorithmically-padded features (colored surfaces).The results show that the two distributions become distinguishable as the number of sample increases.Following the procedure in Section 2.2, we extract a clear view of PPP with the expectation of the pairwise differences between optimally-padded and algorithmically-padded features.We render each visualization in tilted view (first row) and top view (second row).The colors represent the magnitude (blue/cold/weak to green/warm/strong) at each pixel.The features are extracted at the 3rd layer of interest (Appendix A) from a randn-padded (Section 2.4) ResNet50 pretrained on ImageNet.We accordingly propose a new evaluation paradigm and develop a method to consistently detect the presence of the Position-information Pattern from Padding (PPP), which is a persistent pattern embedded in the model features to retain positional information.We present two metrics to measure the response of PPP from the signal-to-noise perspective and demonstrate its robustness and low deviation among different settings, each with multiple trials of training.To weaken the effect of PPP, in Section 2.4, we design a padding scheme with built-in stochasticity, making it difficult for the model to consistently construct such biases.However, our experiments show that the models can still circumvent the stochasticity and end up consistently constructing PPPs.These results suggest that a model likely constructs PPPs purposely to facilitate its training, rather than falsely or accidentally learning some filters that respond to padding features.With reliable PPP metrics, we conduct a series of experiments to analyze the characteristics of PPP in Section 4.1.Specifically, we analyze the formation of PPP throughout each model training process in Section 4.3.The results show PPPs are formed expeditiously at the early stage of model training, slowly but steadily strengthen through time, and eventually shaped in clear and complete patterns.These results show that a model intentionally develops and reinforces PPPs to facilitate its learning process.Moreover, we observe the PPPs of all pretrained networks are significantly stronger than those in their initial states.This indicates an unbiased training procedure is of great importance in resolving the critical failures caused by PPP in numerous vision tasks (Alsallakh et al., 2021a;Xu et al., 2021;Ge et al., 2022;Alguacil et al., 2021)."}
{"paper_id": 480, "introduction": "Training long-horizon robotic policies in complex physical environments is important for robot learning.However, directly learning a policy that can generalize to unseen tasks is challenging for Reinforcement Learning (RL) based approaches (Yu et al., 2020;Savva et al., 2019;Shen et al., 2021;Mu et al., 2021).The state/action spaces are usually high-dimensional, requiring many samples to learn policies for various tasks.One promising idea is to decouple plan generation and plan execution.In classical robotics, a high-level planner generates a abstract trajectory using symbolic planning with simpler state/action space than the original problem while a low-level agent executes the plan in an entirely physical environment Kaelbling & Lozano-P\u00e9rez (2013); Garrett et al. (2020b).In our work, we promote the philosophy of abstract-to-executable via the learning-based approach.By providing robots with an abstract trajectory, robots can aim for one-shot task generalization.Instead of memorizing all the high-dimensional policies for different tasks, the robot can leverage the power of planning in the low-dimensional abstract space and focus on learning low-level executors.The two-level framework works well for classical robotics tasks like motion control for robot arms, where a motion planner generate a kinematics motion plan at a high level and a PID controller execute the plan step by step.However, such a decomposition and abstraction is not always trivial for more complex tasks.In general domains, it either requires expert knowledge (e.g., PDDL (Garrett et al., 2020b;a)) to design this abstraction manually or enormous samples to distill suitable abstractions automatically (e.g., HRL (Bacon et al., 2017;Vezhnevets et al., 2017)).We refer Abel (2022) for an in-depth investigation into this topic.On the other side, designing imperfect high-level agents whose state space does not precisely align with the low-level executor could be much easier and more flexible.High-level agents can be planners with abstract models and simplified dynamics in the simulator (by discarding some physical features, e.g., enabling a \"magic\" gripper Savva et al. (2019); Torabi et al. (2018)) or utilizing an existing \"expert\" agent such as humans or pre-trained agents on different manipulators.Though imperfect, their trajectories still contain meaningful information to guide the low-level execution of novel tasks.For example, different robots may share a similar procedure of reaching, grasping, and moving when manipulating a rigid box with different grasping poses.As a trade-off, executing their trajectories by the low-level executors becomes non-trivial.As will be shown by an example soon, there may not be a frame-to-frame correspondence between the abstract and the executable trajectories due to the mismatch.Sometimes the low-level agent needs to discover novel solutions by slightly deviating from the plan in order to follow the rest of the plan.Furthermore, the dynamics mismatch may require low-level agents to pay attention to the entire abstract trajectory and not just a part of it.To benefit from abstract trajectories without perfect alignment between high and low-level states, we propose TRajectory TRanslation (abbreviated as TR 2 ), a learning-based framework that can translate abstract trajectories into executable trajectories on unseen tasks at test time.The key feature of TR 2 is that we do not require frame-to-frame alignment between the abstract and the executable trajectories.Instead, we utilize a powerful sequence-to-sequence translation model inspired by machine translation (Sutskever et al., 2014;Bahdanau et al., 2014) to translate the abstract trajectories to executable actions even when there is a significant domain gap.This process is naturally reminiscent of language translation, which is well solved by seq-to-seq models.We illustrate the idea in a simple Box Pusher task as shown in Fig. 1.The black agent needs to push the green target box to the blue goal position.We design the high-level agent as a point mass which can magically attract the green box to move along with it.For the high-level agent, it is easy to generate an abstract trajectory by either motion planning or heuristic methods.As TR 2 does not have strict constraints over the high-level agent, we can train TR 2 to translate the abstract trajectory, which includes the waypoints to the target, into a physically feasible trajectory.Our TR 2 framework learns to translate the magical abstract trajectory to a strategy to move around the box and push the box to the correct direction, which closes the domain gap between the high-and low-level agents.Our contributions are: (1) We provide a practical solution to learn policies for long-horizon complex robotic tasks in three steps: build a paired abstract environment (e.g., by using a point mass with magical grasping as the high-level agent), generate abstract trajectories, and solve the original task with abstract-to-executable trajectory translation.(2) The seq-to-seq models, specifically the transformer-based auto-regressive model (Vaswani et al., 2017;Chen et al., 2021;Parisotto et al., 2020), free us from the restriction of strict alignment between abstract and executable trajectories, providing additional flexibility in high-level agent design, abstract trajectory generation and helps bridge the domain gap.(3) The combination of the abstract trajectory and transformer enables TR 2 to solve unseen long-horizon tasks.By evaluating our method on a navigation-based task and three manipulation tasks, we find that our agent achieves strong one-shot generalization to new tasks, while being robust to intentional interventions or mistakes via re-planning.Our method is evaluated on various tasks and environments with different embodiments.In all experiments, the method shows great improvements over baselines.We also perform real-world experiments on the Block Stacking task to verify the capability to handle noise on a real robot system.Please refer to the anonymous project page for more visualizations."}
{"paper_id": 481, "introduction": "Although machine learning (ML) algorithms are not expected to be perfect, their unexplained failures can be detrimental e.g. the well-known incident of 'racist ' algorithm bbc (2015).EXplainable Artificial Intelligence (XAI) has emerged as an effort to help improve trust in the use of ML algorithms.It is a burgeoning field that has been recently studied from different aspects, such as (1) data influence on model training (2) post-hoc attribution methods (3) \"signal methods\" etc (some methods fall into multiple categories as seen in surveys like Arrieta et al. (2020); Gilpin et al. (2018); Tjoa & Guan (2020); Adadi & Berrada (2018)).With improved trust, powerful blackbox models like the deep neural network (DNN) can be adopted into real applications with more accountability.Combining some of these existing concepts, we introduce k-width and Bifold Embedded Data Ordered Neural Network (kaBEDONN), which is a post-hoc XAI method to query relevant data as the explanation for a model prediction.All python codes are available in the supp.materials.Here, we consider the image classification task, including experiments on common image datasets MNIST, CIFAR10, ImageNet.Denote a sample data as (x, y0) \u2208 D = X \u00d7 Y where X is the input space and y0 \u2208 Y the ground-truth class label.x is classified using some base model f as c = argmax i (y i ) where y = f (x) \u2208 R C and C the number of classes/categories.The scenario considered in this paper is the following: users wish to know why f labels the sample as c i.e. they require explanations for the predictions.Like many XAI methods, kaBEDONN aims to provide a form of explanation.We start by clarifying our three objectives.Objective 1. Relevant data as explanations.The relevance of data has been measured in different ways.In Koh & Liang (2017), a training data sample is considered either helpful or harmful to the prediction made by a trained model, quantified by the influence score.In Yeh et al. (2018), data samples are either excitatory or inhibitory, in Pruthi et al. (2020) proponent or opponent.For kaBEDONN, relevant data samples strongly activate main nodes or sub-nodes, hence, they are excitatory in a different sense than Yeh et al. (2018).Here, explanatory images are considered relevant when their features look \"similar\" to x according to the base model f .The explanatory images are then presented to users as shown in fig.1(A) and fig.2(A).More technically, we have three different contexts of \"similar\".(1) A representative data r is a training data sample that has been used to construct a main node in kaBEDONN.In this case, kaBEDONN stores the processed signals of r (also called \"fingerprint\" in (Tjoa & Cuntai, 2021)) and r's index w.r.t the ordered dataset in a main node.(2) A similar data s is a training data sample that has neither been included as a main node nor a sub-node because it is already well-represented by an existing main node.Only the index of s (but not its fingerprint) is stored in a well-represented (WR) node (that belongs to some main node).(3) A boundary data is a training data sample that is similar enough to a representative data r but is different (they have different class labels); this happens, for example, when similar-looking breeds of cats are labeled differently.Its fingerprint and index are stored in the sub-node of the main node constructed from r.Objective 2. Adjustment of Explanation for debugging.Explanations do not necessarily convince every user to the same degree.Suppose users flag some explanations as unsuitable.kaBEDONN is a system that allows developers to quickly finetune explanations based on the given feedback, primarily by adding, removing or reordering the underlying data samples used for kaBEDONN construction.This is useful because problematic data samples (e.g.accidentally mislabeled data and overly-representative data*) can be identified and removed while better explanations can be incorporated to the system when available.Remark.*An example of overly-representative data is an optical illusion; since it appears different from different point of views, it is not helpful as an explanation.A real user feedback example from ImageNet.In fig.2(B) panel 1, the image of interest (bullfrog image) activates a main node with the wrong class label (hammerhead shark).Furthermore, we found that a cartoon image of a hammerhead shark in the ImageNet dataset is associated with that node.Suppose a user considers it undesirable, we need the developer to quickly readjust the explanation.This is done by simply reordering the cartoon image (panel 2): we push it to the back of the queue by renaming the image.kaBEDONN is then reconstructed (panel 3), and a more \"similar\" representative is presented (the node appears to respond to partially dark background).We have deliberately chosen the unclear and ambiguous bullfrog image from ImageNet to demonstrate how problematic case can be handled.The result is thus not perfect; in practice, iterative user/developer feedback may be needed for better result.Also see appendix General info for more remarks.Objective 3. Predictive correctness flag for debugging.kaBEDONN is a posthoc explanatory model complementary to a more complex blackbox base model f .It is constructed using the collection {(x e , y0) : x e = f enc (x), (x, y0) \u2208 D \u2032 \u2286 D} where x e is a latent vector obtained from encoder f enc .The encoder can be f enc = f itself or only the latent encoder part of f .In this paper, x e = y = CN N (x) i.e. f enc = f for the simplicity of demonstration.kaBEDONN can also act as a predictive model through multi-layered processing of latent vectors, partially using the universal approximation (UA) concept in Tjoa & Cuntai (2021).While kaBEDONN no longer has the UA property from Tjoa & Cuntai (2021), the data fitting capability is still very high (see experiment and result section).The discrepancy between predictions made by kaBEDONN and the base model f (e.g ResNet) serves as a flag for user/developer to report abnormal prediction.A) kaBEDONN provides a user with a representative image as the standard explanation and \"similar\" images as supporting explanations.(B) kaBEDONN applied on an ambiguous image of a bullfrog.User finds an explanation (through sub-node activation) with undesirable main representative: cartoon image of hammerhead shark in ImageNet dataset.Developer fixes the issue by reordering the undesirable data.New representative is queried.It is still not quite the right explanation although it has similar colour texture (dark object in dark background).Furthermore, with kaBEDONN's sub-node activation, self-representative sub-node is activated, thus yielding the correct label.To summarize, kaBEDONN is not only a posthoc XAI method that presents users with \"similar\" images, but is also an adjustable system that is friendly to model developers for two reasons.(1) Adjustment of D \u2032 .Problematic samples can be easily readjusted, reducing the likelihood that they turn into representative kaBEDONN main nodes (i.e. less likely served as explanations), thus improving the quality of main explanations.Explicit indexing (y0, idx) = (class, index) of the data makes adjustment/feedback process easy.(2) kaBEDONN is posthoc and model agnostic, hence it does not directly interfere with the main model f .By comparison, Neural Backed Decision Tree Wan et al. (2021) requires finetuning of the main model's weights.Regardless, kaBEDONN helps developers collect user feedback on some sample data; data flagged as problematic may later be excluded from further finetuning of f ."}
{"paper_id": 482, "introduction": "Value-based reinforcement learning (RL) is a popular class of algorithms for solving sequential decision-making problems with unknown dynamics (Sutton & Barto, 2018).For a given problem, value-based algorithms aim at obtaining the most accurate estimate of the expected return from each state, i.e., a value function.For instance, the well-known value-iteration algorithm computes value functions by iterated applications of the Bellman operator (Bellman, 1966), of which the true value function is the fixed point.Although the Bellman operator can be applied exactly in dynamic programming, it needs to be estimated from samples at each application when dealing with unknown models of RL problems, i.e., empirical Bellman operator (Watkins, 1989;Bertsekas, 2019).Intuitively, the dependence of value iteration on the samples has an impact on the efficiency of the algorithms and on the quality of the obtained estimated value function, which becomes accentuated when solving continuous problems that require value-based methods with function approximation, e.g., approximate value iteration (AVI) (Munos, 2005;Munos & Szepesv\u00e1ri, 2008).Moreover, in AVI approaches, costly function approximation steps are needed to project the output of the Bellman operator back to the considered action-value functional space.In this paper, we tackle these limitations by introducing the novel approach of using samples to obtain a new operator, which we call projected Bellman operator (PBO).Our PBO is a function \u039b : \u2126 \u2192 \u2126 defined on parameters \u03c9 \u2208 \u2126 of the value function.Contrarily to the standard (empirical) Bellman operator, which uses action-value functions Q \u03c9 k to compute targets that are then projected to obtain Q \u03c9 k+1 , our PBO uses the parameters of the action-value function to compute updated parameters \u03c9 k+1 = \u039b(\u03c9 k ) directly (Figure 1).The crucial advantages of our approach are twofold: (i) the output of PBO always belongs to the considered action-value functional space, avoiding, therefore, the costly projection step typical when using the Bellman operator, and (ii) once learned, PBO is applicable for an arbitrary number of iterations without using further samples, as visualized in Figure 2. Starting from initial parameters \u03c9 0 , AVI approaches obtain consecutive approximations of the value function Q \u03c9 k by Q * and Q \u03c9 * are respectively the optimal value function and its projection on the parametric space.applying the Bellman operator iteratively over samples (Figure 2b).Instead, our PBO makes use of the samples to learn the operator only.Then, starting from initial parameters \u03c9 0 , PBO can produce a chain of updated parameters of arbitrary length (blue lines in Figure 2a) without requiring further samples.In the following, after formally introducing PBO and a novel algorithm for value estimation based on it, we analyze its advantageous properties for different classes of problems.Thus, our contribution is threefold: (i) we introduce the notion of projected Bellman operator (PBO); (ii) we show how to derive different PBOs according to the class of problems at hand; (iii) we develop a novel algorithm for value estimation based on PBO and show its advantages over related baselines on several RL problems."}
{"paper_id": 483, "introduction": "Text-guided image generation models, such as DALL-E 2 (Ramesh et al., 2022), have recently received a lot of attention from both the scientific community and the general public.Provided with a simple textual description, these models are able to generate high-quality images from different domains and styles.Whereas trained on large collections of public data from the internet, the learned knowledge and behavior of these models are only little understood and have already raised copyright concerns (Heikkil\u00e4archive, 2022).Previous research mainly focused on improving the generated images' quality and the models' understanding of complex text descriptions.See Sec. 2 for an overview of related work in text-to-image synthesis and possible attacks against such models.We are the first to investigate the behavior of text-guided image generation models when conditioned on descriptions that contain non-Latin Unicode characters.Replacing standard Latin characters with visually similar characters, so-called homoglyphs, allows a malicious party to disrupt image generation while making the manipulations for users hard to detect through visual inspection.We show the surprising effect that homoglyphs from non-Latin Unicode scripts not only influence the image generation but also implicitly induce biases from the cultural circle of the corresponding languages.For example, DALL-E 2 generates images of Athens when provided with a generic description of a city and a single character replaced with a Greek homoglyph.We found similar model behavior across various domains and Unicode scripts, for which replacing as few as a single Latin character with any non-Latin character is sufficient to induce biases into the generated images.We present our methodology and experimental results in Sec. 3 and Sec. 4, respectively.We generally refer to the induced cultural and ethnic characteristics corresponding to specific language scripts into the generated images as cultural biases throughout this work.Moreover, homoglyph replacements allow an attacker to even hide complete objects from being depicted in the generated images.It results in misleading image generations and lowers the perceived model quality, as we practically demonstrate in Sec. 4. This behavior is not limited to DALL-E 2 but also apparent for Stable Diffusion (Rombach et al., 2022) and CLIP (Radford et al., 2021).Our experimental results, which we discuss further in Sec. 5, raise the questions of how much we actually understand about the inner processes of multi-modal models trained on public data and how small differences in the textual description could already influence the image generation.As textguided image generation models become available to the general public and have a wide range of applications, such questions are essential for an informed use.In summary, we make the following contributions:\u2022 We are the first to show that text-guided image generation models and other models trained on text-image pairs are sensitive to character encodings and implicitly learn cultural biases related to different scripts during training.\u2022 We demonstrate that by a single homoglyph replacement, an attacker can bias the image generation with cultural influences and even render the whole process meaningless.\u2022 We introduce a novel homoglyph unlearning procedure to make already trained text encoders invariant to homoglyph manipulations.Disclaimer: This paper contains images representing various cultural biases that some readers may find offensive.We emphasize that the purpose of this work is to show that such biases are already present in text-guided image generation models and can be exploited through homoglyph manipulations.We do not intend to discriminate against people or cultures in any way."}
{"paper_id": 484, "introduction": "Human reasoning and decision-making is often underpinned by cause and effect: we take actions to achieve a desired effect, or reason that events would have happened differently had we acted a certain way -or if conditions had been different.Similarly, scientific inquiry uses the same tools, albeit more formalized, to build knowledge about the world and how our society can affect it (Popper, 1962).When building algorithms that automatically build statistical models of the world, as is common in machine learning practice, it would then be desirable to imbue them with similar inductive priors about cause and effect (Glymour et al., 2016).In addition to being more robust than statistical models which only characterize the observational distribution (Peters et al., 2017), they would allow reasoning about changing conditions outside the observed distribution (e.g.counterfactual reasoning).They would also allow communicating their inner workings more effectively -allowing us to ask \"why\" a given conclusion was reached, much in the same way that we do in scientific communication.Despite still being actively researched, there is now a mature body of work on understanding whether two or more variables are related as cause and effect (Peters et al., 2017).Many techniques assume that the variables are given, and concern themselves with finding relationship between them (Spirtes & Glymour, 1991;Chickering, 2003;Lorch et al., 2021).On the other hand, an advantage of modern deep neural networks is that they learn intermediate representations that do not have to be manually labeled (Yosinski et al., 2015), and effective models can be trained without supervision (Kingma & Welling, 2014).An important question then arises: can a deep network simultaneously discover latent variables in the data and establish cause-effect relationships between them?We focus on learning Additive Noise Models (ANM) with Gaussian noise, which are identifiable (i.e.causal directions are distinguishable) as long as the functions relating the variables of interest are not linear (Hoyer et al., 2008).This model fits well a variational learning framework, and so we are able to derive an analogue of a Variational Auto-Encoder (VAE) (Kingma & Welling, 2014) where the prior, rather than being an uninformative Gaussian, corresponds exactly to the ANM.When the ANM is linear with Gaussian noise, the joint probability of the variables also becomes Gaussian, and it is easy to perform variational inference.The dependencies between variables will then be expressed in the covariance matrix's sparsity structure.However, as mentioned earlier to make the causal directions identifiable the model cannot be linear (Hoyer et al., 2008).We resolve this difficulty by learning models that are locally linear, but globally non-linear.This approach affords the full generality of a non-linear ANM, with the simplicity of variational inference on Gaussian models.In summary, our contributions are:\u2022 A rigorous derivation of the variational Evidence Lower Bound (ELBO) of an Additive Noise Model (ANM), allowing efficient inference of Structural Equation Models (SEM) with deep networks.\u2022 A linearization method leveraging automatic differentiation to construct a local Gaussian approximation of arbitrary non-linear ANMs.\u2022 A temporally-aware specialization of the causal ANM that encodes causal directions implicit in the arrow-of-time and is suitable for high-dimensional time series data such as video.\u2022 Experiments demonstrating that the proposed method is able to fit latent variables with a dependence structure in high-dimensional data, namely a synthetic image dataset and video game based data."}
{"paper_id": 485, "introduction": "How to represent the visual existence of an object in a discriminative fashion is a core question of computer vision.In this paper, we propose a hierarchical part-whole representation to represent the visual existence of objects.We adopt multi-object tracking as the application area since the distinguishable appearance feature is critical to avoid the mismatch among target objects when tracking across frames.To gather and process the visual information from different levels, we combine the hierarchical part-whole representation with the attention mechanism from transformers to summarize distinguishable and discriminative visual representations for objects of interest.In the task of multi-object tracking, given a bounding box to localize objects of interest, how should we recognize the major object within the box and distinguish it from the background and other objects, especially some also having partial existence in the box?We believe the visual specificity of one object comes from three perspectives: the compositional, the semantic and the contextual.The compositional suggests the salient and unique visual regions on an object, such as a hat on a pedestrian whose color is different from all others in the same image.With a salient visual composition attached to an object, we can track it across frames even without seeing its full body.The semantic visual information is the commonly adopted one in modern computer vision such as a tight bounding box or instance segmentation mask.It defines the occupancy area of the object with the bond between its visual existence and semantic concept.Finally, contextual visual information describes the surroundings of an object.It helps to distinguish an object via contrast.For example, the bounding box might contain pixels from the background and secondary objects.However, a tight bounding box offers a strong underlying prior when combined with visual context: an object whose parts span across the boundary of the bounding box should not be the major object of this bounding box.Being the secondary object or not an object of interest, it should be regarded as noise when we generate a distinguishable visual representation for the major subject in the bounding box.The analysis above shows each level has its value to represent an object discriminatively.Motivated by the insight, we propose to represent an object by a three-level hierarchy: body parts, full body, and the union area including objects with overlap.We summarize it as a \"Part-Body-Union\" hierarchy.With the hierarchy constructed, an ideal path to solving the target association in multi-object tracking is to leverage the salient information within the body area and discard mismatch by eliminating the noise revealed by the contextual contrast.Without requiring more fine-grained data annotation, we propose to use transformers to process the hierarchical representation as the attention mechanism can discover important visual information.So, by combining the hierarchical visual representation and attention-based feature fusion, we finally propose our method as Hierarchical Part-Whole Attention, or HiPWA for short.In this work, we build a baseline model following this design and demonstrate its effectiveness in solving multi-object tracking problems.Through experiments on multiple multiobject tracking datasets, the proposed method achieves comparable or even better performance than the state-of-the-art transformer-based methods with a more lightweight implementation and better time efficiency during training and inference."}
{"paper_id": 486, "introduction": "Large-scale Pre-Trained Language Models (PTLMs) such as BERT (Devlin et al., 2019) and GPT models (Radford et al., 2019;Brown et al., 2020) have recently achieved great success in varieties of Natural Language Processing (NLP) tasks.These large-scale PTLMs capture knowledge from massively labeled and unlabeled human written data which can potentially contain harmful contents and societal biases.The goal of a language model is to estimate the probability of a sequence of words for the given language.One can argue that, when the data from which the model was trained on is different than the desired behavior of the model at a semantic level, representational harms are present.Several recent studies have highlighted the manifestation of societal biases in language models and proposed metrics and datasets to quantify them based on sentiment (Kurita et al., 2019), regard (Sheng et al., 2019), stereotypes (Zhao et al., 2019;Nadeem et al., 2021), style (Smith et al., 2022), or morality (Schramowski et al., 2022).In this work, we focus on the PTLMs' propensity to associate specific individuals or groups with negative perception.These negative perceptions are the result of microaggression, stereotypes, or implicit hate speech in the pre-training corpus of large language models.These harmful representations are usually overlooked by toxic language detectors (Breitfeller et al., 2019;Hartvigsen et al., 2022), while they can resurface in language technologies and disadvantage an already disadvantaged group of people.Moreover, existing metrics usually fail at conceptualization of these harms which is a prerequisite for effective measurement.And even when the desired construct is clearly articulated, its measurement is not well matched to its conceptualization (Blodgett et al., 2021).Our contributions are two folds.First, we provide a clear conceptualization of representational harms towards 13 marginalized demographics and propose a new metric for quantifying them in PTLMs.Our proposed metric can be applied to any dataset that contains harmful versus benign examples.Moreover, we address some of the shortcomings in the existing metrics in our metric.Second, we conduct an empirical study of the representational harms in 24 well-known PTLMs with respect to demographic, correlation with existing metrics, and network architecture."}
{"paper_id": 487, "introduction": "In recent years, self-supervised contrastive learning (CL) has demonstrated tremendous potential in learning generalizable representations from unlabeled datasets (Chen et al., 2020b;He et al., 2020;Grill et al., 2020;Caron et al., 2020;Chen & He, 2021;Zhong et al., 2021b).Current state-of-the-art CL algorithms learn representations from ImageNet (Deng et al., 2009) that match or even exceed the accuracy of their supervised learning (SL) counterparts on ImageNet and downstream tasks.However, beyond accuracy, little attention is paid on comparing other behavioral differences between contrastive learning and supervised learning, and even less work investigates the robustness during pre-training.Robustness is an important aspect to evaluate machine learning algorithms.For example, robustness to long-tail or noisy training data allows the learning algorithm to work well in a wide variety of imperfect real-world scenarios (Wang et al., 2017).Robustness of the model output across training iterations enables anytime early-stop (Hu et al., 2019) and smoother continual learning (Shen et al., 2020).Robustness to input corruptions at test-time plays an important role in reliable deployment of trained models in safety-critical applications, as signified by the existence of adversarial examples (Goodfellow et al., 2015;Salman et al., 2020) and the negative impact of domain shift (Zhao et al., 2019).In this paper, we investigate whether CL and SL behave robustly to data distribution changes.In particular, how does changes in data affect behaviors of algorithms?And do CL and SL behave similarly?To this end, we design a wide-spectrum of corruptions as shown in Figure 1 to alter data distribution and conduct comprehensive experiments, with different backbones, CL algorithms and datasets.The corruptions are carefully selected to be multi-level, targeting both human-recognizable and unrecognizable structural information, and are rooted in prior literature: pixel-level corruptions distorts intensity distribution, patch-level shuffle corrupts spatial structure (Ge et al., 2021;Neyshabur et al., 2020;Zhang et al., 2017;Hendrycks & Dietterich, 2019), and dataset-level class imbalance (Liu et al., 2022;2019;Samuel & Chechik, 2021) and GAN (generative adversarial network) synthesis (Jahanian et al., 2021) shift the overall distribution.Our main results consist of two sets of experiments: The first set investigates the downstream robustness of pre-trained models towards corruptions of downstream data.The second set studies the robustness under pre-training data corruptions -when the accuracy degradation of an algorithm to some corruption is large, it suggests that the algorithm may leverage such information as learning signal.Note that our work is inspired by Zhang et al. (2017) and Ribeiro et al. (2020) and follows a similar empirical exploratory analysis, rather than a regular adversarial robustness paradigm.We deliver a set of intriguing new discoveries.We generally observe that CL is consistently more robust than SL to downstream corruptions.Meanwhile, contrastive learning on corrupted pre-training leads to diverging observations: CL is more robust to dataset-level corruption than SL, but much less so to pixel-and patch-level corruptions.Moreover, we discover the higher dependence of contrastive learning on spatial information during pre-training, such that a global patch shuffling corruption harms feature learning greatly.To understand why pre-trained CL models are more robust to downstream corruptions, we analyze the learning dynamics through feature space metrics and find that CL yields larger overall and steadily-increasing per-class feature uniformity and higher stability than SL.The instance-level CL objective might capture richer sets of features not limited to semantic classes.Therefore, the perclass uniformity or intra-class variation is not compressed as hard as in SL.This allows the CL models to generalize to unseen corrupted downstream data better than SL.Such hypothesis aligns well with several recent attempts to understand CL (Zhao et al., 2021;Chen et al., 2021a;Liu et al., 2022).An immediate consequence of our insight is an improvement to supervised pre-training by adding a uniformity regularization term to explicitly promote intra-class variance, where the testtime data corruption robustness is improved.As for CL's vulnerability to pre-training data corruptions such as patch shuffling, we speculate that CL is more dependent on the spatial structure of images, and the introduction of high-frequency noise undermines the long-scale spatial coherence of natural images.For example, with global patch shuffling, the random resized cropping used in CL is no longer a proper data augmentation.We verify our intuition by manipulating data pre-processing and analyzing attention maps.We find that corrupting after standard data augmentation recovers a substantial amount of robustness, making CL comparably robust to SL.We summarize our contributions as follows.(1) We design extensive distributional robustness tests to study the behavioral differences of CL and SL systematically.(2) We discover diverging robustness behaviors between CL and SL, and even among different CL algorithms.(3) We offer analyses and explanations for such observations, and show a simple way to improve the downstream robustness of supervised learning.We claim our paper as an empirical study.We hope our findings can serve as an initial step to fully understand CL's behaviors beyond accuracy and inspire more future studies to explore such aspects through theoretical analysis."}
{"paper_id": 488, "introduction": "The state-of-the-art deep neural networks have equipped a various of applications with much better quality, especially the emergence of BertDevlin et al. (2018), a TransformerVaswani et al. (2017)based pre-training language model, and a series of its derivatives Brown et al. (2020); Lan et al. (2019).Their great success is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters.However, it is rather difficult to deploy them to real-time products such as Fraud Detection Senator et al. (1995); Kirkland et al. (1999), Search and Recommendation systems Covington et al. (2016); Ren et al. (2021), and many mobile applications, not only because of the high computational complexity but also the large memory requirements.Several techniques are developed to make the trade-off between performance and model scale.Knowledge Distillation (KD) Hinton et al. (2015); Sanh et al. (2019) is the most empirically successful approach used to transfer the knowledge learnt from a heavy Teacher to a more light-weight and faster Student.Besides, Pruning Han et al. (2015); Frankle & Carbin (2018) and Quantization Han et al. (2015); Chen et al. (2020) further compress deep models even smaller.In many practical situations, however, we need super tiny models to meet the demanding memory and latency requirements, which would inevitably suffer serious performance degradation.From another perspective, multi-stage classification system Trapeznikov et al. (2012) is widely used to reduce the opportunity of calling the deep and cumbersome models by filtering out some or even most of the input samples using simpler and faster models trained with limited data and easier features.In a multi-stage system, light-weight models such as SVM, Logistic Regression or k-Nearest Neighbors are used as earlier stage classifiers, classifying the samples (usually relatively easier negative ones) based on simple or easily accessible features, and leaving indeterminate ones for later.Models of later stages need to be heavier to deal with harder samples as well as more complex and costly features.A two-stage working mechanism is simply shown in Figure 1(a).In several practical multi-stage applications, as shown in Figure 1 To build tighter connections between classifiers in a multi-stage system for better collaboration, most exist methods Mendes et al. (2020); Qi et al. (2019); Sabokrou et al. (2017); Zeng et al. (2013) jointly optimize the multi-stage classifiers in a way like cascade, allowing the contextual information to transfer from earlier stages to later.However, most of them primarily consider classification accuracy rather than latency, and therefore would not make the classification decisions until the final stage.In this paper, we consider to further explore how to forge closer connections between classifiers in a two-stage classification problem.We propose a novel training framework, Feedback Training, where the whole decision-making pipeline is consisted of two classifiers, a extremely lightweight Pre-classifier followed by a relatively heavier Main-classifier.Different from existing methods, these two models are trained in the reverse order of inference, that is, the first-stage model would be trained under the guidance of the second-stage one through a sample weighting method.The capacity of Pre-classifier is more effectively explored by considering the learning results of Main-classifier.Our contributions can be summarized threefold:1) We propose a novel training framework for two-stage classification applications.2) We discuss a sample weighting method that assists Pre-classifier to learn according to its preference.3) We verify our approach on two data sets that it outperforms baseline models significantly and shows greater superiority under few-shot scenarios."}
{"paper_id": 489, "introduction": "Many reinforcement learning (RL) problems require designing a distribution of tasks to train effective policies for the real-world (Taylor & Stone, 2009).However, designing the full space of tasks is challenging; the real world is complicated and specifying every edge task is impractical or even infeasible for certain domains (Wang et al., 2019;Parker-Holder et al., 2022b).These tasks might also be onerous for the agent to solve without the provision of scaffolding.Training the RL agent on all possible tasks is intractable under a limited training budget (Schmidhuber, 2013;Narvekar;Zeng et al., 2022;Florensa et al., 2018a).The state-of-the-art approaches to this problem use multi-agent curriculum generation algorithms for automating task generation.A teacher agent learns to generate tasks judiciously to train a student agent (Dennis et al., 2020;Du et al., 2022;Campero et al., 2020;Florensa et al., 2018a;Portelas et al., 2020;Matiisen et al., 2020).The teacher is rewarded by the difficulty of tasks it generates.Even though prior methods share intuitions on the teacher objective, there is no framework for understanding the kind of tasks that best enables student learning and how the teacher should be rewarded to this end.These challenges suggest that we should re-assess how we formalize and operationalize the objective for the teacher.Thus, our work is interested in the following question: What kind of tasks should the teacher generate and how should the teacher be rewarded?From the lens of developmental psychology, an answer is that the teacher should be incentivized to generate tasks within the student's zone of proximal development (ZPD) (Vygotsky & Cole, 1978;Vygotsky, 2012;Cole et al., 2005;Shabani et al., 2010).These tasks have two properties: They are within the student's difficulty and accelerate the student's learning progression.Albeit intuitive and widely known, ZPD lacks a computational framework.This makes operationalizing ZPD in the teaching setting difficult.Our work proposes ZONE as a computational framework that operationalizes ZPD.ZONE isolates the two properties of ZPD tasks into techniques for enforcing the teacher to generate within a student's ZPD.The first technique is REJECT , which omits training on tasks that fall outside the student's Our work focuses on two popular curriculum generation algorithms: PAIRED (Dennis et al., 2020) and Goal GAN (Florensa et al., 2018b).PAIRED is a successful regret-based algorithm applied to discrete domains, where an adversarial teacher generates tasks that maximize regret between a student and a competing student (the anti-student).Goal GAN is a similarly successful algorithm designed for continuous control domains where a generative adversarial network (GAN) (Goodfellow et al., 2020) teacher generates 2D navigation goals.We focus on these algorithms because several works build on them as either a competitive baseline or the basis of their own algorithms (Jiang et al., 2021a;Gur et al., 2021;Du et al., 2022;Parker-Holder et al., 2022a;Zhang et al., 2020).Additionally, the algorithms cover different properties of prior work, such as domains (discrete vs. continuous), difficulty criteria (static vs. dynamic), and teacher objectives (regret vs. non-regret).Prior intrinsic reward methods can alternatively enable generalization where additional reward supplements the extrinsic reward to incentivize exploration of the environment (Campero et al., 2020;Zhang et al., 2021;Raileanu & Rockt\u00e4schel, 2019).However, our work focuses on the teacher determining the student's curriculum and considers these methods out of scope.ZPD-based teaching A teacher acting on ZPD generates tasks with two types of properties.The first property focuses on difficulty: curriculum tasks are neither too easy nor too hard.In prior work like PAIRED (Dennis et al., 2020) and Goal GAN (Florensa et al., 2018b), a teacher generates a batch of tasks and trains the student on the entire batch regardless of whether the tasks meet the difficulty criterion.Training the student on problems that are too easy and too hard slows down the student's learning.In contrast, our work proposes a sample-efficient rejection sampling technique inspired by ZONE: we replace tasks that don't meet the difficulty criterion with those that do, and train on the resulting batch.The second property focuses on progression: curriculum tasks accelerate the student's ability to generalize.In general, identifying such tasks prior to training the student is difficult.A proxy for identifying such tasks is picking those that induce large model changes in the student, such as the norm of the student's model gradients.Prior work in the supervised learning setting have employed the gradient norm as a heuristic in active learning (Ash et al., 2019;Jiang et al., 2019).Prior work in intrinsic rewards for RL agents has used the gradient norm for learning better world models (Sun et al., 2011;Houthooft et al., 2016).However, to the best of our knowledge, the idea of maximizing the gradient norm of a student has not been used as a curriculum learning objective."}
{"paper_id": 490, "introduction": "BERT (Devlin et al., 2019) has brought about a sea change in the field of Natural Language Processing (NLP).Following BERT, numerous subsequent works focus on various perspectives to further improve its performance, e.g., hyper-parameter (Liu et al., 2019b), pre-training corpus (Liu et al., 2019b;Raffel et al., 2020), learnable embedding paradigm (Raffel et al., 2020), pre-training task (Clark et al., 2020), architecture (Gao et al., 2022) and self-attention (Shi et al., 2021), etc.However, there are massive redundancies in the above BERT-style models w.r.t.attention heads (Michel et al., 2019;Dong et al., 2021), weights (Gordon et al., 2020), and layers (Fan et al., 2020).Consequently, many compact BERT-style language models are proposed via pruning (Fan et al., 2020;Guo et al., 2019), quantization (Shen et al., 2020), parameter sharing (Lan et al., 2020) and Knowledge Distillation (KD) (Iandola et al., 2020;Pan et al., 2021).In this paper, we focus on the KD-based compression approaches.From the point of view of learning procedure, KD is used in the pre-training (Turc et al., 2019;Sanh et al., 2019;Sun et al., 2020;Jiao et al., 2020) and fine-tuning phases (Sun et al., 2019;Jiao et al., 2020;Wu et al., 2021).On the other hand, from the point of view of distillation objective, KD is employed for the outputs of hidden layer (Sun et al., 2020), final layer (Wu et al., 2021), embedding (Sanh et al., 2019) and self-attention (Wang et al., 2020).Wu et al. (2021) employ multiple teachers to achieve better performance than single-teacher KD based approaches on several downstream tasks of GLUE benchmark (Wang et al., 2019).As shown in Table 1, nevertheless, the ensemble of multiple teachers are not always more effective than the single teacher for student distillation.There are two possible reasons: 1) diversity losing (Tran et al., 2020) and 2) capacity gap (Mirzadeh et al., 2020).On the one hand, the ensemble prediction of multi-teacher KD loses the diversity of each teacher.On the other hand, between the large-capacity teacher ensemble and small-capacity student, there is a capacity gap which can be prone to unsatisfactory distillation performance.Table 1: Performances of knowledge distillation using single and multiple teachers for a 6-layer BERT-style language model on the development set of GLUE benchmark.In this experiment, we employ five teachers, i.e.T10 to T14 shown in Appendix C.1, for single-teacher distillation and multi-teacher distillation.We introduce the implementation details in Appendix H.MRPC RTE CoLA SST-2 QQP QNLI MNLI Metrics F1+acc 2 acc Mcc acc F1+acc 2 acc m Best Single Teacher \u2020 T13 T10 T10 T12 T11 T12 T14 \u2021 Single-teacher KD 90.0 73.3 49.3 93.1 89.0 91.4 83.5 Multi-teacher KD 89.7 73.7 50.1 92.2 88.6 91.1 83.6 Gain -0.3 +0.4 +0.8 -0.9 -0.4 -0.3 +0.1\u2020 The best teacher for student distillation on each downstream task as shown in Table 15.\u2021 Pre-training with whole word masking.To solve the above mentioned issues, we propose AutoSKDBERT which stochastically samples a teacher from a predefined teacher team following a categorical distribution in each step, to transfer knowledge into student.The task of AutoSKDBERT is learning the optimal categorical distribution to achieve high performance.1) Given a teacher team which consists of multiple teachers with multi-level capacities, AutoSKDBERT optimizes an initialized categorical distribution to distinguish effective teachers from ineffective teachers in phase-1 optimization.2) The sampling weights of the ineffective teachers are assigned to the effective teachers via teacher selection strategy after phase-1 optimization completion.3) AutoSKDBERT further optimizes the weights of the effective teachers rather than the ineffective teachers' in phase-2 optimization.We implement extensive experiments on GLUE benchmark (Wang et al., 2019) to verify the effectiveness of the proposed AutoSKD-BERT.Moreover, to show the generalization capacity, we have also distilled deep convolutional neural network (e.g., ResNet (He et al., 2016), Wide ResNet (Zagoruyko & Komodakis, 2016)) by AutoSKDBERT for image classification on CIFAR-100 (Krizhevsky et al., 2009), as shown in Appendix B. Our contributions are summarized as followsfoot_0 :\u2022 We propose AutoSKDBERT which stochastically samples a teacher from the predefined teacher team following the categorical distribution in each step, to transfer knowledge into the student of BERT-style language model.\u2022 We propose a two-phase optimization framework with teacher selection strategy to select effective teachers and learn the optimal categorical distribution in a differentiable way.\u2022 We propose Stochastic Single-Weight Optimization (SSWO) strategy to alleviate the consistency gap between the categorical distribution optimization and evaluation for performance improvement."}
{"paper_id": 491, "introduction": "Domain generalization (DG) asks learned models to perform well on unseen domains, which lies its key in learning domain-invariant representations that are robust to domain shift (Ben-David et al., 2006).Standard training often results in entangled domain-invariant and domain-specific features, which hinders the model from generalizing to new domains.Existing methods address this issue by introducing various forms of regularization, such as adopting alignment (Muandet et al., 2013;Ghifary et al., 2016;Li et al., 2018b;Hu et al., 2020), using domain-adversarial training (Ganin et al., 2016;Li et al., 2018b;Yang et al., 2021;Li et al., 2018c), or developing meta-learning methods (Li et al., 2018a;Balaji et al., 2018;Dou et al., 2019;Li et al., 2019).Despite the success of these arts, DG remains challenging and is far from being solved.For example, as a recent study (Gulrajani & Lopez-Paz, 2021) suggests, under a rigorous evaluation protocol, it turns out that the naive empirical risk minimization (ERM) method (Vapnik, 1999), which aggregates training data from all domains and trains them in an end-to-end manner without additional efforts, can perform competitively against more elaborate alternatives.This observation indicates that a more effective approach might be needed to disentangle the domain-invariant and domain-specific features for better DG.In this paper, we adopt a simple method by leveraging a conventional dual-branching network with one branch predicting image classes (target prediction) and another predicting domain labels.Regarding the features from the target and domain branches as domain-invariant and domain-specific representations, respectively, entanglement will result in an undesired situation where the domainspecific information is also encoded in the target branch, which will inevitably corrupt the prediction when the domain varies during inference.Thus, to explicitly disentangle the domain-invariant and domain-specific features, we impose a regularization to require the former to be independent of the latter.This idea seems straightforward, but we show that several factors need to be carefully considered for it to work effectively.Particularly, we first investigate the structure of the dual-branching network and somehow surprisingly discover that the common practice of using a shared base feature extractor with two lightweight prediction heads (Chen et al., 2021;Atzmon et al., 2020) is detrimental to the performance.Instead, a simple early-branching architecture, where the domain classification branch and target classification branch share the first few blocks while diverging thereafter, yields the optimal results.Incorporating this discovery, we propose the basic form of the proposed method.Specifically, we employ Hilbert-Schmidt Information Criterion (HSIC) (Gretton et al., 2005;2007) as a measurement of the feature independencefoot_0 and use two sub-networks (branches) with only a few shared convolution blocks for target prediction task and domain prediction task.A glimpse of the basic form is shown in Figure 1.Next, to further unleash the power of the proposed method, we suggest using domain augmentation to encourage the domain-invariant features to explore sufficient diversity of domain-specific representations.Precisely, we propose a new random style sampling (RDS) scheme that explores augmenting the domain types by incorporating features with randomly modified style statistics.In contrast to previous methods (Zhou et al., 2021;Li et al., 2022) that use mixing or adding noise to synthesize new domains, RDS can directly perturb the mean and variance of feature maps with a controllable perturbing strength.To seamlessly integrate the basic form and the augmentation strategy, we further propose subsequential loss terms to encourage the target branch to be invariant to the original and augmented representations and vice versa for the domain-specific branch.Through our experimental studies, we illustrate, (1) the effectiveness of enforcing independence of the class and domain features within the early-branching design; (2) the advantages of the proposed RDS methods compared to existing solutions (Zhou et al., 2021;Li et al., 2022), and effectiveness of the adopted loss functions; (3) our complete method performs favorably against other state-of-the-art algorithms when evaluated in the current benchmark (Gulrajani & Lopez-Paz, 2021)."}
{"paper_id": 492, "introduction": "Self-supervised learning (SSL) has achieved overwhelming success in unsupervised representation learning, with astonishingly high performance in many downstream tasks like classification (Zhou et al., 2022a;b), object detection, and segmentation (Bao et al., 2021;He et al., 2022).In SSL, a pretext task is first built, e.g., instance discrimination task (He et al., 2020;Chen* et al., 2021) or masked image modeling (MIM) (Bao et al., 2021;He et al., 2022), and then pseudo labels are generated via the pretext task to train a network model without requiring manual labels.Though successful, SSL is developing towards a direction of requiring increasingly large training costs, e.g., 200 training epochs in MoCo (He et al., 2020) while 16,00 epochs in MAE (He et al., 2022) to release its potential.Unfortunately, most researchers only have limited computational budgets and often cannot afford to train large SSL models.Moreover, the pretrained non-SOTA SSL models are rarely used in practice, since SOTA is updated frequently and a previous one quickly becomes useless, wasting huge training resources.Thus, a sustainable SSL framework is much demanded.Just like how human experience is enriched and passed from one generation to the next in human society, we try to let an SSL model inherit the knowledge from a pretrained SSL base model to achieve superior representation learning ability for \"sustainable\" learning and also to improve learning efficiency than training a new SSL model from scratch.Fig. 1 illustrates the sustainable SSL for more clarity, in which we call the new SSL model to be trained as the new model and the pretrained SSL model as the base model.To surpass the base model, in sustainable SSL, the new model exploits not only the implicit base model knowledge but also the absent knowledge in the base model.Such a learning process follows a fully self-supervised manner and differs from the self-training schemes (Xie et al., 2020;Yalniz et al., 2019) that require labels for supervised learn-ing.This process can be regarded as a special case of Knowledge Distillation (KD) (Hinton et al., 2015;Gou et al., 2021), which targets at learning a more powerful new model based on the base model in a self-supervised setting.In this work, we take an explorative step towards sustainable SSL by efficiently learning from existing pretrained SSL models and surpassing them.In this work, to achieve this challenging goal, we encourage the new model to learn not only knowledge of the base model but also more semantic-related new knowledge.We therefore choose a mask-reconstruction (He et al., 2022) SSL scheme to train the new model, in which the base model generates reconstruction targets from the full input images and the new model tries to predict the generated targets from randomly masked image input.With this pretext task, the new model is forced to learn the semantics of the full input and its patch relations so that the new model can reason the desired full information from an incomplete input.As illustrated by Fig. 2, the attentions of iBOT (Zhou et al., 2022a) miss some semantic regions, e.g., ears, while TEC with iBOT as the base model captures all semantics and well distinguishes all different components of an input image.Because of its more powerful ability to capture comprehensive semantic, TEC helps achieve the challenging sustainable SSL, and actually can provide rich and flexible semantics for downstream tasks.However, different SSL base models could have various properties due to their various training targets and training strategies, e.g., iBOT models with more category semantics while MAE models with more image details (He et al., 2022).So it is important to build high-qualified and compatible reconstruction targets from the base model so that the new model learns these targets in a complementary manner.A good model target should reveal the semantic relations among patches, e.g., the relation between car wheels and car body, so that new model can learn these general relation patterns and adapts to downstream tasks.To this end, we propose to enhance the target quality of the base model by using two complementary reconstruction targets: a) the patch-dim normalization which normalizes base model targets along patch dimension to enhance the relations among input patches, and b) patch attention maps with rich semantics to filter out possible noise and establish the correlation between the whole image semantic and the patch semantic.For target compatibility, we introduce conditional adapters into the new model so that new model predictions can be adaptable to various base models with different properties.Given a base model target, adapters conditionally active and adjust mid-level features of the new model to predict the target more effectively.These adapters are discarded after pretraining but can serve parameter-efficient finetuning (Jia et al., 2022;Chen et al., 2022b) if kept.We call the above method for sustainable SSL as Target-Enhanced Conditional (TEC) maskreconstruction.As shown in Fig. 3, on ImageNet, TEC without any extra training data improves the SSL base model by a remarkable margin, e.g., MAE (He et al., 2022) and iBOT (Zhou et al., 2022a).For instance, taking iBOT with 1600 epochs as base model, TEC with only 800 training epochs makes 1.0% improvement.Moreover, we also find that TEC can significantly accelerate the SSL learning process and saves training cost.For example, training TEC for only 100 epochs with random initialization and a 300-epochs-trained MAE base model outperforms MAE trained with 1600 epochs.This work takes one step closer to sustainable SSL, and we hope our initial effort will inspire more works in the future to sustainably improve SSL in a cost-friendly manner."}
{"paper_id": 493, "introduction": "Room acoustic [3,50,1,27] aims at accurately capturing the sound propagation process in a room environment, so as to determine the received sound characterizing reverberation at any spatial position.Robots and other intelligent agents (e.g.voice assistants) can exploit such sound for a variety of tasks, including navigation [9,44,29], reconstruction [62,8] and audio-dependent simultaneous localization and mapping (SLAM) [16].As a complementary sensor to vision, sound perception exhibits strengths in scenarios involving dim or no lighting and occlusion.In most cases, room acoustic can be treated as linear time-invariant system (LTI).Thus, the underlying sound propagation problem is to precisely derive a room impulse response (RIR) -a transfer function of time and other acoustic variables that measures the behaviour a sound takes from a sound source to a receiver.The sound recorded at a particular position can thus be obtained by convolving the sound at the source position with the RIR.We call such RIR source-to-receiver RIR (s2r-RIR).Accurately deriving s2r-RIR, however, is a difficult task.The challenge is three-fold: 1) high computational cost: traditional methods [3,50,1,27,41,26,4,22] undergo rigorous mathematical derivation and extensive measurements to compute s2r-RIR, by treating sound as either rays [50,1,27] or waveforms [3,41].The corresponding computation complexity is proportional to the room geometric layout complexity and the number of sources.2) non-scalable: the whole computation process needs to be re-executed once the source/receiver location changes slightly, or the room layout has altered (e.g.furniture movement).3) too strong assumptions: the assumption that the source/receiver location and room acoustic properties are well-defined and known in advance is too strong to be held in real scenarios.The sound source location is mostly unknown in real-scenarios, and localizing sound sources is an extremely difficult task [20,23,6,19].In this work, we propose SoundNeRirF, a receiver-to-receiver Sound Neural Room impulse response Field for efficiently predicting what sounds will be heard at arbitrary positions.SoundNeRirF requires knowledge of neither sound sources (e.g.source number and position) nor room acoustic properties 1 , but instead represents a room acoustic scene through a sparse set of 3D spatial positions that robots have explored, as well as the sound recorded by the robot at each position.A robot equipped with a receiver can easily collect massive such datasets by simply moving around the room and recording its position and received sound at each step.SoundNeRirF represents a room acoustic scene by a continuous 6D function, which takes two receivers' 3D positions (one reference and one target position) as input, and outputs two room impulse responses that jointly project the sound from the reference position to the target position.SoundNeRirF thus learns receiver-to-receiver RIR (r2r-RIR).SoundNeRirF is reinforced to implicitly encode the interaction between sound sources and receivers, in addition to room acoustic properties by minimizing the error between predicted and the truly recorded sounds, because sound recorded at any position is naturally a result of such an interaction.By instantiating the 6D function as multi-layer perceptrons (MLPs), SoundNeRirF is differentiable and continuous for any spatial position, and can be optimized with gradient descent.Figure 1 visualizes SoundNeRirF's motivation.Specifically, SoundNeRirF splits r2r-RIR into two parts: an inverse-RIR for the reference position and a forward-RIR for the target position.We further introduce receiver virtual position settings and a physical constraint strategy that guides SoundNeRirF to explicitly learn direct-path, specularreflection and late-reverberation that commonly exist in room acoustics.SoundNeRirF has numerous applications in robotics, including immersive audio/video game experience in augmented reality (AR) [24,57], sound pre-hearing without actually reaching to the location [5], and improving robot localization (by utilizing the predicted sound and truly heard sound).In summary, we make four main contributions: First, we propose SoundNeRirF that learns r2r-RIR neural field from a sparse set of receiver recordings.Second, SoundNeRirF requires knowledge of neither sound source nor room acoustic properties, but instead depends on more accessible data that can be collected by robot walking around in the room environment.Third, SoundNeRirF directly predicts sound raw waveform.It disentangles sound prediction and r2r-RIR learning, exhibiting strong generalization capability in predicting previously unheard sound (see Experiment Sec.).Lastly, we release all datasets collected in either synthetic and real-world scenario to the public to foster more research."}
{"paper_id": 494, "introduction": "Suppose you went to the seaside and heard a cacophony of seagulls, squawking and squabbling.An interesting question that naturally arises is whether you can tell the number of seagulls flocking around you from the sound you heard?Although a trivial example, this sound \"crowd counting\" problem has a number of important applications.For example, passive acoustic monitoring (PAM) is widely used to record sounds in natural habitats, which provides measures of ecosystem diversity and density [2,15,12].Sound counting helps to quantify and map sound pollution by counting the number of individual polluting events [4].It can also be used in music content analysis [24].Despite its importance, research on sound counting has far lagged behind than its well-established crowd counting counterparts from either images [49,46], video [29] or joint audio-visual [22].We conjecture that the lack of exploration stems from three main factors.First, sound counting has long been thought of as an over-solved problem by sound event detection (SED) methods [35,9,1,19], in which SED goes further to identify each sound event's (e.g. a bird call) start time, end time and semantic identity.Sound counting number then becomes easily accessible by simply adding up all detected events.Secondly, current SED only tags whether a class of sound event is present within a window, regardless of the number of concurrent sound sources of the same class like a series of baby crying or multiple bird calls [41].Thirdly, labelling acoustic data is technically-harder and more time-consuming than labelling images, due to the overlap of concurrent and diverse sources.The lack of well-labelled sound data in crowded sound scenes naturally hampers research progress.Existing SED sound datasets [1,20] capture simple acoustic scenarios with low polyphony and where the event variance is small.The simplified acoustic scenario in turn makes sound counting task by SED methods tackleable.But when the sound scene becomes much more complex with highly concurrent sound events, SED methods soon lose their capability in discriminating different sound events [38,9].Therefore, a study specific for sound counting problem is desirable and overdue.In this paper, we study the general sound counting problem under highly polyphonic, cluttered and concurrent situation.Whilst the challenges of image-based crowd counting mainly lie in spatial density, occlusion and view perspective distortion, the sound counting challenges are two-fold.Firstly, acoustic scenes are additive mixtures of sound along both time and frequency axes, making counting overlapping sounds difficult (temporal concurrence and spectrum-overlap).Secondly, there is a large variance in event loudness due to spherical signal attenuation with distance (loudness variance).Tackling these challenges require a more elegant method to process sound raw waveform so as to better localize sound in time-frequency domain.In this paper, we propose a novel dyadic decomposition neural network to learn a sound density representation capable of estimating cardinality directly from raw sound waveform.Unlike existing sound waveform processing methods that all apply frequency-selective filters on the raw waveform in single stage [19,10,48,18,14], our network progressively decomposes raw sound waveform in a dyadic manner, where the intermediate waveform convolved by each parent filter is further processed by its two child filters.The two child filters evenly split the parent filter's frequency response, with one child filter encoding the waveform approximation (the one with the lower-half frequency response) and the other one encoding the waveform details (the one with the higher-half frequency response).To accommodate sound loudness variance, spectrum-overlap and time-concurrence, we further propose an energy gain normalization module to regularize each intermediate parent waveform before feeding it to two child filters for further processing.This hierarchical dyadic decomposition front-end enables the neural network to learn a robust TF representation in multi-stage coarse-to-fine manner, while introducing negligible extra computation cost.By setting each filter's frequency cutoff parameters to be learnable and self-adjustable during optimization in data-driven way, the final learned TF representation can better characterize sound existence in time and frequency domain.Following the front-end, we add a backbone network to continue to learn a time framewise representation.Such representation can be used to derive the final sound count number by either directly regressing the count number, regressing density map (the one we choose) or following SED pipeline.Apart from the network, we further propose three polyphony-aware metrics to quantify sound counting task difficulty level: polyphony ratio, maximum polyphony and mean polyphony.We will give detailed discussion to show the feasibility of three metrics.We run experiments on four cross-domain sound datasets: a bird sound set (both real-world and synthetic), a telephone-ring sound set (synthetic), and music sound [24] (real-world).Experimental results show our method (DyDecNet) outperforms exiting SED-based methods significantly on both real-world and synthetic dataset.Replacing existing methods' one-stage sound raw waveform processing front-end with our dyadic decomposition front-end dramatically improves their performance accordingly.Since the real-world datasets contain relatively small polyphony level, we specially synthesize a bird sound dataset that contain much higher sound polyphonic level and spectral overlap.The synthesized sound dataset has two sub-sets: one involves four kinds of bird sound (exhibits heterophony); the other has just one kind of sound (this encapsulates homophonic scenario).Experiment on such synthetic dataset helps to test performance under highly polyphonic situation.In summary, we make three main contributions: First, propose dyadic decomposition front-end to decompose the raw waveform in a multi-stage, coarse-to-fine manner, which better handles loudness variance, spectrum-overlap and time-concurrence.Second, propose a new set of polyphony-aware evaluation metrics to comprehensively and objectively quantify sound counting difficulty level.Third, Show the efficiency and generalization of DyDecNet on sound datasets across different domains."}
{"paper_id": 495, "introduction": "Mobile agents often create maps to represent their surrounding environments [6].Typically, such a map is either topological or metrical (including hybrid ones).We consider a topological map to be metric-free, which means it does not explicitly store global/relative position/orientation information with measurable geometrical accuracy [39,38].Instead, it is a graph that stores local sensor observations, such as RGB images, as graph nodes and the spatial neighborhood structure (and often navigation actions) as graph edges that connects observations taken from nearby locations.While metric maps are often reconstructed by optimizing geometric constraints between landmarks and sensor poses from classic simultaneous localization and mapping (SLAM), topological maps have recently attracted attention in visual navigation tasks due to the simplicity, flexibility, scalability, and interpretability [58,13,27,40,12].A topological map used for visual navigation could be constructed in two ways.The first and simplest way is to let the agent explore the new environment through metric-free random walks, after which the map could be built by projecting the recorded observations into a feature space and adding edges between nearby or sequentially obtained features [58].However random walk is very inefficient especially in large or complex rooms, leading to repeated revisits of nearby locations in the same area.The other way is to design a navigation policy that controls the agent to more effectively explore the area while creating the map.It is known as active SLAM and often involves some metric information as either required input [42,12] or intermediate estimations [13].As shown in Fig. 1, could we combine the merits of the two ways by finding a metric-free (neither input nor estimates) exploration policy that discovers informative traversing trajectories in unknown environments for topological map construction after exploration?To achieve this objective, we propose Active Topological Mapping (ATM) as shown in Fig. 2. It contains two stages: active exploration through a learned metric-free policy, and topological mapping through visual place recognition (VPR) [51].The first stage adopts the task and motion planning formalism (TAMP) [26,55] and imitation learning [63] from expert demonstrations which could come from either an oracle policy having full access to virtual environments, or simply a human expert in real world.Our main novelty is to design such an imitation at both the task and the motion levels with joint end-to-end training.Our task planner, a two-layer LSTM [31] network trained with deep supervision, conceives the next best goal feature to be We focus on the active mapping problem where a mobile agent needs to decide how to efficiently explore a novel environment.For planning and navigation, we embrace the topological feature space where each feature corresponds to an image observation, while the metric space involves distance/pose information which is onerous to obtain accurately.Our main idea is to hallucinate goal features to guide exploration actions, learned by imitating expert demonstrations.explored by hallucination from the current and historical image features.Our motion planner, a simple multi-layer perceptron, fuses the current and the hallucinated features and generates the best action that will move the agent to a location whose feature is closer to the goal.The second stage of ATM takes all observations recorded during the active exploration stage to create the topological map.This stage could be solved similar to [58], where nodes are first connected by the sequential visiting order, and then additional node connections are discovered by a binary classifier estimating the spatial adjacency between two nodes through their image similarity.Differently, we adopt VPR, a classic technique in SLAM for loop closure detection, to discover additional edges more effectively.We further train an action assigner to assign each newly-added edge with corresponding actions that will move the agent between the two connected nodes.Finally, the topological map becomes our efficient environment representation for visual navigation as in [58].We validate the efficacy of ATM on two tasks: exploration in which the goal is to maximize the explored area within a fixed step budget, and navigation in which the goal is to use ATM-constructed topological map to navigate the agent to a target image.In summary, our contributions are:\u2022 We propose a simple and effective framework named as active topological mapping (ATM) for efficient and lightweight visual exploration.The topological map it constructs can be used for efficient visual navigation.\u2022 We develop joint trainable feature-space task and motion planning (TAMP) networks to achieve metric-free and generalizable exploration.\u2022 We design a deeply-supervised imitation learning strategy to train the feature-space TAMP networks with better data efficiency.\u2022 We validate our method on the photo-realistic Gibson [72] and MP3D [9] datasets in both visual exploration and navigation."}
{"paper_id": 496, "introduction": "A remarkable property underlying human intelligence is its systematic compositionality: the algebraic capacity to interpret an infinite number of novel combinations from finite known components (Chomsky, 1957)-\"infinite use of finite means\" (Chomsky, 1965).This type of compositionality is central to the human ability to generalize from limited data to novel combinations (Lake et al., 2017).Recently, several datasets have been proposed to test systematic generalization of machine learning models-SCAN (Lake & Baroni, 2018), PCFG (Hupkes et al., 2020), CFQ (Keysers et al., 2020), and HINT (Li et al., 2021), to name a few.While conventional neural networks fail dramatically on these datasets, certain inductive biases have been explored to improve systematic generalization.Csord\u00e1s et al. (2021); Ontan\u00f3n et al. (2022) improve Transformers' generalization performance by using relative positional encoding and sharing weights between layers.Chen et al. (2020) introduce a neural-symbolic stack machine to achieve nearly perfect accuracy on SCAN-like datasets.Despite the improved performance, these neural-symbolic methods often require domain-specific knowledge to design non-trivial symbolic components and are difficult to transfer to other domains.To achieve human-like systematic generalization in a wide range of domains, we propose Neural-Symbolic Recursive Machine (NSR), which integrates the joint learning of perception, syntax, and semantics in a principled framework.The core representation of NSR is a Grounded Symbol System (GSS) (see Fig. 1), which entirely emerges from training data without domain-specific knowledge.NSR implements a modular design for neural perception, syntactic parsing, and semantic reasoning.Specifically, we first utilize a neural network as the perception module to ground symbols on the raw inputs.Next, the symbols are parsed into a syntax tree of the Grounded Symbol System by a transition-based neural dependency parser (Chen & Manning, 2014).Finally, we adopt functional programs to realize the semantic meaning of symbols (Ellis et al., 2021).Theoretically, we show that the proposed NSR is expressive enough to model various sequence-to-sequence tasks.Critically, the inductive biases of equivariance and recursiveness, encoded in each module, enable NSR to break down the long input into small components, process them progressively, and compose the results, encouraging the model to learn meaningful symbols and their compositional rules.Such inductive biases are the crux of NSR's superb systematic generalization.It is challenging to optimize NSR in an end-to-end fashion since annotations for the internal GSS are oftentimes unavailable and NSR is not fully differentiable.To tackle this issue, we present a probabilistic learning framework and derive a novel deduction-abduction algorithm to coordinate the joint learning of different modules.In the learning phase (see also Fig. 2), the model first performs greedy deduction over these modules to propose an initial GSS, which may yield wrong results.Next, a search-based abduction is applied top-down to search the neighborhood of initial GSS for possible solutions; such abduction revises the GSS until it generates the correct result.As a plausible solution, the revised GSS provides pseudo supervision to train each module, facilitating the learning of individual components in NSR.We evaluate NSR on three benchmarks from various domains to study systematic generalization: (1) SCAN (Lake & Baroni, 2018), mapping natural language commands to action sequences; (2) PCFG (Hupkes et al., 2020), predicting the output sequences of string manipulation commands; (3) HINT (Li et al., 2021), predicting the results of handwritten arithmetic expressions.All these datasets include multiple splits for evaluating different aspects of systematic generalization.NSR achieves state-of-the-art performance on all these benchmarks.Specifically, NSR obtains 100% generalization accuracy on SCAN and PCFG and improves the state-of-the-art accuracy on HINT by about 23%.Result analyses reveal that NSR possesses stronger generalization than pure neural networks due to its symbolic representation and inductive bias.It also demonstrates better transferability than existing neural-symbolic approaches due to less domain-specific knowledge required.We also evaluate NSR on a proof-of-concept machine translation task from Lake & Baroni (2018) and the results demonstrate the promise of applying NSR to realistic domains."}
{"paper_id": 497, "introduction": "Recently, the inversion of Generative Adversarial Networks (GANs) (Goodfellow et al., 2020) has dramatically improved by using the prior knowledge of powerful unconditional generators (Karras et al., 2019;2020;2021) for the robust and disentangled image attribute editing (Abdal et al., 2019;Richardson et al., 2021;Tov et al., 2021;Alaluf et al., 2021a;b;Wang et al., 2022a).The early GAN inversion models mostly rely on per-image optimization (Abdal et al., 2019;2020;Zhu et al., 2020), which is extremely time-consuming.For real-time inference, the encoder-based GAN inversion method becomes prevalent (Richardson et al., 2021;Tov et al., 2021;Alaluf et al., 2021a;Moon & Park, 2022), which trains an encoder that returns the corresponding GAN latent of an input image.The acquired latent from the encoder is desired to reproduce the input image as closely as possible.However, the encoder needs to compress the image into a small dimension, i.e., low-rate inversion.For instance, in the case of StyleGAN2 (Karras et al., 2020), for encoding an image with the size 1024 2 \u00d7 3 the encoders return the corresponding latent with the size 18 \u00d7 512, which is extremely smaller than the original image dimension (about 0.3%).Due to the Information Bottleneck theory (Tishby & Zaslavsky, 2015;Wang et al., 2022a), an attempt to encode information into a small tensor occurs severe information loss, and it deteriorates the image details, i.e., high-frequency features.To overcome this shortage, recent GAN inversion models propose new directions, such as finetuning the generator (Roich et al., 2021;Alaluf et al., 2021b) or directly manipulating the intermediate feature of the generative model (Wang et al., 2022a) to deliver more information using higher dimensional features than latents, i.e., high-rate inversion.However, results of high-rate inversion models are still imperfect.Figure 1 shows the inversion results of high-rate inversion models, Hyper-Style (Alaluf et al., 2021b), and HFGI (Wang et al., 2022a).Though both models generally preserve coarse features, the details are distorted, e.g., boundaries of the letter and the shape of accessories.The aforementioned high-rate inversion models remarkably decrease distortion compared to the state-of-the-art low-rate inversion models, i.e., Restyle (Alaluf et al., 2021a).However, this does not mean that distortion on every frequency spectrum is evenly decreased.To explicitly check distortion per frequency sub-bands, we adopt a wavelet transform, which enables to use of both frequency and spatial information.The wavelet transform yields a total of four coefficients by passing filters, which are in a low-pass filter set F l = {LL} and a high-pass filter set F h = {LH, HL, HH}.In Figure 2a, we visualize the coefficients obtained by each filter.In Figure 2b, we compare L 2 between the coefficients of ground truth images and inverted images, yielded by filter f , i.e., L 2,f .While the high-rate inversion models apparently decrease L 2,f for f \u2208 F l , they marginally decrease or even increase L 2,f for f \u2208 F h , compared to Restyle.In the light of this observation, we can argue that the existing methods of increasing the information rate can decrease distortion on the low-frequency sub-band, but are not effective for decreasing distortion on the high-frequency sub-band.Contributions First, we prove that the widely used loss term in GAN inversions, i.e., L 2 , is biased on low-frequency by using the wavelet transform.Then, we propose a simple wavelet-based GAN inversion model, named WaGI, which effectively lowers distortions on both the low-frequency and high-frequency sub-bands.To the best of our knowledge, WaGI is the first attempt to interpret GAN inversion in the frequency domain.Especially, we propose two novel terms for our model: (i) wavelet loss and (ii) wavelet fusion.First, (i) amplifies the loss of the high-frequency sub-band by using the wavelet coefficients from f \u2208 F h .By using (i) at training, WaGI is proficient in reconstructing high-frequency details.Second, (ii) transfers the high-frequency features directly to the wavelet coefficients of the reconstructed image.Due to the wavelet upsampling structure of SWAGAN, we can explicitly manipulate the wavelet coefficients during the hierarchical upsampling.We demonstrate that WaGI shows outstanding results, compared to the existing state-of-the-art GAN inversion models (Alaluf et al., 2021b;Wang et al., 2022a).We achieve the lowest distortion among the existing GAN inversion models on the inversion scenario.Moreover, qualitative results show the robust preservation of image-wise details of our model, both on the inversion and editing scenarios via InterFaceGAN (Shen et al., 2020) and StyleCLIP (Patashnik et al., 2021).Finally, we elaborately show the ablation results and prove that each of our proposed methods is indeed effective."}
{"paper_id": 498, "introduction": "The popularization of portable devices with cameras greatly promotes the creation and broadcasting of online videos.These sufficient video data serve as essential prerequisites for relevant researches, e.g.video summarization (Potapov et al., 2014;Song et al., 2015;Zhang et al., 2018;Fajtl et al., 2018;Zhu et al., 2021), video highlights detection (VHD) (Yang et al., 2015;Xiong et al., 2019;Lei et al., 2021;Bhattacharya et al., 2021), and moment localization (Liu et al., 2018;Zhang et al., 2020;Rodriguez et al., 2020), to name a few.Currently, most VHD methods are developed under the closed world assumption, which requires both the number of highlight domains and the size of training data to be fixed in advance.However, as stated in Rebuffi et al. (2017), natural vision systems are inherently incremental by consistently receiving new data from different domains or categories.Taking the gourmet video as an example, in the beginning, one may be attracted by the clips of eating foods, but lately, he/she may raise new interests in cooking and want to checkout the detailed cooking steps in the same video.This indicates that the target set the model needs to handle is flexible in the open world.Under this practical setting, all existing VHD methods suffer from the scalability issue: they are unable to predict both the old and the newly added domains, unless they retrain models on the complete dataset.Since the training cost on videos is prohibitive, it is thus imperative to develop new methods to deal with the above incremental learning issues.Broadly speaking, there exist two major obstacles that hinder the development of incremental VHD: a high-quality VHD dataset with domain annotations and strong models tailored for this task.Recall existing datasets that are widely used in VHD research, including SumMe (Gygli et al., 2014), TVSum (Song et al., 2015), Video2GIF (Gygli et al., 2016), PHD (Garcia del Molino & Gygli, 2018), and QVHighlights (Lei et al., 2021), all of them suffer from threefold drawbacks: (1) only the feature representations of video frames are accessible instead of the raw videos, thus restricting the application of more powerful end-to-end models; (2) most datasets only have a limited number of videos with short duration and coarse annotations, which are insufficient for training deep models;(3) none of them has the video highlight domain or category labels, thus can not be directly used in incremental learning.In order to bridge the gap between VHD and incremental learning, we first collect a high-quality gourmet dataset from live videos, namely LiveFood.It contains over 5,100 carefully selected videos with 197 hours in total.Four domains are finely annotated, i.e., cooking, eating, ingredients and presentation.These related but distinctive domains provide a new test bed for incremental VHD tasks.To solve this new task, we propose a competitive model: Global Prototype Encoding (GPE) to learn new highlight concepts incrementally while still retaining knowledge learned in previous video domains/data.Specifically, GPE first extracts frame-wise features using a CNN, then employs a transformer encoder to aggregate the temporal context to each frame feature, obtaining temporalaware representations.Furthermore, each frame is classified by two groups of learnable prototypes: highlight prototypes and vanilla prototypes.With these prototypes, GPE optimizes a distance-based classification loss under L 2 metric and encourages incremental learning by confining the learned prototypes in new domains to be close to that previously observed.We systematically compare the GPE with different incremental learning methods on LiveFood.Experimental results show that GPE outperforms other methods on highlight detection accuracy (mAP) with much better training efficiency, using no complex exemplar selection or complicated replay schemes, strongly evidencing the effectiveness of GPE.The main contributions of this paper are summarized as follows:\u2022 We introduce a new task named incremental video highlights detection, which has important applications in practical scenarios.A high-quality LiveFood dataset is collected to facilitate research in this direction.LiveFood comprises over 5,100 carefully selected gourmet videos in high resolution, providing a new test bed for video highlights detection and domain-incremental learning tasks.\u2022 We propose a novel end-to-end model for solving incremental VHD, i.e., Global Prototype Encoding (GPE).GPE can incrementally identify highlight and vanilla frames in new highlight domains via learning extensible and parameterized highlight/vanilla prototypes.GPE achieves superior performance compared with other incremental learning methods, improving the detection performance (mAP) by 1.57% on average.The above results suggest that GPE can serve as a strong baseline for future research.\u2022 We provide comprehensive analyses of LiveFood as well as the proposed GPE model for deepening the understanding of both, as well as giving helpful insight for future development.We hope our work can inspire more researchers to work in incremental VHD, finally pushing forward the application of VHD in practical scenarios."}
{"paper_id": 499, "introduction": "The success of deep neural networks is based on their great expressive power, which increases exponentially with depth (Chatziafratis et al., 2019).However, the deep hierarchical architecture of a deep neural network may induce the exploding/vanishing gradient problem (Pascanu et al., 2013), which degrades performance and may even make training impossible.Several techniques have been suggested to address the aforementioned problem, including better weight initialization (Glorot & Bengio, 2010;He et al., 2015), activation functions (Nair & Hinton, 2010;Klambauer et al., 2017), residual connections (He et al., 2016), normalization methods (Ioffe & Szegedy, 2015;Ulyanov et al., 2016;Nam & Kim, 2018;Ba et al., 2016;Wu & He, 2018;Qiao et al., 2019), etc.However, Contrary to popular belief, gradient explosion is present in various modern deep learning architectures that use batch normalization and ReLU-like activation functions (Figure 1).The stable flow of forwarding activation does not guarantee the stable flow of backward gradient when an entropy difference exists between the layers (Philipp et al., 2017).A gradient explosion in modern deep learning architecture has been reported in a number of papers (Philipp et al., 2017;Frankle et al., 2020;You et al., 2017), but its cause and solution have not been sufficiently researched.(Philipp et al., 2017) discussed the context and presented an intuitive understanding of the problem, considering the modality of gradient explosion while activation remains stable, the increase in entropy induced by the activation function, the alleviation of the problem using residual learning, etc.However, although the authors observed that gradient explosion occurs only in the case of batch normalization, they did not explain this phenomenon.They speculated that the sampling error of the normalization procedure amplifies the problem-however, no clear correlation has been reported between the gradient explosion rate and the expected sampling error (batch size, difference normalization scheme, etc.).To the best of our knowledge, this is the first attempt to demonstrate how disharmony between activation function and batch normalization causes gradient explosion and training instability during the early stages of neural network training mathematically.The alleviation of the problem during training is also discussed.Exploding/vanishing gradient is a well-known problem in deep learning.It may seem unnatural that it still exists but is not widely known.One reason is that the problem is not as severe as before.The exploding rate is approximately \u03c0/(\u03c0 -1) \u223c 1.21 per effective depth.It is tolerable in networks with tens of layers or those with hundreds of layers and dense residual connections.Moreover, the exploding gradient at the initialization state is rapidly relieved during training, and even a vanishing gradient state can be approached.Thus, this problem has been referred to by different names in Figure 1: Gradient explosion rate ( V ar(g n )/V ar(g N )) of deep neural network models at the initialization state corresponding to (a) different architectures and (b) activation functions.The explosion rate is approximately \u03c0/(\u03c0 -1) in vanilla networks with batch normalization; but it is lower in architectures with residual connections (He et al., 2016), which reduces the effective depth.Moreover, gradient explosion does not occur in architectures with layer normalization (Ba et al., 2016), including transformer-based architectures (Dosovitskiy et al., 2020;Liu et al., 2021).et al., 2013;Clevert et al., 2015;Ramachandran et al., 2017;Hendrycks & Gimpel, 2016) exhibit lower exploding rates as they exhibit flatter behavior near zero and lower signal blockage at the initialization state.In the extreme case, gradient explosion does not occur if the activation function is not used.Note that it also does not occur with DropOut (Srivastava et al., 2014), which can be regarded as a ReLU that blocks signals randomly.the literature, such as 'large gradient magnitude at the early stage of training' or 'instability of deep neural network at the early phase of training' (Frankle et al., 2020;Goyal et al., 2017b).It is also often treated as an 'instability problem of large batch training' (Goyal et al., 2017b) since the problem becomes severe when a large batch size (and high learning rate, accordingly) is used during training.Although training instability occurs only during early stages of training, it can disrupt the balance between the layers and degrade the final performance (Goyal et al., 2017b;You et al., 2017)."}
{"paper_id": 500, "introduction": "Retrosynthetic planning is a fundamental problem in organic chemistry (Coley et al., 2018a;Genheden et al., 2020).The goal of retrosynthetic planning is to find a series of starting molecules that go through a sequence of reactions, which can also be represented as reaction tree, to synthesize the target molecule.Retrosynthetic planning can be decomposed into multi-step retrosynthesis reactions through which we find all starting molecules that meet the requirements.The multi-step reactions outline the transformation direction of organic molecules and the transformation target.In the production of the Organic Chemical Industry, it requires us to design efficient organic synthesis routes to synthesize our desired target products at a low cost.Therefore, given a target molecule, predicting reasonable and efficient reaction routes to synthesize this molecule is a very crucial problem in both machine learning and organic chemistry (Segler et al., 2018).To tackle this problem, past works, including MCTS (Segler et al., 2018), DFPN-E (Kishimoto et al., 2019), Retro* (Chen et al., 2020), self-improved retrosynthetic planning (Kim et al., 2021), RetroGraph (Xie et al., 2022), and Grasp (Yu et al., 2022), model the retrosynthetic planning as a search problem (Xie et al., 2022).Specifically, they first utilize reactions to train a template-based MLP retrosynthesis model (Segler et al., 2017) and then learn a search algorithm to perform a backward search to transform the molecules through retrosynthesis predictions until all the reactants are starting materials (Chen et al., 2020).The current benchmark for test evaluation of retrosynthetic planning models consists of 189 test routes (Chen et al., 2020).These approaches have the following limitations: 1) the training dataset of single-step reactions limits the understanding of the transformation of organic molecules as a sequence of chaining chemical reactions.2) past works use single-step retrosynthesis models, which neglect the context information in the reaction tree.3) the test set is too small to comprehensively evaluate the performance.4) the evaluation unit of existing benchmark is the reaction route which is one path from the root node to the leaf node in the reaction tree.In this work, we address these limitations by first constructing a new benchmark with 124,869 reaction trees retrieved from the public USPTO dataset and leverage the retrosynthesis transformer with an additional memory module to capture reaction tree information for retrosynthetic planning.Benchmark.SCScore (Coley et al., 2018b) concludes that the number of steps required to synthesize a molecule is an accurate metric for estimating molecule synthetic accessibility.Based on this observation and inspired by the prediction of synthesis accessibility with reaction knowledge graph (Li & Chen, 2022), we construct a reaction graph from the existing reactions in the database.On the reaction graph, directed edges represent retrosynthesis reactions where the starting point denotes the product molecule and the ending point represents the reactant molecule to synthesize this product.Given a target molecule, we can search the shortest routes to form an efficient reaction tree from the reaction graph, while the ending points of these routes are the starting molecules that satisfy the requirements.By constructing the reaction trees for target molecules, we can obtain a new benchmark for our retrosynthetic planning task.Metro.In this work, we propose Metro: Memory-Enhanced Transformer for RetrOsynthetic planning by extending Transformer with an additional memory module.Our proposed Metro can capture the dependency among the molecules on the reaction route as context information.By taking the context information into consideration, we can control the search within a reasonable reaction space specified for the reaction route.Extensive experimental results on retrosynthetic planning show that Metro achieves up to 13.2%, 14.5%, 11.1%, 10.5%, and 10.0% over transformer on top-1, top-2, top-3, top-4, and top-5 accuracy, which demonstrates the superiority of exploiting context information for retrosynthetic planning task."}
{"paper_id": 501, "introduction": "Human episodic memory breaks our continuous experience of the world into episodes or fragments that are divided by event boundaries that involve large changes of place, context, affordances, and perceptual inputs (Baldassano et al., 2017;Ezzyat & Davachi, 2011;Newtson & Engquist, 1976;Richmond & Zacks, 2017;Swallow et al., 2009;Zacks & Swallow, 2007).The episodic nature of memory is a core component of how we construct models of the world.It has been conjectured that episodic memory makes it easier to perform memory search, and to use the retrieved memories in chunks that are relevant for the current context.Humans also continue to learn and memorize new information throughout their lives, without needing to reconfigure all previously stored memories.These observations suggest a certain locality or fragmented nature to how we model the world.Chunking of experience has been shown to play a key role in perception, learning and cognition in humans and animals (De Groot, 1946;Egan & Schwartz, 1979;Gobet et al., 2001;Gobet & Simon, 1998;Simon, 1974).In the hippocampus, place cells appear to chunk spatial information by defining separate maps when there has been a sufficiently large change in context or in other non-spatial or spatial variables, through a process called remapping; see Colgin et al. (2008); Fyhn et al. (2007).Grid and place cells in the hippocampal formation have also been shown to fragment their representations when the external world or their own behaviors have changed only gradually rather than discontinuously (Derdikman et al., 2009;Low et al., 2021).Recently, Klukas et al. (2021) proposed how such remapping could occur in even during continuous navigation through a continuous environment, modeling the process as one of online clustering based on observational surprisal.Similarly, when fitting complicated manifolds or functions, it is common to build a set of simpler local models of the manifold or function.Inspired by these ideas, here we propose building models for complicated spaces by fitting a sequence of local models, and using local models obtained through an online process of fragmentation to aid in the the exploratory process of moving through a large space and building a model of the space.We propose a new framework for exploration based on a concept of online Fragmentation-and-Recall, schematized in Figure 1.This model combines two ideas: 1) when faced with a complex world, it can be more efficient to build and combine multiple (and implicitly simpler) local models than to build a single global (and implicitly complex) model, and 2) boundaries between local models should occur when a local model ceases to be predictive.In what follows, as an agent explores, it predicts its next observation.Based on a measure of surprisal between its observation and prediction, there can be a fragmentation event, at which point the agent writes the current model into long-term memory (LTM) and initiates a new local model.While exploring the space, the agent consults its LTM, and recalls an existing model if it matches its observations.For the spatial domain, this is very similar to Klukas et al. (2021).The agent uses its current local model to act locally, and its LTM to act more globally.We apply this concept to solve spatial exploration and more general reinforcement learning exploration problems, and call the corresponding approaches FarMap and FarCuriosity, respectively.We evaluate the proposed framework on procedurally-generated spatial environments and reinforcement learning benchmarks.Experimental results support the effectiveness of the proposed framework; FarMap explores the spatial environment with much less memory usage and is faster than its baselines (Yamauchi, 1997) by large margins, and FarCuriosity achieves better performance than the baseline fragmentation-less curiosity module (Burda et al., 2019) on standard heterogeneous Atari game benchmarksfoot_0 .The contribution of this paper is three-fold as follows:\u2022 We propose a new framework for exploration based on Fragmentation-and-Recall that divides the exploration space into multiple fragments and recalls previously explored ones.\u2022 We implemented our framework in spatial exploration tasks, referring to it as FarMap with short and long-term memory.Our experiments showed that FarMap reduces online memory size and wall-clock time relative to baselines.\u2022 We implemented our framework in a curiosity-driven reinforcement learning exploration setting, referring to it as FarCuriosity.FarCuriosity avoids catastrophic forgetting and achieves better performance compared to the baseline in heterogenous environments."}
{"paper_id": 502, "introduction": "Heterologous image conversion is a process of generating a modal B image of the same scene from a modal A image by constructing a pixel conversion model from modality A to modality B. Early heterogeneous image conversion requires 3D scene models and complete material information to manually design pixel conversion models.However, in practical applications, the construction of 3D models of actual scenes is difficult, the composition of materials is complex, and the acquisition of imaging characteristics of materials is difficult, which makes the solution of heterogeneous image conversion models an ill-posed problem with low efficiency and low applicability.In recent years, deep learning has led the trend in the field of computer vision, and image translation model learning is a new solution for solving heterogeneous image translation models.Image transformation model learning is an image generation technique for image super-resolution, image enhancement, and multimodal image fusion.It uses a deep convolutional network as a representation model for an arbitrary function, and solves a transition model between images of two modalities by using a fitting optimization on a training set of paired images.The most mainstream way to learn image conversion models is Conditional Generative Adversarial Nets (CGAN) proposed by Mirza & Osindero (2014).Since CGAN can model the semantic information of images and constrain the output results, CGAN is currently used for Image transformation has become a research hotspot in the field of computer vision and the latest heterogeneous image transformation methods are based on it.Image conversion model learning relies on a large amount of training data.This paper uses the Multispectral Pedestrian Detection (MPD) dataset published by the Korea Advanced Institute of Science and Technology (KAIST) (Hwang & Jaesik Park, 2015), referred to as the KAIST-MPD dataset, which is currently the only publicly available dataset that contains a large number of visual and infrared approximate common aperture images of multiple scenes.Although the heterogenous image pairs of this dataset have viewing angle deviation and the view field of visual image is smaller, the viewing angle deviation is small and the size of the view field remains fixed, so it is feasible to use this dataset to solve the transformation model.The research in this paper is to solve the heterogeneous image conversion model from infrared to visual based on the KASIT-MPD dataset, as shown in the figure below.The contributions of this paper are summarized as: 1.A new four-layer multi-scale encoder generator is proposed, in which the images are downsampled three times, and then they are encoded and decoded from small to large in turn.Such a network structure can make full use of the structural information in the input images; 2.Two structure-sensitive loss functions are proposed to solve the problem that L1 loss and generative adversarial loss cannot effectively constrain the structural information of a single image."}
{"paper_id": 503, "introduction": "Proteins are critical for life, playing a role in almost every biological process, from relaying signals across neurons (Zhou et al., 2017) to recognizing microscopic invaders and subsequently activating the immune response (Mariuzza et al., 1987), from producing energy for cells (Bonora et al., 2012) to transporting molecules along cellular highways (Dominguez & Holmes, 2011).Misbehaving proteins, on the other hand, cause some of the most challenging ailments in human healthcare, including Alzheimer's disease, Parkinson's disease, Huntington's disease, and cystic fibrosis (Chaudhuri & Paul, 2006).Due to their ability to perform complex functions with high specificity, proteins have been extensively studied as a therapeutic medium (Leader et al., 2008;Kamionka, 2011;Dimitrov, 2012) and constitute a rapidly growing segment of approved therapies (H Tobin et al., 2014).Thus, the ability to computationally generate novel yet physically foldable protein structures could open the door to discovering novel ways to harness cellular pathways and eventually lead to new treatments targeting yet incurable diseases.Many works have tackled the problem of computationally generating new protein structures, but have generally run into challenges with creating diverse yet realistic folds.Traditional approaches typically apply heuristics to assemble fragments of experimentally profiled proteins into structures (Schenkelberg & Bystroff, 2016;Holm & Sander, 1991).This approach is limited by the boundaries of expert knowledge and available data.More recently, deep generative models have been proposed.However, due to the incredibly complex structure of proteins, these commonly do not directly generate protein structures, but rather constraints (such as pairwise distance between residues) that are heavily post-processed to obtain structures (Anand et al., 2019;Lee & Kim, 2022).Not only does this add complexity to the design pipeline, but noise in these predicted constraints can also be compounded during post-processing, resulting in unrealistic structures -that is, if the constraints are at all satisfiable to begin with.Other generative models rely on complex equivariant network architectures or loss functions to learn to generate a 3D point cloud that describes a protein structure (Anand & Achim, 2022;Trippe et al., 2022;Luo et al., 2022;Eguchi et al., 2022).Such equivariant architectures can ensure that the probability density from which the protein structures are sampled is invariant under translation and rotation.However, translation-and rotation-equivariant architectures are often also symmetric under reflection, leading to violations of fundamental structural properties of proteins like chirality (Trippe et al., 2022).Intuitively, this point cloud formulation is also quite detached from how proteins biologically fold -by twisting to adopt energetically favorable configurations ( \u0160ali et al., 1994;Englander et al., 2007).Inspired by the in vivo protein folding process, we introduce a generative model that acts on the inter-residue angles in protein backbones instead of on Cartesian atom coordinates (Figure 1).This treats each residue as an independent reference frame, thus shifting the equivariance requirements from the neural network to the coordinate system itself.A similar angular representation has been used in some protein structure prediction works (Gao et al., 2017;AlQuraishi, 2019;Chowdhury et al., 2022).For generation, we use a denoising diffusion probabilistic model (diffusion model, for brevity) (Ho et al., 2020;Sohl-Dickstein et al., 2015) with a vanilla transformer parameterization without any equivariance constraints.Diffusion models train a neural network to start from noise and iteratively \"denoise\" it to generate data samples.Such models have been highly successful in a wide range of data modalities from images (Saharia et al., 2022;Rombach et al., 2022) to audio (Rouard & Hadjeres, 2021;Kong et al., 2021), and are easier to train with better modal coverage than methods like generative adversarial networks (GANs) (Dhariwal & Nichol, 2021;Nichol & Dhariwal, 2021).We present a suite of validations to quantitatively demonstrate that unconditional sampling from our model directly generates realistic protein backbones -from recapitulating the natural distribution of protein inter-residue angles, to producing overall structures with appropriate arrangements of multiple structural building block motifs.We show that our generated backbones are diverse and designable, and are thus biologically plausible protein structures.Our work demonstrates the power of biologically-inspired problem formulations and represents an important step towards accelerating the development of new proteins and protein-based therapies."}
{"paper_id": 504, "introduction": "Researchers have proposed many benchmarks to train machine learning models and test their generalization abilities, from ImageNet (Krizhevsky et al., 2012) for image recognition to Atari games Bellemare et al. (2013) for reinforcement learning (RL).More complex benchmarks promote the development of more sophisticated learning algorithms and architectures that can generalize better.However, we lack rigorous and quantitative measures of the generalization difficulty of these benchmarks.Generalizing on a task requires both training data and a model's inductive biases, which are any constraints on a model class enabling generalization.Inductive biases can be provided by a model designer, including the choice of architecture, learning rule or hyperparameters defining a model class.While prior work has quantified the training data needed to generalize on a task (sample complexity), analysis of the required inductive biases has been limited.Indeed, the concept of inductive bias itself has not been rigorously and quantitatively defined in general learning settings.In this paper, we develop a novel information-theoretic framework to measure a task's inductive bias complexity, the information content of the inductive biases.Just as sample complexity is a property inherent to a model class (without reference to a specific training set), inductive bias complexity is a property inherent to a training set (without reference to a specific model class).To our knowledge, our measure is the first quantification of inductive bias complexity.As we will describe, our measure quantifies the fraction of the entire hypothesis space that is consistent with inductive biases for a task given that hypotheses interpolate the training data; see Fig 1 for an illustration and Definition 1 for a formal definition.We use this inductive bias complexity measure as a measure of the generalization difficulty; we hope our measure can guide the development of tasks requiring greater inductive bias.As one concrete, but widely-applicable, suggestion, we find that adding Gaussian noise to the inputs of a task can dramatically increase its required inductive bias.We summarize our contributions asfoot_0 :\u2022 We propose a feature-based task framework to analyze generalization difficulty in various learning problems, including supervised learning, RL and meta-learning.\u2022 We provide a formal, information-theoretic definition of inductive bias complexity.\u2022 We develop a novel and to our knowledge first quantifiable measure of inductive bias complexity, which we propose using as a measure of the generalization difficulty of a task.This measure also allows us to quantify the relative inductive biases of different model architectures applied to a task.\u2022 We propose a practical algorithm to estimate and compare the inductive bias complexities of tasks across the domains of supervised learning, RL and few-shot meta-learning tasks.Empirically, we find that 1) partially observed RL environments require much greater inductive bias compared to fully observed ones and 2) simple few-shot meta-learning tasks can require much greater inductive bias than complex, realistic supervised learning tasks."}
{"paper_id": 505, "introduction": "Graph is a general format for many real-world data.For instance, molecules can be treated as graphs [10,14] where the chemical atoms and bonds correspond to the topological nodes and edges respectively.Processing point clouds as graphs is also a popular strategy [53,58], where points are viewed as nodes and edges are built among the nearest neighbors.Many existing works on deep generative models (DGMs) focus on modeling the graph data and improving the synthesis quality.However, understanding DGMs on graph and their learned representations has been much less explored, which may hinder the development of important applications like the controllable generation (also referred to as the data editing) and the discovery of interpretable data structure.The graph controllable generation task refers to modifying the steerable factors of graph so as to obtain graphs with desired properties [9,43].This is an important task in many applications, but traditional methods (e.g., manual editing) possess certain inherent limitations under certain circumstances.A classic example is molecule editing, which aims at modifying the substructures of molecules [38] and can relate to some key tactics in drug discovery like functional group change [13] and scaffold hopping [2,23].This is a routine task in pharmaceutical companies, yet, relying on domain experts for manual editing can be subjective or biased [9,15].Different from previous works, this paper aims to explore the unsupervised graph editing with DGMs.It can act as a good complementary module to conventional methods and bring many crucial benefits: (1) It enables the efficient graph editing in the large-scale setting.(2) It alleviates the requirements for extensive domain knowledge for factor change labeling.(3) It provides another perspective for editing preference, which reduces biases from the domain experts.One core property relevant to the general unsupervised data editing using DGMs is the disentanglement.While there does not exist a widely-accepted definition of disentanglement, the key intuition [36] is that a disentangled representation should separate the distinct, informative, and steerable factors of variations in the data.Thus, the controllable generation task would become trivial with the disentangled DGMs as the backbone.Such a disentanglement assumption has been widely used in generative modeling on the image data, e.g., \u03b2-VAE [19] learns disentangled representation by forcing the representation to be close to an isotropic unit Gaussian.However, it may introduce extra constraints on the formulations and expressiveness of DGMs [11,19,47,60].For graph data, one crucial question remains: is the latent representation space learned from DGMs on graph data disentangled?In image generation, prior work [36] shows that without inductive bias, the latent space learned by VAEs is not guaranteed to be disentangled.However, the disentanglement property of graph DGMs is much less explored.In Sec. 3, we first study the latent space of DGMs on two typical graph data (molecular graphs and point clouds), and empirically illustrate that the learned space is not perfectly disentangled.This observation then raises the second question: Given a pretrained DGM with not perfectly disentangled latent space, is there a flexible framework enabling the graph controllable generation in an unsupervised manner?To tackle this, we propose a model-agnostic and task-agnostic framework called GraphCG for unsupervised graph controllable generation.GraphCG has two main phases, as illustrated in Fig. 1.During the training phase (Fig. 1(a)), GraphCG starts with the assumption that the steerable directions can be learned by maximizing the mutual information (MI) among the semantic directions.We formulate GraphCG with an energy-based model (EBM), which provides a large family of solutions.Then during the test phase, with the learned semantic directions, we can carry out the editing task by moving along the direction with certain step sizes.As the example illustrated in Fig. 1(b), the molecular structure (hydroxyl group) changes consistently along the editing sequence.For evaluation, we visually verify the learned semantic directions on both types of graph data.Further for the molecular graphs, we propose a novel evaluation metric called sequence monotonic ratio (SMR) to measure the output sequences.We summarize our contributions as follows: (1) We conduct an empirical study on the disentanglement property of three pretrained deep generative models (DGMs) on two types of graph data, molecular graphs and point clouds.We find that the latent space of these pretrained graph DGMs is not perfectly disentangled.(2) We propose a model-agnostic and task-agnostic method called GraphCG for the unsupervised graph controllable generation.GraphCG aims at learning the semantic directions by maximizing their corresponding mutual information, and its outputs are sequences of graphs.(3) We qualitatively evaluate the proposed methods on two types of graph data, molecular graphs and point clouds.Besides, the quantitative results further show the clear improvement over the baselines.Recent works leverage the DGMs for various controllable generation tasks [5,61], where the inherent assumption is that the learned latent representations encode rich semantics, and thus traversal in the latent space can help steer factors of data [17,26,52].Among them, one research direction [40,52] is using supervised signals to learn the semantic-rich directions, and most works on editing the graph data focus on the supervised setting [27,57,64].However, these approaches can not be applied to many realistic scenarios where extracting the supervised labels is difficult.Another research line [17,45,51] considers discovering the latent semantics in an unsupervised manner, but most unsupervised methods are designed to be either model-specific or task-specific, making them not directly applicable to the graph data.More comprehensive discussion is in Appendix B."}
{"paper_id": 506, "introduction": "The rapid development of visual recognition in recent years has led to the need for frequently updating existing models in production-scale systems.However, when replacing a legacy classification model, one has to weigh the benefit of decreased error rate against the risk of introducing new errors that may disrupt post-processing pipelines (Yan et al., 2021) or cause friction with human users (Bansal et al., 2019).Positive-Congruent Training (PC-Training) refers to any training procedure that minimizes the negative flip rate (NFR) along with the error rate (ER).Negative flips are instances that are misclassified by the new model, but correctly classified by the old one.They are manifest in both visual and natural language tasks (Yan et al., 2021;Xie et al., 2021).They typically include not only samples close to the decision boundary, but also highconfidence mistakes that lead to perceived \"regression\" in performance compared to the old model.They are present even in identical architectures trained from different initial conditions, or with different data augmentations, or using different sampling of mini-batches.Yan et al. (2021) have shown that in state-of-the-art image classification models, where a 1% improvement is considered significant, NFR can be in the order of 4\u223c5% even across models that have identical ER.These intriguing properties motivate us to investigate causes of negative flips and mechanism of reducing negative flips to establish a model update method that achieves the cross-model compatibility, thus lower NFR, and lower error rate, for better PC-training.Two questions.A naive approach to cross-model compatibility is to bias one model to mimic the other, as done in model distillation (Hinton et al., 2015).In this case, however, compatibility comes at the expense of accuracy (Yan et al., 2021;Bansal et al., 2019).On the other hand, averaging a number of models in a deep ensemble (Lakshminarayanan et al., 2017) can reduce NFR without negative accuracy impact (Yan et al., 2021), even if it does not explicitly optimize NFR nor its surrogates.The role of ensembles in improving accuracy is widely known, but our first question arises: what is the role of ensembles in reducing NFR?Even though using deep ensembles achieves state-of-the-art performance in terms of reducing NFR (Yan et al., 2021), it is not viable in real applications at scale since it multiplies the cost of"}
{"paper_id": 507, "introduction": "Optimal transportation (Villani, 2009;Ambrosio, 2003;Santambrogio, 2015;Peyr\u00e9 et al., 2019;Merigot and Thibert, 2021) is thriving in domains including economics (Galichon, 2016), reinforcement learning (Dadashi et al., 2021;Fickinger et al., 2021), style transfer (Kolkin et al., 2019), generative modeling (Arjovsky et al., 2017;Seguy et al., 2018;Huang et al., 2020;Rout et al., 2021), geometry (Solomon et al., 2015;Cohen et al., 2021), domain adaptation (Courty et al., 2017;Redko et al., 2019), signal processing (Kolouri et al., 2017), fairness (Jiang et al., 2020), and cell reprogramming (Schiebinger et al., 2019).A core component in these settings is to couple two measures (\u03b1, \u03b2) supported on domains (X , Y) by solving a transport optimization problem such as the primal Kantorovich problem, which is defined by: \u03c0 (\u03b1, \u03b2, c) \u2208 arg min \u03c0\u2208U (\u03b1,\u03b2) X \u00d7Y c(x, y)d\u03c0(x, y),(1)where the optimal coupling \u03c0 is a joint distribution over the product space, U(\u03b1, \u03b2) is the set of admissible couplings between \u03b1 and \u03b2, and c : X \u00d7 Y \u2192 R is the ground cost, that represents a notion of distance between elements in X and elements in Y.Challenges.Unfortunately, solving eq. ( 1) once is computationally expensive between general measures and computationally cheaper alternatives are an active research topic: Entropic optimal transport (Cuturi, 2013) smooths the transport problem with an entropy penalty, and sliced distances (Kolouri et al., 2016;2018;2019;Deshpande et al., 2019) solve OT between 1-dimensional projections of the measures, where eq. ( 1) can be solved easily.Furthermore, when an optimal transport method is deployed in practice, eq. ( 1) is not just solved a single time, but is repeatedly solved for new scenarios between different input measures (\u03b1, \u03b2).For example, the measures could be representations of images we care about optimally transporting between and in deployment we would receive a stream of new images to couple.Repeatedly solving optimal transport problems also comes up in the context of comparing seismic signals (Engquist and Froese, 2013) and in single-cell perturbations (Bunne et al., 2021;2022b;a).Standard optimal transport solvers deployed in this setting would re-solve the optimization problems from scratch, but this ignores the shared structure and information present between different coupling problems.Overview and outline.We study the use of amortized optimization and machine learning methods to rapidly solve multiple optimal transport problems and predict the solution from the input measures (\u03b1, \u03b2).This setting involves learning a meta model to predict the solution to the optimal transport problem, which we will refer to as Meta Optimal Transport.We learn Meta OT models to predict the solutions to optimal transport problems and significantly improve the computational time and number of iterations needed to solve eq. ( 1) between discrete (sect.3.1) and continuous (sect.3.2) measures.The paper is organized as follows: sect. 2 recalls the main concepts needed for the rest of the paper, in particular the formulations of the entropy regularized and unregularized optimal transport problems and the basic notions of amortized optimization; sect.3 presents the Meta OT models and algorithms; and sect.4 empirically demonstrates the effectiveness of Meta OT.Settings that are not Meta OT.Meta OT is not useful in OT settings that do not involve repeatedly solving OT problems over a fixed distribution, including 1) standard generative modeling settings, such as Arjovsky et al. (2017) that estimate the OT distance between the data and model distributions, and 2) the out-of-sample setting of Seguy et al. (2018); Perrot et al. (2016) that couple measures and then extrapolate the map to larger measures containing the original measures."}
{"paper_id": 508, "introduction": "Scaled dot-product attention (Vaswani et al., 2017) has become a core mechanism of state-of-the-art deep learning models for variety of machine learning tasks, including natural language processing (Devlin et al., 2018), multi-modal task (Li et al., 2019), and graph data processing (Hamilton et al., 2017).Specifically, the Transformers using the self-attention method have replaced recurrent neural networks (RNN) by outperforming them in most of the tasks.Despite its success, there exist limitations to the mechanism.First, it needs the information of the entire sequence to compute one attention so that its computational complexity becomes quadratic to the length of the sequence.Hence, it is inefficient for long sequence tasks (Tay et al., 2020) or edge inference environments (Tambe et al., 2020).Also, its stateless design enables efficient parallelism but makes it impossible to solve tasks that require memory states.Hence, Transformers fail to generalize the rules that require inductive bias (Dehghani et al., 2018) or compositional generalization (Lake & Baroni, 2018).There have been studies designing neural networks with external memory to solve algorithmic tasks where Transformers fail.These memory-augmented neural networks (MANN) design differentiable read/write functions that can be trained by backpropagation.Some of them implement basic data structures like stack (Joulin & Mikolov, 2015) and queue (Grefenstette et al., 2015) while some implement complex memory structures using attention mechanisms (Graves et al., 2014;2016).They outperform generic neural networks in synthetic algorithmic tasks but are considered impractical due to their complexities and inefficiencies.In this work, we re-invent the attention mechanism as a memory architecture for neural networks, namely neural attention memory (NAM).NAM's design objective is to build simple, efficient, yet powerful external memory which also incorporates the attention mechanism.Following the same query-key-value structure of attention, NAM stores key-value pairs to a memory matrix via additively writing their outer-products.Reading the memory matrix is simply done by multiplying the matrix with a unit query vector.We provide mathematical formulation for the read/write primitives, and make theoretical analyses showing that these read and write primitives can replace attention.One big benefit of NAM is that it can perform attention in a more efficient way.By sacrificing the erasure capability of the NAM write operation, we can design an efficient and parallel attention mechanism, namely normalized outer-product attention.This special variant of NAM is almost equivalent to linear attention Katharopoulos et al. (2020), enjoying the same linear computational complexity to the sequence length.We evaluate NAM-based efficient Transformer in long-range arena (Tay et al., 2020) tasks.Its efficacy is on par with the base Transformer and Linear Transformer, implying that NAM can be an efficient alternative to the scaled dot-product attention.The bigger value of NAM is that its read and write primitives can be building blocks for augmenting memory structures to deep neural networks.Using NAM read/write primitives, we design two memory-augmented neural networks (MANN), namely Long Short-term Attention Memory (LSAM) and NAM Turing Machine (NAM-TM).LSAM is a generic RNN architecture that replaces LSTM's long-term cell state with a memory matrix.Instead of additively writing a vector cell state, LSAM reads and writes the memory matrix using NAM primitives.The design combines strengths of attention and RNN while maintaining the same computational complexity as LSTM.NAM-TM is a MANN for algorithmic tasks, leveraging a Turing tape structure.A tape has read and write heads accessing the memory with NAM read/write primitives.They can move along the tape with four actions: NO-OP, LEFT, RIGHT, and JUMP.The actions are implemented as differentiable functions to enable end-to-end training with backpropagation.We compare LSAM and NAM-TM to others in compositional generalization tasks of number sequence prediction (Nam et al., 2019), sequence reduction, and SCAN (Lake & Baroni, 2018).Specifically, we test their zero-shot generalization capability in length by training the models with sequences of limited length and validating them with longer sequences unobservable during training.The evaluation results show that their computational powers are superior to other baselines, including Universal Transformer (Dehghani et al., 2018) and DNC (Graves et al., 2016).While the generic LSAM model consistently outperforms the others, NAM-TM shows even better results at algorithmic tasks.The results indicate that NAM is a powerful method to implement memory in neural networks.The efficient, simple, and flexible structure of NAM opens up new possibilities in multiple machine learning research fields.One straightforward application is leveraging NAM's efficiency for edge inference environment.Another possibility is using NAM for hierarchical data modeling by generalizing NAM with tensor products.Moreover, memorization of input-output mapping using NAM can be a solution for one-shot and few-shot learning.The main contributions of this work are as follows:\u2022 We re-invent the attention mechanism as a memory architecture for neural networks, namely neural attention memory (NAM).\u2022 We present mathematical basis for NAM read/write primitives, and give theoretical proofs that NAM is equivalent to attention in certain conditions.\u2022 We show that NAM can construct an efficient Transformer for long-range sequence tasks.\u2022 We propose two memory-augmented neural network designs of LSAM and NAM-TM and show their capabilities in compositional generalization tasks."}
{"paper_id": 509, "introduction": "The recent rapid advances in machine learning (ML) can be largely attributed to powerful computing infrastructures as well as the availability of large-scale datasets, such as ImageNet1K (Deng et al., 2009) for computer vision, WMT (Foundation, 2019) for neural machine translation, and Lib-riLight (Kahn et al., 2020) for speech recognition.Studies have shown that ML models trained on large-scale datasets can usually prove effective in many additional downstream tasks (Brown et al., 2020).On the other hand, ethical concerns have been raised surrounding proper data usage in issues like data privacy (Liu et al., 2021), data minimization (Goldsteen et al., 2021), etc.Therefore, if there are more data available and can be used without worrying whether or not the data is handled properly, the ML models can be further improved by more data and help the ML community to advance on many domains.Attempting to balance between model performance and proper data usages, a common approach usually taken is to simply modify the data to remove its \"sensitive\" attributes, and experimentally demonstrate that the targeted sensitive attributes are indeed removed.What's crucially important but usually omitted here, however, is the preservation of the \"total utility\" of the data, because the suppression operation oftentimes also negatively impact, or even completely destroy, the other \"non-sensitive\" attributes, hence greatly damaging the dataset's potential future utility.For example, DeepPrivacy (Hukkel\u00e5s et al., 2019) is able to demonstrate its privacy protection capability, but the modified data it produces can no longer be utilized for additional downstream tasks like sentiment analysis, age detection, or gender classification.Since data is one of the main driving forces for the rapid advancement of machine learning research, we argue that the ideal scenario would be to have the flexibility of selecting an arbitrary set of attributes and only suppressing them while leaving all the other attributes completely intact.In this way, the community could unleash the potential utility of the modified data to develop more advanced algorithms.Towards this exact goal, we present Multi-attribute Selective Suppression (or MaSS) in this paper, to enable such capability of precise attributes suppression for multi-attribute datasets.The high-level Figure 1: MaSS is able to precisely target any selected attributes in a multi-attribute dataset for suppression while leaving the rest of the attributes intact for any potential downstream ML-based analytic tasks.For example as illustrated by the diagram, when operating on the original multiattribute dataset and configured to suppress Attr.0, MaSS is able to transform the dataset such that the model for detecting Attr.0 is unable to reliably detect Attr.0 from the transformed data, while the models for Attr. 1 and 2 are not affected.objective of MaSS is also illustrated in Figure 1, where MaSS is configured to suppress Attr.0 without knowing in advance that Attr. 1 and 2 will be used for downstream tasks.After the data transformation performed by MaSS, Attr.0 becomes suppressed, nondetectable by its corresponding machine learning model, but at the same time, Attr. 1 and 2 are left intact, and still can be extracted by their corresponding ML models.As a concrete example, suppose we are working with a facialimage dataset which contains attributes like age, gender, and sentiment, where, let us assume, age and gender are considered sensitive.Then, MaSS would transform this facial-image dataset such that age and gender information could no longer be not be inferred by the corresponding ML models, whereas sentiment information could still be extracted from the transformed data.The contributions of our work are threefolds, 1.We propose the novel MaSS framework to enable the powerful flexibility of precise suppression of arbitrary, selective data attributes.2. We employ multiple learning mechanisms in MaSS to enable its attribute-specific as well as generic feature preservation capabilities, which help it achieve satisfactory data utility protection both with and without the prior knowledge about downstream tasks.3. We thoroughly validate MaSS using a wide range of multi-attribute datasets, including image, audio, and video.All our results demonstrate MaSS' strong performance in its intended selective attribute suppression and preservation."}
{"paper_id": 510, "introduction": "Weakly-supervised object localization (WSOL) aims to use only image-level labels (class labels) to perform localization.Compared to methods that require full annotations, WSOL can be much more resource efficient; it has therefore been widely studied (Choe & Shim, 2019;Singh & Lee, 2017;Zhang et al., 2018a;b;Zhou et al., 2016;Guo et al., 2021;Wei et al., 2021;Babar & Das, 2021;Gao et al., 2021;Xie et al., 2021).Class Activation Mapping (CAM) (Zhou et al., 2016) is a heatmap-based explainable artificial intelligence (XAI) method that enables Convolutional Neural Network (CNN) to perform WSOL.Other heatmap-based XAI methods have been designed to compute relevance/attribution maps, some of which have been treated as localization maps after some processing e.g.Saliency (Simonyan et al., 2014) has been used for WSOL using only gradient (obtained from backpropagation) and minimal post-processing.In this paper, besides Saliency, we will also investigate the WSOL capability of several heatmap-based XAI methods: GradCAM (Selvaraju et al., 2016) (generalization of CAM), Guided Backpropagation (GBP) (Springenberg et al., 2015) and DeepLift (Shrikumar et al., 2017).Admittedly, there are many other methods that are not included in this paper e.g., Layerwise Relevance Propagation (also its derivatives (Bach et al., 2015;Montavon et al., 2017;Kohlbrenner et al., 2020)) and modifications of CAM (Muhammad & Yeasin, 2020;Wang et al., 2020;Jalwana et al., 2021;Kindermans et al., 2018).Main objective of this paper: measure the WSOL capability of existing heatmap-based XAI method applied on ResNet50 and improve them.Fig. 1 shows how existing XAI methods can be very unsuitable for WSOL (e.g.high granular heatmaps and uneven edge detection).This paper shows that it is possible to modify the aforementioned methods and improve their localization ability beyond baseline CAM.Important clarifications:1.It is not our intention to invent yet another XAI method.Instead, we add intermediate steps (based on CAM-like concept) on existing techniques to improve WSOL performance.2. We do not claim to attain the best localization.We are in fact testing the metric MaxBoxAcc presented in CVPR 2020 (Choe et al., 2020).In that paper, a dedicated training process is performed to optimize the said metric; in their github, this training is simply called the WSOL training.While their training improved WSOL, as a trade-off, their classification performance has degraded, and we quote them \"There are in general great fluctuations in the classification resultsFigure 1: Localization capabilities of XAI methods differ, some may not be suitable for WSOL (A) When naively normalized to [0, 1], saliency method shows sparse and granular heatmaps compared to CAM, while guided BP shows an attribution map with unclear localization.(B) Heatmaps from various methods during sanity checks.We can infer that different localization information may (but not necessarily) exist within different layers.Figure (B) is used with permission obtained from the authors of (Sixt et al., 2020).(26.8% to 74.5% on CUB)\".This accuracy degradation can be found in their appendix, section C2.By contrast, we prioritize interpretability, hence our baseline is CAM without WSOL training.Instead of WSOL training, we use NBDT training (see later).Other XAI methods are tested on the same metric and compared to CAM.1. Vanilla CAM after Neural Backed Decision Tree (NBDT) training yields the highest performance, besting most other methods by a significant margin.Heatmaps derived from Saliency method applied to the input layer obtains high scores as well but the method requires a peculiarly low operating threshold.2. With the proper application of CAM-like concepts, heatmaps obtained from the inner layers of existing XAI methods can perform WSOL that beats the original CAM without NBDT.3. The NBDT original paper (Wan et al., 2021) trains larger models from scratch.However, we successfully fine-tuned pre-trained ResNet50 without causing the collapse of predictive performance."}
